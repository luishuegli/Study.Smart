--- PAGE 1 ---
Statistik für VWL
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 2 ---
Inhaltsverzeichnis
Statistik für VWL
Teil I: Wahrscheinlichkeitsrechnung
1. Grundlagen der Wahrscheinlichkeit
Folie 
7
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
Folie 
  8
 
1.2. Das Rechnen mit Ereignissen
Folie 
19
 
1.3. Der Wahrscheinlichkeitsbegriff
Folie  27
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
Folie 
41
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
Folie   44
 
1.6. Wahrscheinlichkeitsräume
Folie 
50
 
1.7. Bedingte Wahrscheinlichkeit und stochastische
 
       Unabhängigkeit
Folie 
64
 
1.8. Totale Wahrscheinlichkeit
Folie 
77
 
1.9. Das Bayes-Theorem
Folie 
85
2.
Elementare Kombinatorik
Folie 
93
 
2.1. Fakultät und Binomialkoeffizient
Folie 
94
 
2.2. Das Fundamentalprinzip der Kombinatorik
Folie 
97
 
2.3. Permutationen
Folie 
99
 
2.4. Kombinationen
Folie 101
 
2.5. Variationen
Folie 103
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 3 ---
3. Zufallsvariablen
Folie 105
 
3.1. Die Verteilungsfunktion
Folie 117
 
3.2. Diskrete Zufallsvariablen
Folie  125
 
3.3. Stetige Zufallsvariablen
Folie  129
 
3.4. Erwartungswerte von Zufallsvariablen
Folie  138
 
3.5. Varianz
Folie  156
 
3.6. Standardisieren
Folie  169
4. Stochastische Modelle und spezielle Verteilungen
Folie 173
 
4.1. Gleichförmige Verteilung (diskret)
Folie 176
 
4.2. Bernoulli-Verteilung (diskret)
Folie  183
 
4.3. Binomialverteilung (diskret)
Folie 187
 
4.4. Poisson-Verteilung (diskret)
Folie  195
 
4.5. Rechteckverteilung (stetig)
Folie  201
 
4.6. Exponentialverteilung (stetig)
Folie 205
 
4.7. Normalverteilung (stetig)
Folie 209
5. Mehrdimensionale Zufallsvariablen
Folie  218
 
5.1. Gemeinsame Verteilung und Randverteilungen
Folie  223
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
Folie 240
 
5.3. Kovarianz und Korrelationskoeffizient
Folie 248
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
Folie 254
6. Der zentrale Grenzwertsatz
Folie 261
3
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 4 ---
Teil II: Statistik
7. Beschreibende/Deskriptive Statistik
Folie  278
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
Folie  281 
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
Folie 289
 
7.3. Boxplot
Folie 302
 
7.4. Quantile-Quantile Plot
Folie 305
 
7.5. Streudiagramm
Folie 310
8. Schätzung unbekannter Parameter: Punktschätzung
Folie  313  
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
Folie 318 
 
8.2. Eigenschaften von Punktschätzungen
Folie 328
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
Folie 346
9. Intervallschätzungen-Konfidenzintervalle
Folie  361  
 
9.1. Konzept des Konfidenzintervalls
Folie 362 
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
Folie 366
 
9.3. Zusammenhang mit Hypothesentests
Folie  370
10. Hypothesentests
Folie  375  
 
10.1. Arten von Hypothesen
Folie 376 
 
10.2. Kritischer Bereich und Teststatistik
Folie 379
 
10.3. Gütefunktion und Arten von Fehlern
Folie  383
 
10.4. Der p-Wert 
Folie  392
Teil III: Aufgaben 
Folie  395
 
4
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 5 ---
Teil I: 
Wahrscheinlichkeitsrechnung
5
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 6 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 7 ---
1. Grundlagen der 
 
Wahrscheinlichkeitstheorie
7
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 8 ---
1.1. Ereignisse, Ereignisraum und 
Ereignismenge
Ein Experiment heisst Zufallsexperiment, wenn es:
➔ nach einer ganz bestimmten Vorschrift aus-
 
geführt wird;
➔ unter gleichen Bedingungen beliebig oft  
 
wiederholbar ist; und
➔ das Ergebnis ungewiss ist und nicht 
 
vorausgesagt werden kann.
8
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 9 ---
Definition
Die einzelnen, nicht weiter zerlegbaren und sich 
gegenseitig ausschliessenden Ausgänge oder 
Ergebnisse eines Zufallsexperimentes heissen 
Elementarereignisse.
9
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
351
Elastase
Z
K


--- PAGE 10 ---
Definition
Die Menge S aller Elementarereignisse eines 
Zufallsexperiments heisst
Ereignisraum 
dieses Zufallsexperiments.
10
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S
Bsp
muntwent
S
Z
A


--- PAGE 11 ---
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5
1,2
6
S
KK
KZ
ZA
ZZ


--- PAGE 12 ---
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S
t
to
R
S
alle funktionen
IR
IR


--- PAGE 13 ---
Beispiel 1.1.1:
) Beim Werfen eines Würfels:
 
) Wirft man eine Münze zweimal:
 
 
) Man werfe eine Münze so lange, bis Kopf 
erscheint: 
 
(unendlich viele Elemente, aber abzählbar)


S = 1, 2, 3, 4, 5, 6


S = K, ZK, ZZK, ZZZK, ....


S = KK, KZ, ZK, ZZ
11
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 14 ---
) Für die Lebensdauer einer Glühbirne gilt:
 
(mehr als endlich viele oder abzählbar unendlich 
 viele Elemente) ➔ stetiges Kontinuum von 
 Elementarereignissen.
) Für die Entwicklung eines Aktienkurses ist eine 
mögliche Wahl des Funktionenraumes


S = t : t 
 0  = [0, ) = 
+


R


+
+
→
S = alle Funktionen: R
R
.
12
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 15 ---
Merke:
Ein Zufallsexperiment kann durch 
mehrere Ereignisräume beschrieben 
werden. 
13
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 16 ---
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5
KK
KZ
Zk
ZZ
52
2,0
1,1
0
2 9
53
g
u


--- PAGE 17 ---
Beispiel 1.1.2:
2 Münzen werden geworfen:
→ falls die Ausgänge K und Z beider Münzen 
interessieren:
 
→ falls die Anzahl K und die Anzahl Z interessieren:
 
→ falls interessiert, ob die Bilder gleich (g) oder 
verschieden (u) sind:
 
(
)
(
) (
) (
)


1
S = 
K , K  , K, Z , Z, K , Z, Z
(
) (
) (
)


2
S  = 
2, 0 , 1, 1 , 0, 2



3
S  = 
g , u
.
14
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 18 ---
Definition
Ein zufälliges Ereignis A ist eine Teilmenge des 
Ereignisraumes S.
Das Ereignis A ist eingetreten, wenn das Ergebnis 
des Zufallsexperimentes ein Element dieser 
Teilmenge A ist.
15
Definition
Ein zufälliges Ereignis A ist eine Teilmenge des 
Ereignisraumes S.
Das Ereignis A ist eingetreten, wenn das Ergebnis 
des Zufallsexperimentes ein Element dieser 
Teilmenge A ist.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S
Snisz
AES
Si
wenn
SEA
A
ist
eingetmeten


--- PAGE 19 ---
Definition
Alle Ereignisse eines Zufallsexperimentes mit 
dem Ereignisraum S bilden die dazugehörige 
Ereignismenge E(S).  Menge der Teilmengen 
von S
16
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Ereignistaum
S
BSI
Manawunt
S
8,24
E
K
E
Biz


--- PAGE 20 ---
Zur Ereignismenge E müssen zwei besondere 
Teilmengen von S gehören:
1. Der Ereignisraum selbst als das sogenannte 
sichere Ereignis: 
S ⊂ E;
2. Die leere Menge als das sogenannte 
unmögliche Ereignis: 
Ø ⊂ E.
17
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 21 ---
18
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 22 ---
1.2. Das Rechnen mit Ereignissen
 
Negation: 
Ā  (“nicht A”)
 
tritt genau dann ein, wenn A nicht eintritt.
19
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
I
SLA
A
I


--- PAGE 23 ---
Das Ereignis Ā  = S\A heisst das zu A 
komplementäre Ereignis.
Ā
A
S
20
Definition
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 24 ---
Definition
Vereinigung:   A∪B  („A oder B„) tritt genau dann ein, 
wenn Ereignis A oder Ereignis B oder beide zugleich 
eintreten.
A
B
S
21
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 25 ---
Definition
Durchschnitt:   A ∩ B  („A und B“) tritt genau dann 
ein, wenn Ereignis A und Ereignis B gleichzeitig 
eintreten.
A∩B
B
S
A
22
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
Eg


--- PAGE 26 ---
Definition
Zwei Ereignisse A und D heissen disjunkt oder 
unvereinbar, wenn A ∩ D  =  Ø ist.
A
D
S
Disjunkte Ereignisse
23
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 27 ---
Definition
Differenz: A\B („A ohne B“) tritt genau dann ein, wenn 
zwar A, aber nicht B eintritt.
A\B
A
B
S
24
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 28 ---
Definition
Tritt ein Ereignis A stets ein, wenn ein Ereignis C 
eintritt, so sagt man, 
Ereignis C impliziert Ereignis A.
C impliziert A
C
A
S
C impliziert A
⇔ C ⊂ A
25
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
SEAC
8s


--- PAGE 29 ---
26
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.3-5
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 30 ---
1.3. Der Wahrscheinlichkeitsbegriff
P: E 
ℝ
 
A 
P(A)
ist eine reellwertige Funktion:
Jedem Element von E wird genau ein 
Element von ℝ zugeordnet.
27
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
f
Wahrschentichheitsmass
PCA
IPCA
50,1
AES
AEE
S
A


--- PAGE 31 ---
2) Statistische Wahrscheinlichkeit
Experiment als einzig zulässiges
 
Verfahren zur Bestimmung von 
 
Eintrittswahrscheinlichkeiten
29
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 32 ---
Definition
Der Grenzwert P(A)   = 
heisst die statistische Wahrscheinlichkeit des 
Ereignisses A, mit
hn (A) = "relative Häufigkeit des Eintretens von A".
30
→
n
lim  h (A)
n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
in
n
Versuchen
Bsp
Manzwart
A
64
Un A
relative hianfight
von Kopf
in
a
winter


--- PAGE 33 ---
(
)
"
"
P
6
=
→

1
 
 drittes Konzept 
 0.1666
6
    "Klassische Wahrscheinlichkeit"
31
(
)
"
"
P
6
Beispiel 1.3.1:
Ein Würfel werde 3‘000-mal hintereinander geworfen. 
Nach jedem Wurf notieren wir, wie viele Sechser 
bisher gefallen sind.
 
 
(z.B. Tabelle 8.1 im Buch)
Was hätten wir für             erwartet?
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
IP
eine
6
zu warfeln
76


--- PAGE 34 ---
32
Illustration Beispiel 1.3.1:
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 35 ---
3) Klassische Wahrscheinlichkeit 
Allerdings hat Bernoulli schon über 100 Jahre 
früher denselben Vorschlag gemacht.
entwickelt von Laplace, 1812
33
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
O


--- PAGE 36 ---
“Wenn ein Experiment eine Anzahl 
verschiedener und gleich möglicher Ausgänge 
hervorbringen kann und einige davon als günstig 
anzusehen sind, dann ist die Wahrscheinlichkeit 
eines günstigen Ausgangs (Ereignis A) gleich 
dem Verhältnis der Anzahl der günstigen zur 
Anzahl der möglichen Ausgänge.“
34
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 37 ---
 
# der günstigen Ausgänge 
 
 g
P(A) =  
=
 
# der möglichen Ausgänge 
 
 m
35
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Laplace Method


--- PAGE 38 ---
Definition
Ein Zuffallsexperiment mit endlich vielen 
gleichwahrscheinlichen Elementarereignissen 
heisst
Laplace-Experiment.
 
36
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 39 ---
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ptribt
Elementarereignisse
S
K
4
K
4
z
4
2,4
Sind
diese
Elementarereignisse gleichwahuschaintion
52
K 1
B12
A 3
516
2117
2,6
gleichwamscheintion
Ja
A
K
5
516
S
p
A
gunshige Ausgage
22
1
moglicheAusgage


--- PAGE 40 ---
Beispiel 1.3.2:
Eine Münze und ein Würfel werden gemeinsam 
geworfen. Wir stellen uns folgende Frage: 
Wahrscheinlichkeit, dass 
 
A = “Kopf und Augenzahl grösser als 4 erscheint“
Konstruktion des Ereignisraumes:
 
Elementarereignisse:
 
sind gleichwahrscheinlich? Nein!
(
) (
) (
) (
)




K,
4 , K,
4 , Z,
4 , Z,
4
37
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 41 ---
→ Elementarereignisse:
 
 
 
 
gleichwahrscheinlich!
Benützen wir das Laplace-Modell
(
) (
) (
) (
)
(
) (
)
K,1 , Z,1 , K,2 , Z,2 ,…, K,6 , Z,6
(
) (
)


( )

=
=
=
2
1
 A 
 
K,5 , K,6
 und P A
 
 
 
6
12
38
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 42 ---
 
 
 
 
 
 
 
 
 
 
 
 
Frage
IP
mindestens cinnal Kopf
3
4
e


--- PAGE 43 ---
Beispiel 1.3.3:
Beim zweimaligen Münzwurf ist 


S = KK, KZ, ZK, ZZ .


3
P "mindestens einmal Kopf"  = 4
39


--- PAGE 44 ---
40
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.6


--- PAGE 45 ---
Kolmogorov
1933
ÉÉ A FÉsstFhhshetÉhÉF
ff
wahrscheinlichheits mass
wenn
Lgzppyzqyyyyygg.jp
Third
P
A
1
P
A
for jedes AEE
Ben
IPg
1
IP A
PCI
s
AUF
disjunkt
thmzi.ph
Ba
iP 500 EiP s
P 0
17 0
0


--- PAGE 46 ---
1.4. Axiome der Wahrscheinlichkeitstheorie
 
41
Definition
Eine jede Funktion P 
 
die jedem Ereignis A aus der Ereignismenge E 
eine reelle Zahl zuordnet, heisst Wahrschein-
lichkeitsfunktion und P(A) heisst Wahrscheinlich-
keit von A wenn sie die folgenden Axiome  (von 
Kolmogorov, 1933) erfüllt:
P: 
E 
ℝ
 
A 
P(A)


--- PAGE 47 ---
Definition (Fortsetzung)
Axiom 1: 
P(A) 
≥ 0, ∀ A є E
Axiom 2: 
P(S) 
= 1
Axiom 3: 
P(A∪B) = P(A) + P(B), 
 
 
 
 
falls A∩B = Ø
(Additionsregel für disjunkte Ereignisse)
42


--- PAGE 48 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.7


--- PAGE 49 ---
Sei
ne IN beliebig
Thus
Seien
Eveignisse
A1 Az
An EE
paarweise disjunkt
Dann
gilt
P
A
PCA
Be
per
indubtion
Iburganfgabe
An
Az
Anntz
IP
AUA
A
P A2


--- PAGE 50 ---
Theorem 4
seien
A
B
EE
Zwei
beliebig
Energuisse
Dann
gilt
P
B
P
A
ADB
Be
qq.EE
B
I
s
AIB
Thm_5
enA
BbelebEveinisse
A
P
A
P
B
P
AnB
A


--- PAGE 51 ---
Theme
Monotonic
Seien
A
B E
E
Eveignisse
so
class
AEB
Dann
gilt
P
A
PCB
1


--- PAGE 52 ---
1.5. Wichtige Regeln der Wahrschein-
 
lichkeitsrechnung
 
44
Die Wahrscheinlichkeit des zu A  komplementären 
Ereignisses ist  stets
P(Ā) = 1 - P(A), für jedes A є E
Theorem 2
Das unmögliche Ereignis hat die Wahrscheinlichkeit 
Null:  
P(Ø) = 0
Theorem 1


--- PAGE 53 ---
45
Sind die Ereignisse A1,A2, ..., An є E paarweise disjunkt, so ist 
die Wahrscheinlichkeit für das Ereignis, das aus der 
Vereinigung all dieser Ereignisse entsteht, gleich der Summe 
der Einzelwahrscheinlichkeiten. Das heisst
Theorem 3
(
)
n
n
i
i
i=1
i=1
P
A
=
P A .






U
Für eine Differenzmenge A\B gilt stets 
  
P(A\B) = P(A) – P(A∩B).
Theorem 4


--- PAGE 54 ---
Impliziert
Impliziert das Ereignis A das Ereignis B, dann ist die 
Wahrscheinlichkeit von B niemals kleiner als die von A, das 
heisst
 
 
A ⊂ B  
P(A) ≤ P(B)
46
Für zwei beliebige Ereignisse A und B aus E gilt  
      
P(A∪B) = P(A) + P(B) – P(A∩B).
Theorem 5 (Additionssatz)
Theorem 6 (Monotonie Eigenschaft)


--- PAGE 55 ---
A
3stellige Zahl
enthalt wiederhotte Ziffern
I
Zahl
enthalt
Leine
wider hollififfern
Ide
P A
1
P A
1
1
It
10
9 8
720
151
1000
IP A
1
7
0
02g


--- PAGE 56 ---
Beispiel 1.5.1:
Wie gross ist die Wahrscheinlichkeit, dass eine 
zufällig gewählte dreistellige Zahl wiederholte 
Ziffern enthält? (Benützen Sie das Laplace-Modell)
Hier ist offenbar


3
S = 000,…,999 ,  S
# Elemente von S = 10
=
47


--- PAGE 57 ---
Betrachten wir das Ereignis 
 
so ist 
( )
A
# dreistellige Zahlen mit lauter 
verschiedenen Ziffern = 10 9 8 = 720 
720
P A
1
0.28
1000
=


= −
=


A = Zahl enthält wiederholte Ziffer ,
( )
( )
Th.1
A
P A = 1 P A  = 1
,   und
S
−
−
48


--- PAGE 58 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.8


--- PAGE 59 ---
E
Eneignismenge
2 vortesang
Bsp
Warfel
S
11213,4 5,6
E
112,3
415,6
1123,51133
4112,3 4.5.64
S
Repetition
Kolmogorov 1933
AEE
A
1124
P
A
A
SLA
PCA
1
PCI


--- PAGE 60 ---
Beweis
IDEE
induction
tber
n
1 21
A3
Indabtions veranteung
that
Annature Aussap gilt for
n
Zu zeigen
Aussage gilt auch for
nt
P
iAi
P
van
p
B
IP
Anti
YAi
P
Anti
P A
asanas
ÉiPlAi


--- PAGE 61 ---
Beweis
Theorem
5
B
B
BIA
v
AnB
P
B
P
BIA
P
AMB
e
AUB
BIA
u
A
P
AUB
IP
BIA
IP A
2
p
B
P
AUB
IP A
IP AMB


--- PAGE 62 ---
1.6. Wahrscheinlichkeitsräume
Diskrete Wahrscheinlichkeitsräume
50
Definition (Diskreter Wahrscheinlichkeitsraum)
Ist ein Ereignisraum S endlich oder abzählbar 
unendlich, heisst er diskret.
S


--- PAGE 63 ---
Sei nun P() ein Wahrscheinlichkeitsmass, das 
den Axiomen genügt, dann wird jedem Elementar-
ereignis ei eine Wahrscheinlichkeit pi = P(ei) 
zugeordnet. Also:
 
 
e1 e2 
e3 
..... ei 
.....
 
 
p1 p2 
p3 
..... pi 
.....
51
Eutin
E
S
fer ez
ez
P
e
P


--- PAGE 64 ---
(1) pi 
≥ 0 für jedes i = 1,2,...
(2) ∑  pi 
= 1, weil ei ∩ ej  =  Ø   ∀i ≠ j
(paarweise disjunkt)   und   ∪ei =  S
 
 
∑  pi    =   P(∪ei)   =   P(S)   =   1
(3) P(A)
=
∑   pi 
 
 
 
d.h. die Wahrscheinlichkeit für jedes Ereignis 
A є E berechnet sich dann aus der Summe der 
Wahrscheinlichkeiten derjenigen Elementar-
 
ereignisse, die in A enthalten sind.
alle i
alle i
alle i
Th. 3
alle i
eiєA
52
Axione
i epeA


--- PAGE 65 ---
S
en
ez
em
P
P
ei
In
ETP
i
Ei
1
tm
1


--- PAGE 66 ---
S
K
Zk
ZZK
ZZZK
e
ez
ez
ey­e
ZEE.it
for
ii
a
P 1k
B
P
Zk
1
P3
P
ZZk
1
P
P
e
1
12
oct
Er
ftp.E
ty­qo
Eiqi
q
EFF


--- PAGE 67 ---
Beispiel 1.6.1:
1. Gleichwahrscheinlichkeitsraum mit m 
Elementarereignissen (Laplace-Experiment):
1
m
i
m
i=1
1
e ,
,e   ;   p
   i=1,
,m
m
1
 
p
m 
1
m
i
=


=

=

K
K
53
ios
ios


--- PAGE 68 ---
2. (abzählbar unendlicher Ereignisraum)
Zufallsexperiment: “Man werfe eine Münze 
solange, bis 'Kopf' erscheint“.


( )
(
)
1
2
3
4
5
......
1
2
3
i
i
i
i
i
i
i
alle i
i=1
i=1
i=1
S 
 K,ZK,ZZK,ZZZK,ZZZZK,…
1
1
1
p
P K
, p
P ZK
, p
,… 
2
4
8
1
p
P ZZ.....ZK
2
1
1
p  = 
p  = 
 = 
=1, unendliche Reihe, 
2
2
die aber endlich bleibt und kon
e
e
e
e
e



=
=
=
=
=
=


=
=















14 2 43
vergiert!
54
0
If
2


--- PAGE 69 ---
Beweis durch Eigenschaften der Reihen 
oder Visualisierung der Konvergenz:
Fläche des 
Quadrats = 1
55
1
1  = p
2
2
1  = p
4
18
116
......
......


--- PAGE 70 ---
Stetige Wahrscheinlichkeitsräume
 
56
Ist ein Ereignisraum S nicht abzählbar, heisst er stetig.
Ist ein Ereignisraum S nicht abzählbar, heisst er 
stetig.
Definition (Stetiger Wahrscheinlichkeitsraum)
Bsp
S
IR
oder
S
Taib
acb


--- PAGE 71 ---
Beispiel 1.6.2:
 
Man denke an ein Geradenstück zwischen Null und 
Eins (abgeschlossen).
Auf diesem Geradenstück befinden sich Punkte, die 
einzeln betrachtet keinerlei Ausdehnung und damit 
die Länge Null haben. Allerdings sind es unendlich 
viele Punkte.
1
0
57
É


--- PAGE 72 ---
Nun denken wir uns ein Zufallsexperiment, welches 
darin besteht, dass aus dem abgeschlossenen 
Intervall [0,1] der reellen Zahlen eine Zahl 0 ≤ a ≤ 1 
zufällig gewählt wird.
In jedem Fall ist dann
 
Definieren wir drei Teilintervalle als Ereignisse:
 
( )


P a =0    a
S= 0,1 .







A = a | a 
 0.4
B = a | 0.6 
 a 
 0.9
C = a | a 
 0.8




58


--- PAGE 73 ---
e
o
o
o
dy t.od.si
Intuit
1
P
A
0.4
Laplac's she
statige
P B
0.3
Gleichwahrscheinlichler
PCC
0.2
PLA
fer
alle
es
existient
es
existierteincindenti.ge
iP
Bnc
Lag
0.1


--- PAGE 74 ---
Intuitiv:
( )
( )
( )
( )




( )
( )
Th.5
P A  = 0.4;  P B  = 0.3  und  P C  = 0.2.   Warum?
 S ist ein Laplacescher stetiger Gleichwahrscheinlich-
Länge A
      keitsraum:   P A  = 
; 
Länge S
Länge B
C
0.1
      P B
C =
 = 
 = 0.1
Länge S
1
      P B
C = P B +P C
→



−

P B
C =0.3+0.2-0.1=0.4

0.5
0.9
0.8
0.6
1
B
C
0
59


--- PAGE 75 ---
Bemerkung:
Die Intervalle können offen oder abgeschlossen sein. Es 
gilt:
auch wenn B+ zwei Elementarereignisse mehr hat;
aber die Wahrscheinlichkeit beider Randpunkte ist “vom 
Mass Null“.


(
)
( )
+
+
B  = a | 0.6 
 a 
 0.9  
 P B
 = 0.3 = P B ,


→
60


--- PAGE 76 ---
2. Definiere
(
)


(
)


(
)
(
)
(
)


(
)
2
2
 Im Rechteck einbeschriebener Kreis
S = 
a,b  | 0 < a < 3  und  0 < b < 2
D = 
a,b  | 0 < a < b < 2
K = 
a,b  | a-2
+ b-1 < 1
       
61
K
D
S
a
b
IP D
I
A
0.52


--- PAGE 77 ---
( )
( )
Fläche von D
2
1
      P D  = 
 = 
 = 
Fläche von S
6
3
Fläche von K
      P K  = 
 
 
 
 0.5236
Fläche von S
6

=

62


--- PAGE 78 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.9


--- PAGE 79 ---
1.7. Bedingte Wahrscheinlichkeit und       
stochastische Unabhängigkeit
64


--- PAGE 80 ---
Beispiel 1.7.1:
Einfacher Würfelwurf:
Wie gross ist die Wahrscheinlichkeit eine “6“ zu
bekommen?
Und wenn wir erfahren, dass die gewürfelte 
Augenzahl eine gerade Zahl ist?
(
)
1
P "6"  = 
6
→
(
)
1
P "6"  Augenzahl ist gerade  = 
3
65
O


--- PAGE 81 ---
Maschine
Production
defecheare
Mr
10
Mz
7
IP stuck defeat Me
0.1
IP Stuckdefekt
Mz
0.07
IP
stuck out Myproduciet
PC
Mz
i
P
stuck defeat
A
And's
Mz
My
A
AnMz
P
An
M
P AIM
P Mi
5
1,2
117
IP A
P Anm
P AnMa


--- PAGE 82 ---
Seien
A
B
Zwei
Eveignisse
eines
gegebenen­wahrscheintichheitsrams.D.ie
bedingle
at
utero
Bearing
AIB
FEET
fate
P
B
0
Be
For
P
B
0
ist
P
AIB
mightiest


--- PAGE 83 ---
66
Seien A und B zwei Ereignisse eines gegebenen 
Wahrscheinlichkeitsraums. Die bedingte 
Wahrscheinlichkeit A unter der Bedingung B ist 
definiert als
 P(AIB) =  
 
    , für P(B) > 0 
und undefiniert für P(B)=0
Definition (Bedinge Wahrscheinlichkeit)
P(A∩B)
   P(B)


--- PAGE 84 ---
Beispiel 1.7.2:
Ein roter und ein grüner Würfel werden geworfen. 
Wie gross ist die Laplacesche Wahrscheinlichkeit, 
dass mindestens einer der beiden Würfel eine Sechs 
zeigt unter der Bedingung, dass die Augensumme 
grösser als neun ist?
67
strikt
A
mindestens
eine
6
B
Augen summe
9
Genet
IP
AIB
P
s


--- PAGE 85 ---
B
A
mindestens
eine
6
B
Augensumme
9
Genet
IP
AIB
P
s
P
A
IP
B
1
P
An B
IP
AIB
56s


--- PAGE 86 ---
(1,1) (1,2) ...... ......  ...... (1,6)
 
(2,1) (2,2) ...... ......  ...... (2,6)
 
(3,1) (3,2) ...... ......  ...... (3,6)
 
(4,1) (4,2) ......  ...... (4,5) (4,6)
 
(5,1) (5,2) ......  ...... (5,5) (5,6)
 
(6,1) (6,2) ...... (6,4) (6,5) (6,6)
(
)
(
)
( )
 B
P A
B
5/36
5
 P A
=
=
P B
6/36
6


=
( )
11
A = "mindestens eine Sechs": P A
36
=
( )
(
)
  
6
5
B = "Augensumme > 9": P B  = 
und  P A
B
36
36

=
68


--- PAGE 87 ---
69
Die Wahrscheinlichkeit, dass zwei Ereignisse A und B 
gleichzeitig eintreten, beträgt
 
P(A ∩ B) = P(A) P(B I A) 
oder
P(B ∩ A) = P(B) P(A I B).
Theorem 7 (Multiplikationssatz)
IP
AIB
t


--- PAGE 88 ---
3R
1B
PCratrel
R
i
the
getogene Angel
ist
not
5
1,2
E
44


--- PAGE 89 ---
Beispiel 1.7.3:
Eine Urne enthält 4 Kugeln, nämlich 3 rote und 1 blaue. 
Wie gross ist die Wahrscheinlichkeit, beim Ziehen von 2 
Kugeln (ohne Zurücklegen), 2 rote zu erwischen?
Sei Ri  = “i-te gezogene Kugel ist rot“  für  i=1, 2. Dann ist




1
2
1
2
1
2
3
2
1
P R
R
 = P R
P R
 = 
.
R
2
4
3



=




70


--- PAGE 90 ---
72
Sind zwei Ereignisse A und B unabhängig, dann ist 
P(A∩B) = P(A) P(B).
Theorem 8 (Multiplikationssatz für unabhängige Ereignisse)
Ben
AIBunabh.IR
AIB
PlAI
111
P t
Papers
plans


--- PAGE 91 ---
Zeigen Sie
A
B
sind
mabhang
PCA
PCB
An B
KK
IP
AAB
IP
An B
IP A
PCB
AB
math


--- PAGE 92 ---
Beispiel 1.7.4:










( )
( )
Eine Münze wird zweimal geworfen.
S= ZZ, ZK, KZ, KK ,  Laplace-Modell.
Für die Ergebnisse:   A= Kopf beim 1. Wurf  = KK, KZ
                                  B= Kopf beim 2. Wurf  = KK, ZK
2
1
ist P A =P B =
  und 
2
4 =
(
)


(
)
( )
( )
 
1
P A
B
P
KK
=P A
P B .
4
Also sind A und B unabhängig (wie erwartet).

=
=

73


--- PAGE 93 ---
Merke: 
Stochastische Unabhängigkeit ist keine transitive 
Relation!
Aus "A und B unabhängig" und  "B und C 
unabhängig" folgt nicht "A und C unabhängig"!
Beispiel: 
Man würfle mit zwei Würfeln. Sei A das Ereignis 
"1,2 oder 3 beim 1. Würfel", B "4, 5 oder 6 beim 2. 
Würfel" und C "4,5 oder 6 beim 1. Würfel".
Offensichtlich ist A und B sowie B und C 
unabhängig, keinesfalls aber ist A und C 
unabhängig.
75
A
B
unabhangig
B
C
unabhangig
A
C
nicht
unable


--- PAGE 94 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.10


--- PAGE 95 ---
Ereignismenge
Bisher
Ereignisram
Wahrscheintichheitsraum
S
E
Watchithets
ECS
so
class
eE
SEE
P
AI
P
E
50.1
Ereignis
AEE
Bsp
A
2,4 6
PCA
die
wheit
class Eneignis A
cintrit
Bedingle
Wahrscheinlichheit
at
A BEE
beliebigeEveignisse
AIB
PLANT
PLANB
111
PCB
PCB
fallsP B 20
PIB
P AIB PCB
pay ppps
bedwhat
P BNA
IP BIA PLA
PIA B
BELT
PCA B
P B
P B A IPCA
10411
IPCA B
PCBLAFI
Satzes
Yates
IPCA
Zwei Ereignisse
A
BEE
heisser
unabhangig
falls
p
AIB
PCA
PLANB
P A
PIB


--- PAGE 96 ---
1.8. Totale Wahrscheinlichkeit
 
Beispiel 1.8.1:
 
Ein und derselbe Massenartikel werde auf zwei 
Maschinen gefertigt. Die schnellere Maschine M1 
hinterlässt 10% Ausschuss, produziert aber 
doppelt soviel wie die langsamere Maschine M2, 
die aber nur einen Ausschussanteil von 7% 
aufweist. Wie gross ist die Wahrscheinlichkeit, 
dass ein zufällig aus der Gesamtproduktion 
gezogenes Einzelstück defekt ist?
77


--- PAGE 97 ---
Maschine
Production
FehlerWheit
Mn
0.1
Mz
0.07
AnMn
AnMa
PCA
M
A
S
A
stuck defeat
M
Stack worde von Machine 1 getertiff
M
stick
wide
non Machine2 getertist
PCA
P
ANM
P
An Mz
R
P
t
lmzplmQ­0.1
0.07
0
09


--- PAGE 98 ---
Gegeben sind also: 
Maschine 
Produktion 
Ausschuss
 
 
 
M1 
 
2/3 
 
10%
 
 
 
M2 
 
1/3 
 
7%
P(Stück defekt | M1) = 0.1  ;  P(Stück defekt | M2) = 0.07
P(Stück auf M1 produziert) = 2/3
P(Stück auf M2 produziert) = 1/3
 
→ P(Stück defekt) = ?
2
A
M

1
A
M

2
M
1
M
A="Stück defekt"
i
i
M : "auf M  produziert"
S
78


--- PAGE 99 ---
(
)
(
)
(
)
( )
(
)
(
)
(
)
(
)
(
)
(
)
i
i
i
1
2
1
2
1
2
Wir haben:   
       P A
M
P A
P M ,  i = 1,2
M
Nach Axiom 3:
       P A  = P A
M +P A
M
 
                 = P A
P M +P A
P M
M
M
2
1
                 = 0.1
 + 0.07
0.09
3
3

=







=
79


--- PAGE 100 ---
80
Definition:
Beliebige n Ereignisse H1, H2,..., Hn, die sich 
gegenseitig ausschliessen, aber zusammen-
genommen den Ereignisraum ganz ausfüllen, 
heissen eine Aufteilung oder Partition von S. 
Eine Partition H1, H2,..., Hn erfüllt also
 
 
Hi ∩ Hj 
= 
Ø 
für 
i ≠ j 
 
und
 
 
H1 ∪ H2 ∪ ... ∪ Hn  
= 
S. 
Definition (Partition)
disjuntt
Hi
H
s
S


--- PAGE 101 ---
Theorem
Totale
Wahrscheinlichheit
Seien
Hi
Ha
Hn
eine
Partition
von S
Dann gilt fur jedesEreignis
AEE
IP
A
P
AIH
P
IP
An Hi
S
42
Ht


--- PAGE 102 ---
81
Seien H1, H2, ... , Hn eine Aufteilung von S, 
Dann gilt für jedes Ereignis A є E
 
 
P(A) = ∑  P(A I Hj) P(Hj)
Theorem 6 (Totale Wahrscheinlichkeit) 
H1
H2
H3
H4
A
Hn
...
...
...
n
j=1
Partition
IP AnHj


--- PAGE 103 ---
Ziehen
ohne Zuracklegen
PLEegzlistr.tl
Rn
i
erste
getogene Kugel ist not
Ry
erste
gezogene Kugel
blan
IP R2
Ren R
IP RenRT
EERIE Elif
R
R
S
2 4 31
R2
1
3 R2
R1
R2
R2
En
R2


--- PAGE 104 ---
Beispiel 1.8.2: 
In Beispiel 1.7.3, der Urne mit drei roten und einer 
blauen Kugel bilden R1 = {erste gezogene Kugel ist rot} 
und 
= {erste gezogene Kugel ist blau} offenbar eine
Aufteilung von S. Für R2 = {zweite gezogene Kugel ist
rot} erhalten wir also: 
1
R
(
)
(
)
(
)
(
)
(
)
2
2
1
2
1
1
1
P R
 = P R
P R
 + P R
P R
R
R
2 3
3 1
3
 
 
4
3 4
3 4


=

+

=
82


--- PAGE 105 ---
 
rot-rot
 
 
rot-blau
 
 
blau-rot
 
 
blau-blau (unmöglich)
2
R
2
R
2
R
2
R
1
R
1
R
Bemerkung: Überlegungen wie im Satz der totalen 
Wahrscheinlichkeit werden oft in 
einem Baumdiagramm dargestellt. 
83


--- PAGE 106 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 8.11


--- PAGE 107 ---
Bayes Theorem
Seien
He
Hn irgendeine
Partition
von
S
Sei
B
ein Eveignis
BEE
so
dass
IP
B
0
Dann
gilt for jedes H
P
H
B
PCB
Hi
PCH
PCB Hj
P Hp
Satz de tot Wheit
Beweis
Nene
i 11 1 1
4
PCB
P
H
B
it
SIYA.si
i eitiii


--- PAGE 108 ---
85
1.9. Das Bayes-Theorem
Sei H1,...,Hn irgendeine Aufteilung von S und 
sei B ein Ereignis mit P(B) > 0, dann gilt für 
jedes Hi
 
Bayes-Theorem:
=
=

1
(
|
) (
)
(
|
)
(
|
) (
)
i
i
i
n
j
j
j
P B H P H
P H
B
P B H P H


--- PAGE 109 ---
P stuckaufMAprodutient
IP stack auf M1 producient stuckdefelot
IPCH
B
H
stuck
words
on
My getertist
H2
stuck
Ma getertiff
H2HT
Ha Ha
ist
eine Partition
B
stuck defelot
IP H
I Fin
p
Ahnan
i i
i
IP
Hn B
0.74
PRO
0.09
Folien zavor


--- PAGE 110 ---
Beispiel 1.9.1:
Im Beispiel 1.8.1 ist die Wahrscheinlichkeit, dass ein 
zufällig aus der Tagesproduktion herausgegriffenes 
Stück von Maschine M1 produziert worden ist, a-priori
 
Beobachtet man allerdings, dass das Stück defekt ist, 
wird man die Wahrscheinlichkeit sicher höher 
einschätzen wollen, da Maschine M1 ja mehr Ausschuss 
produziert.
=
=
2
(Stück auf M1 produziert)
0.66.
3
P
86


--- PAGE 111 ---
Beispiel 1.9.1 (Fortsetzung):
Nach der Bayes-Formel errechnet man
In der Bayes-Statistik heissen H1,...., Hn alternative 
Hypothesen, P(Hi) die a-priori-Wahrscheinlichkeit der i-ten 
Hypothese und P(Hi|B) die a-posteriori-Wahrscheinlichkeit 
der i-ten Hypothese nach Kenntnis der Beobachtung B.
(
)
2
0.1
3
 
 
|  
 
 
1
2
1
0.1
 
 0.07
3
3
20  
 0.741
27
P Stück auf M
Stück defekt


+

=
=
=
87


--- PAGE 112 ---
Beispiel 1.9.2:
Bei einer Krankheitsdiagnose sind die folgenden 
Angaben bekannt: 
 
➢ Von den kranken Personen werden 90% durch die 
Untersuchung entdeckt.
➢ Von den gesunden Personen werden 99% durch 
die Untersuchung als gesund eingestuft.
➢ In der gesamten Bevölkerung sind 0.1% krank.
88


--- PAGE 113 ---
IP
Hn
B
1
Definiene Ereignisse
Ha
Person
ist
Frank
B
Person
wind
als
krank diagnostiziest
P
B
H1
org
P B
Hp
0
1
P
B HT
0
99
P
B
F
0.07
IP
Hr
0
007
IP HT
0
999
PCB
tree
0
01
089
Bayes
IP
HIB
0.0
0 07089
Lei
Zuverlassiger
Test


--- PAGE 114 ---
Beispiel 1.9.2 (Fortsetzung):
Nun wird eine Person aus der Bevölkerung 
herausgegriffen, untersucht und als krank 
eingestuft. Wie wahrscheinlich ist es, dass das 
stimmt?
 




=
1
Sei:    H
Person ist krank ; 
          B = Person wird als krank diagnostiziert
(
)
(
)
(
)
(
)
→
=
=
=
=
1
1
1
1
   P B
0.9 ; P B
0.1 ; 
H
H
                  P B
0.99 ; P B
0.01
H
H
         
89


--- PAGE 115 ---
(
)
(
)
( )
(
)
(
)
(
)
(
)
(
)
(
)
(
)
( )
1
1
1
a-priori-
Wahrscheinlichkeit
1
1
Satz der totalen
Wahrscheinlichkeit
1
1
1
1
Bayes-Theorem
P B
P H
H
P H
P B
B
 P H
0.001   und   P H
0.999
 P B
P B
P H
P B
P H
H
H
                 
0.9 0.001 + 0.01 0.999 = 0.01089   und
   

=
→
=
=

=
+
=





0.9 0.001
0.0826,
0.01089
                  d.h. die Vertrauenswürdigkeit einer 
                  Krank-Diagnose ist nur 8.3%.

=
=
90
Beispiel 1.9.2 (Fortsetzung):


--- PAGE 116 ---
Baumdiagramm:
B
B
Diagnose
Zustand
B
B
1
H
1
H
91


--- PAGE 117 ---
Simpson's
Paradox
Hospital 1
Hospital 2
Hn
Hz
success
Ah
PCS AH2
success
Treatment 817
0.93
Treatment
1
3
0.73
Treatment 23
0.87
Treatment
0.5
Fur
beide
Spitaler
scheint
Behandlung
A
besser
81
192
PCS
A
87
263
233
30
0.78
234
55
2
0.83
P
S
B
270
80
Intuitive
Enkiang


--- PAGE 118 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 7.1


--- PAGE 119 ---
2. Elementare Kombinatorik
93


--- PAGE 120 ---
n
for
new
ist
definientals
IIIIIIIII.IE
The 0
1
Definition
Binomialboefficient
E
FIE
ist der Binomialkoefficient for
gaze Zahlen
no
610
net
E
k aus n
beschneibt wieviele
Moglichleitenes gibt aus n Objelsten
k austuwahon
ohneZurichlegen
ohneReihenfolge
Bsp
n
5
Personen
A B C D
E
Wie viele
2
er
Teams
lassen
sich
bilden
A B
CAC1
A D
A E
B C
BD
Bie
C D
C E
D
E
10 Moghchleiter
t


--- PAGE 121 ---
94
 
Definition
Das Symbol n!  bezeichnet das Produkt der natür-
lichen Zahlen von 1 bis n
 
  
 
und heisst n – Fakultät. 
 
Zusätzlich ist festgelegt: 0!= 1
 
n! 1 2 3
(n-1) n
= 


L
2.1. Fakultät und Binomialkoeffizient


--- PAGE 122 ---
Definition
  Der Binomialkoeffizient  
ist für ganze n > 0,
  ganze k ≥ 0 und n ≥ k definiert als 
 
 
 
n
k
n
k
=
n!
k! (n – k)!
95


--- PAGE 123 ---
f
gibtan
wievicle
Moghichheiten
es gibt
aus
n objekten genay
austunchlen
ohne Reihenfolge
ohneZurietlegon
Beispiel
n
5
Personen
A B C D E
Frage
wie
viele
verschiedene
2
er teams
lassen sich bilden
E
a
to
CAB1
CAC1 CA D
CA EI
Bic
B D
Biel
CID
CIE
DIET


--- PAGE 124 ---
2.2. Das Fundamentalprinzip der 
Kombinatorik
 
 "Wenn ein Sachverhalt auf n1 Arten erfüllt
 
werden kann und ein zweiter Sachverhalt 
                            unabhängig davon auf n2 Arten, so ist
 
die Gesamtzahl der Möglichkeiten, gleichzeitig
 
beide Sachverhalte zu erfüllen, gerade gleich
 
dem Produkt n1  n2.''
97


--- PAGE 125 ---
Def
Gegeben
eine Merge
von
n Elementen
Jede
Zusammenstalling dieser
n Element
in
ingendeiner Reitenfolge heist
Permutation
dieser
n
Elemente
Bsp
A B
C
ABC
ACB
BAC
BCA
CAB
CBA
Q
was
ist
die
Anzahl
der
Permutation
on
n
Elemarten
n


--- PAGE 126 ---
99
Definition
 
 
Gegeben sei eine Menge mit n Elementen. 
 
Jede Zusammenstellung all dieser Elemente 
in irgendeiner Reihenfolge heisst eine 
Permutation dieser n-Elemente.
2.3. Permutationen
Permutations
n
Bse ABIC
ABC
ACB
BAC
B.CA
CAB
CBA


--- PAGE 127 ---
Def
Gegeben
seicine
Merge
mit
n
verschiedeen
Elementen
Jede
Zusamman stelling
von
to
Elementen
daraus
heist
Kombination
k
ter
Orduing
Kombination
mit
n
Beracksichting der
n
Anordning
I
Combination ohne
Beridesidtin
der trouding


--- PAGE 128 ---
101
Definition
Gegeben sei eine Menge mit n verschiedenen 
Elementen. Jede Zusammenstellung von K 
Elementen daraus heisst Kombination K-ter 
Ordnung aus diesen Elementen.
➢ Kombinationen mit Berücksichtigung der    
Anordnung
➢ Kombinationen ohne Berücksichtigung der 
Anordnung
2.4. Kombinationen
Me


--- PAGE 129 ---
Def
Gegeben sei
eine Merge mit
n
Elementen
Wiewiele
Sequenter der Lange
on
Kann
man
mit
n Symbolen
bilden
Diese
Anzahl
heist
Ansah
n
action
n
n
mm
1 Stelleselle
mitestake


--- PAGE 130 ---
103
Definition
Wie viele Sequenzen der Länge m kann man 
mit n Symbolen bilden?
Diese Anzahl heisst die Anzahl der 
Variationen (mit Wiederholung) und ist 
gegeben durch nm .
2.5. Variationen


--- PAGE 131 ---
3. Zufallsvariablen
105


--- PAGE 132 ---
Wahrscheinlich
Definition
Gegeber
sei
ein
theits ft
walrscheinlichleitsraum
if
P
Ereignisram Ereignismange
Eine Funttion
S
R
S
51.52.53
s
to
s
die jeden Elementarereignis
SE S
eine
qq.ggygyqqqg
g
a
g
Eneignisram
P
E
0
1


--- PAGE 133 ---
Definition
Gegeben sei ein Wahrscheinlichkeitsraum 
[S, E, P ()]. Eine Funktion
 
X: S  
ℝ
 
 
e  
X(e) є ℝ,
die jedem Elementarereignis e eine reelle Zahl 
X(e) zuordnet, heisst Zufallsvariable, wenn 
dabei zu jedem reellen r ein Ereignis Ar є E 
gehört, mit Ar = {e | X(e) ≤ r}.
106


--- PAGE 134 ---
x є ℝ
S








-1
0
1
2
3
107


--- PAGE 135 ---
Beispiel 1
Warfel
S
1
2,3
4 5,64
gewirtette
Angentahl
s
s
fer
alle SES
Beispiel 2
Werfen
einer
Mine
S
Kiz
Definies
Z
0
1 7
1


--- PAGE 136 ---
Beispiel 2
Werfen einer Mine
S
Kiz
Definies
Z
0
k
1
E
Kiz
K
Z
P
on
Behauptung
Sei
rer
d
Ar
seS
Xister
eE
Far
ocr
o
Ar
EE
For
Otrc
1
Ar
Z
eE
fav
1
rco
Ar
2,5
eE


--- PAGE 137 ---
Beispiel 3.0.1:
Beim Würfeln mit einem Würfel ist
Die geworfene Augenzahl ist eine 
Zufallsvariable, beschrieben durch die 
Funktion 


S = 1, 2,…,6 .
( )
X e  = e .
108
USE S


--- PAGE 138 ---
Beispiel 3.0.2:
Eine Münze werde einmal geworfen. X bezeichne 
die Anzahl der Köpfe. X kann nur zwei Werte 
annehmen:
Die Ereignismenge E hat die vier Ereignisse: 
Es gilt: 
(
)
(
) =
X Zahl = 0   und  X Kopf
1.



E = 0, Zahl, Kopf, S .




=

=


=
r
r
r
für   -  < r < 0    ist   A
0
für    0 
 r < 1   ist   A
Zahl
für    1 
 r <   ist   A
S
109
S
Zahl Kopf


--- PAGE 139 ---
Beispiel 3.0.3:
Zwei regelmässige Würfel werden geworfen. Der 
Ereignisraum besteht aus 36 
Elementarereignissen:
Mehrere Zufallsvariablen lassen sich hier bilden:
(
)


S = 
i, j | i=1,...,6, j=1,...,6 .
(
)
(
)
X = "Summe der Augenzahlen":     
       X i, j  = i+j = x,   x = 2, 3,…,12.
Y = "Differenz der Augenzahlen":
       Y i, j  = i-j  = y,  y = 0, 1,…,5.
110
Ziel
Ausblict
p
x
A
to


--- PAGE 140 ---
Definition
Sei W die Menge, die gerade jene reellen Zahlen enthält, die 
durch die Abbildung erreicht werden. 
W heisst Wertevorrat.
Hat der Wertevorrat W ⊂ ℝ einer Zufallsvariablen X endlich 
viele oder abzählbar unendlich viele Werte heisst sie 
diskret.
Besteht der Wertevorrat W ⊂ℝ einer Zufallsvariablen X aus 
der ganzen reellen Achse oder aus Teilintervallen so heisst 
sie stetig 
(überabzählbar unendlich viele Werte).
111
SET
a
o
WEIR


--- PAGE 141 ---
Beispiel 3.0.5: Umsatz bei unsicherer Auftragslage
Der Umsatz eines Unternehmens ist bestimmt durch 
das Auftragsvolumen weniger Grossaufträge.
Für das nächste Jahr hofft die Geschäftsleitung, drei 
Grossaufträge A, B und C zu erhalten, wobei sie die 
Erfolgsaussichten bei der Akquisition unterschiedlich 
einschätzt (Aufträge stochastisch unabhängig).
113
Was
ist
die
Wheit
das
die
Firma
Auttrag C
bekommt
Auttrage A and B jedoch nicht


--- PAGE 142 ---
we
A
B
0.5
C
24
0.75
In Bnc
PEPEz
0.075
S
A B C AB AC CB
ABC
Zufallsvariable
X
Unsatz
0
Wertevornot
1
w
onion
us
dishmeteZU.­XLABC
48­P
X
24
P
C
AB
P C
IP AB
sievie
P
C
IP
An Bnc
0
075
IP AB
P
AnBnc
0.8.0.5
0.25


--- PAGE 143 ---
Beispiel 3.0.5 (Fortsetzung):
 
Auftrag 
Auftragsvolumen 
Wahrscheinlichkeit 
 
 
   [Mio. CHF] 
des Auftrags
 
 
A 
 
10 
 
0.8
 
 
B 
 
14 
 
0.5
 
 
C 
 
24 
 
0.75
Der Umsatz ist dann eine Zufallsvariable X, die von 
der unsicheren Auftragslage abhängt. 
Wertevorrat von X ?
 
(
)
(
) (
)


=
−

−

=
A 
 B 
 
1 0.8
1 0.5
0.75 
 0.075
P
C
114
Unsatz


--- PAGE 144 ---
Auftragslage 
Umsatz 
P({ei}) 
 
 
 (ei) 
 
 (X(ei)) 
 
 
 
− 
 
  
0 
0.025
 
 
A 
 
  10 
0.1
 
 
B 
 
  14 
0.025
 
 
C 
 
  24 
0.075
 
 
AB 
 
  24 
0.1
 
 
AC 
 
  34 
0.3
 
 
BC 
 
  38 
0.075
 
 
ABC 
 
  48 
0.3
 
 
 
 
                        Σ = 1 








W = 0, 10, 14, 24, 34, 38, 48
 P X=24
P 
C, AB
0.175.
→
=
=
115


--- PAGE 145 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.1


--- PAGE 146 ---
Definition
Die Funktion F(x) = P(X ≤ x) = P({e|X(e) ≤ x}), die 
jeder reellen Zahl x die Wahrscheinlichkeit 
zuordnet, mit der die Zufallsvariable X einen Wert 
X ≤ x annimmt, heisst Verteilungsfunktion von X.
117
3.1. Verteilungsfunktion


--- PAGE 147 ---
Def
Sei
X
eine Zufallsvariable
Dann
rennen
wir
die
Funktion
F
IR
oil
definiest a's
gyhqygqypqg.gg
BSP
X
Keeffenkmanzwart
P X
0
PK
1
0
2
0
Floc
04
1
1
221
1
FG
or
a
0
DX
0
7


--- PAGE 148 ---
W
heitsraum
Wiederholing
S
E
IP
Zufallsvariable
X
S
IR
W
wer
seS
Xls
w
Wertevorrat
W
endlicho.abzahlbar
Xdisk.net
W
unabiallbar
stetig
Verteilungsfunktion
F
IR
0
1
Flat
P
X I
P
seS
Xis
x


--- PAGE 149 ---
Beispiel 3.1.1: 
X = “Anzahl der Köpfe beim einfachen Münzwurf“. 
X kann die beiden Werte 0 oder 1 annehmen. Wenn 
die Wahrscheinlichkeit, Kopf zu werfen, 0.5 beträgt, 
dann gilt:
  0    ,   für   x < 0
1
F(x)  = 
   ,   für   0 
 x <1
2
  1    ,   für   x 
 1







1
0
½
1
F(x)
118
o_0


--- PAGE 150 ---
Beispiel 3.1.2: 
Zwei Würfel werden geworfen. Y sei die Differenz der 
Augenzahlen.
Dann: 
 Y = 0 , wenn (i, j) = (1,1); (2,2); (3,3); ...; (6,6) :  6 Paare
 Y = 1 , wenn (i, j) = (1,2); (2,1); ... 
:  10 Paare
 Y = 2 , wenn (i, j) = (1,3); (3,1); (2,4); ... 
:  8 Paare
 Y = 3 , wenn (i, j) = (1,4); (4,1); ... 
:  6 Paare
 Y = 4 , wenn (i, j) = (1,5); (5,1); (2,6); (6,2) 
:  4 Paare
 Y = 5 , wenn (i, j) = (1,6); (6,1) 
:  2 Paare
  
  
  
  36 Paare
119
Y diskete Z U
W
0 1,21
5
5
24117,412
W
wer
Ise S
Y
s
w


--- PAGE 151 ---












6
6
1
1
Also:   P Y
0
;   P Y
3
 
 
;
6
6
36
36
10
5
4
1
           P Y
1
;    P Y
4
 
 
;
9
36
18
36
8
2
2
1
           P Y
2
;     P Y
5
 
 
;
18
36
9
36
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
120
Beispiel 3.1.2 (Fortsetzung): 
8


--- PAGE 152 ---
( )
F y

=
 0 
, y < 0
  1/6 , 0 ≤ y < 1
  4/9 , 1 ≤ y < 2
  2/3 , 2 ≤ y < 3
  5/6 , 3 ≤ y < 4
17/18 , 4 ≤ y < 5
 1 
, y     5

121
P Y
y
for
1
yc2
ii
g
Yes
yo
O
Y
P Y
1 2


--- PAGE 153 ---
1
0
2/3
F(y)
1
1/3
2
3
4
5
122
Fly
F
1
41g
Fly
P
Y
y
41g
y
Fly


--- PAGE 154 ---
Eigenschaften der Verteilungsfunktion:
(1) F(x) ist an jeder Stelle x zumindest rechtsseitig 
 
stetig: 
lim F(x + Δx) = F(x).
(2) F(x) ist überall monoton steigend: 
 
  
 
F(a) ≤ F(b)   für   a < b.
(3) F(x) hat die Grenzwerte: 
 
  
 
lim F(x) = 0 und lim F(x) = 1.
Δx0
x ∞
x+∞
123


--- PAGE 155 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.2


--- PAGE 156 ---
125
Definition
 
Ist X eine diskrete Zufallsvariable, dann heisst die Funktion
f(x) = P [X=x] die Massenfunktion der Zufallsvariablen X.
Natürlich hat sie nur an den Stellen x=xi, die zum
Wertevorrat W gehören, positive Werte pi = P(X = xi). 
Dazwischen ist sie Null:
,
( )
(
)
0
.
i
i
p
x
x
W
f x
P X
x
sonst
=

=
=
= 
3.2. Diskrete Zufallsvariablen
0
812 eiE
p
fail
to
ti
p
P X xp
1
Pies


--- PAGE 157 ---
Jede Massenfunktion f hat die Eigenschaften:
 
(1) f(xi) ≥ 0 
(Wahrscheinlichkeit nicht 
 
 
 
negativ!)
 
(2) ∑ f(xi) = 1 (Sicheres Ereignis hat 
 
 
 
Wahrscheinlichkeit 1)
Aus (1) und (2) folgt unmittelbar (3) f(xi) ≤ 1.
alle i
126
01


--- PAGE 158 ---
(
)
( )
( )
( )
(
)
( )
( )
( )
(
)
( )
( )
( )
( )
Diese ist bei diskreten Zufallsvariablen
zu modifizieren:
P
 = F
F
f
a < X < b
b
a
b
P
 = F
F
+f
a
X
b
b
a
a
P
 = F
F
+f
f
a
X < b
b
a
a
b
−
−
−


−
−

Merke: 
Für Intervall-Wahrscheinlichkeiten gilt die folgende 
Grundformel:
(
)
( )
( )
( )
i
i
a x
b
P
 = F
F
p x
a<X
b
b
a


−
=


127
ab
o
3
P Cats
Beguid.mg one
a b
o
a
v
a b
PE
PÉ
F b
P X
b
P X
a
Fla


--- PAGE 159 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.3


--- PAGE 160 ---
Definition
 
 
Ist X eine stetige Zufallsvariable mit der
Verteilungsfunktion F, so heisst die erste
Ableitung von F nach x
f(x) = 
F(x)
die (Wahrscheinlichkeits)dichtefunktion
von X.
d
dx
129
3.3. Stetige Zufallsvariablen
Fundamet
sat
pal
f s ds
derAnalysis


--- PAGE 161 ---
Jede Dichtefunktion hat die Eigenschaften:
(1) f(x) ≥ 0 
(Verteilungsfunktion monoton 
 
  
 steigend!)
(2) ∫f(x)dx = 1 
(Die Fläche unter jeder Dichte- 
 
  
 funktion ist genau 1.)
∞
∞
130
8


--- PAGE 162 ---
Beispiel 3.3.1:
Eine stetige Zufallsvariable X sei charakterisiert 
durch die Verteilungsfunktion
( )
(
)
3
           0            ,   x < 0
1
F
1   ,   0 
 x < 3
x
x - 3
27
          1             ,   x 
 3
=
+






131
O
O
KCO
f
x
27
6 3
o
xc3
0
220


--- PAGE 163 ---
Berechnung der Dichtefunktion: 
(F(∙) in allen drei Teilstücken ableiten)
( )
(
)
2
           0     ,   x < 0
1
f
   ,   0 
 x < 3
x
x - 3
9
          0      ,   x 
 3
=






132
Beispiel 3.3.1 (Fortsetzung):


--- PAGE 164 ---
0
1
2
3
0.5
1
F(x)
0
1
2
3
0.5
1
f(x)
x
4
x
0.2593
133
Beispiel 3.3.1 (Fortsetzung):
so
Bei statisen ZU
P
X
c
0
gilt
for alle
s


--- PAGE 165 ---
(
)
( )
( )
(
)
3
Wie gross ist P
F
F
1
X
2
2
1
1
7
2
1
0.2593
1
27
27
27
=
−
=


−
−
+ −
=
=
+






(
)
2
2
1
2
3
1
1
oder      P
 
(x-3)  dx
1
X
2
9
1
8
7
1
            
.
(x
3)
27
27
27
27
=
=


−
−
=
−
=
−







Beide Rechnungen führen selbstverständlich zum 
gleichen Ergebnis. Die Information enthalten in 
F(∙) und f(∙) ist dieselbe!
134


--- PAGE 166 ---
Sei
X
eine
statise
Z.U
and
sei
acb
beliebig
Damn gilt
P EE.gg
fees doc
webei
fl
die
Dichte
Funktion
von
ist


--- PAGE 167 ---
Beispiel 3.3.2: Wartezeit an der S-Bahn Station
An einer S-Bahn Station fahren die Züge im
12-Minuten Takt. Ein Fahrgast, der den Fahrplan 
ignoriert, erscheint zu einem zufälligen Zeitpunkt an 
der Station.
X = Wartezeit 
Zufallsvariable mit Wertevorrat W = [0,12] (Minuten)
135
fetermiform
verteilta
Z V
Dichlefenbtion fat
2
keton
soust
Fec
Éf
9 dy
keto
12
Frage
G
EEj
cx
Edy
E1
yl
Bereae


--- PAGE 168 ---
( )


1
   ,   für   x
0, 12
12
Dichtefunktion: f x
 0       ,   sonst.

=


( )
Verteilungsfunktion: 
  0     ,   x < 0
x
F
  ,   0
x
12
x
12
  1     ,   x > 12
=







x
x
0
0
1
x
u
12
12
12
du =
=


⎯⎯





(
)
(
)
(
)
(
)
( )
10
Dann:   P
F
F
1
0.1667
10 < X < 15
15
10
12
9
             P
1 F
1
0.25
X > 9
9
12
=
−
=
−
=
=
−
=
−
=
136
uniformVerkiling
d


--- PAGE 169 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.4


--- PAGE 170 ---
Tonkin
Sei
eine
2
V
Sei
f
ihre
Massenfunktion
Dichtefunktion
Der
Enwartingswert
von
ist definient as
yyg.gg
jyygqgfggP
Satz
Zentraleignschaft
Y
Zentriest
form
von
EXEILL
Be
Ubungsaufgabe


--- PAGE 171 ---
Definition
Sei X eine Zufallsvariable und f ihre Massen- 
bzw. Dichtefunktion.
Ihr Erwartungswert ist definiert als
 
E[X] = ∑ xjf(xj) = ∑ xjpj,       falls X diskret
 
E[X] = ∫ xf(x)dx,                 falls X stetig. 
∞
∞
alle j
alle j
138
3.4. Erwartungswerte von Zufallsvariablen 


--- PAGE 172 ---
139
Satz:
Der Erwartungswert der Abweichungen jeder 
Zufallsvariablen X von ihrem Erwartungswert 
μx ist Null, das heisst
 
E[X - μx] = 0 (Zentraleigenschaft).
Der Erwartungswert ist als Lageparameter der 
Verteilung anzusehen.


--- PAGE 173 ---
Beispiel 3.4.1:
( )

X  Zufallsvariable mit Massenfunktion  
x
 ,  x = 1, 2
3
     f x
 0   ,   sonst.
1
2
5
 E
1
2
X
3
3
3
=

= 
+

=


140
EE
Exifer
1 EEt2fE


--- PAGE 174 ---
Beispiel 3.4.2:
( )

i
Y = "Differenz der Augenzahlen zweier Würfel"
Wir erhalten:
   Y          0         1        2         3         4         5
6
10
8
6
4
2
f
     
     
     
     
     
     
y
36
36
36
36
36
36
1
5
2
 E
 
 0
 + 1
 + 2
 
Y
6
18
9

=



1
1
1
+ 3
 + 4
 + 5
 
6
9
18
5 + 8 + 9 + 8 + 5
35
                 = 
 = 
18
18



141
Absoute
disbrete 2.6


--- PAGE 175 ---
Bp
Sei
X
stetig
mit Dictefenttion
fix
c
1
43
0
soust
Itman
Jada
c
EEZ
1
c
X
fFndx
4
x doc
F
1
3


--- PAGE 176 ---
Sei
X
Zufalls variable
Sei
g
IR
R
ziel
IE
g X
pefe.cm
Emel
X disbret
IE
g x
P
stating
IE g x
gas fade


--- PAGE 177 ---
Beispiel 3.4.3:
( )

( )
3
3
2
3
1
1
X stetige Zufallsvariable mit Dichtefunktion
1  x   ,   1
x
3
4
    f x
  0       ,   sonst.
1
1
 E
x  f
dx
x dx
x
X
x
4
12
27
1
26
13
              
.
12
12
6

−


=

=

=
=
−
=
=
=




142


--- PAGE 178 ---
Erwartungswert einer Funktion von 
Zufallsvariablen
Zuweilen gilt es, anstelle von E[X] mit gegebener 
Massen- oder Dichtefunktion f(x) den Erwartungs-
wert einer Funktion g(X) zu berechnen:
 
E[g(X)] = ∑ g(xj)  pj 
, falls X diskret
 
E[g(X)] = ∫ g(x)  f(x)dx 
, falls X stetig.
alle j
ℝ
143


--- PAGE 179 ---
Beispiel 3.4.4:
Beim 
Betrieb 
einer 
Produktionsanlage 
treten 
Störfälle 
auf. 
Aus 
Beobachtungen 
in 
der 
Vergangenheit hält man für X="die Anzahl der 
Störfälle pro Tag" folgende Wahrscheinlichkeits-
verteilung für korrekt:
   X       0       1       2       3
f( )  0.35   0.4   0.15   0.1
x
144
g EEX
IE g
X
g
0
0.35 9111 0.4
g 2
0.15
9 3
0.1
II
was
sind
die
erwarteter
Kosten
0
Massenfunktion
Kosten
als function der
Stouta
g
x
5
1
in tausend CHF


--- PAGE 180 ---
Beispiel 3.4.4 (Fortsetzung):
Mit der Behebung der Störfälle sind Kosten
gemäss 
(in tausend CHF)
verbunden.
( )
4
g
 
 5
x
x
1
=
−
+


(
)
Würde man
E
 0 0.35 
 1 0.4 
 2 0.15 
 3 0.1 = 1
X
in die Kostenfunktion einsetzen, erhielte man 
erwartete Kosten in Höhe von 
4
                     g
5
3.  
E X
1 1
=

+

+

+

=
−
=
+
145


--- PAGE 181 ---
Beispiel 3.4.4 (Fortsetzung):
Aber:
 
Korrekt müssten wir jedoch die erwarteten 
Kosten als
( )


( )
( )
( )
( )
E g X
     
g
0.35 
 g
0.4 
 g
0.15 
 g
0.1
0
3
1
2
11
     
1 0.35
3 0.4
0.15
4 0.1 
3
     
2.5 
berechnen. 
=

+

+

+

=

+

+

+

=
146


--- PAGE 182 ---
Beispiel 3.4.5:
x
 X       0       1       2       3
f      0.1     0.3    0.2    0.4
(
)


2
y
y
Y=
 f ?         W = 0, 1, 4
X-2
→
y
 Y       0       1       4
f      0.2     0.7    0.1

 E
 
 0.7 
 0.4 
 1.1
Y
→
=
+
=
147


--- PAGE 183 ---
Sei
X
eine
Z
V
seien
a
b
ER
Éf
fy
j
Éff
jÉ
I
Linearittg x
for
gloa
axtb
des
EW
g
EIX
Lineation
Bsp
b
EEX
a
1
EIX
IEEX
EX
HB
O


--- PAGE 184 ---
Beispiel
Sei
X
eine
2
0
Mit Did
fear
é
so
px
0
Dichleflot
einer exponentien verteilter Z
V
Y
2
1
Fy
EEY
salt
EEY
2
1
2E
1
D
EIX
de
xé
F
L Elda
I
Bevi


--- PAGE 185 ---
Bisher
Massenfunktion
P X xj
flx
Wahrscheinlichteitsdiditefunktion
Dichtefunktion
g IR
IR
II gW
g xjlf xp
flappy
distret
91 1
19liafed doc
stating
Dichtefubtion
far
20
HER
174
dx
1


--- PAGE 186 ---
Satz
 
Wenn X eine Zufallsvariable mit Erwartungswert E[X] 
ist und a, b reelle Konstanten sind, dann hat
Y = aX + b 
den Erwartungswert  
E[Y] =E[aX + b]= a  E[X] + b.
 
 
148
Rechnen mit Erwartungswerten
 
 


--- PAGE 187 ---
Bet
ax
b
a IX
b
Be
Disketer Fall
I
ax
a
IX
E aX
asg f og
aEgfg
a EX
EEX
Z
EIX
IZ
X Z
aj
2 fyzlij.tt
fxizbsizi.lt
Zity
it
if
g
fxz
til
EX
ELZ
tax
b
tax
b
FEET


--- PAGE 188 ---
Beispiel 3.4.6: 
Sei X eine Zufallsvariable mit Dichtefunktion
( )



(
)

(
)
-x
x
-x
-x
-x
-x
0
0
P.I.
0
0
y
e  ,   x
0
f
                    und   Y = 2X
1
x
0   ,   sonst
         E
?           E
2 E
1
3
Y
2x + 1
X
   E
xe dx 
 
e dx = -e
1
X
-x.e
   f
?
+
+
+



=
+
=
=

+
=
=
=
+
=
=





149
statig
ñ
I
Satz
I
Dety
w
G
x


--- PAGE 189 ---
Transformation
van Zufalls variable
A
Sei
X
eine stetige
Z V
Sei
Y
g
X
Sei f 1
die
Dichtefumbtion
von
Frage
was
ist
die Dichtefkt
von Y
f
Satz
Dichtetransformation
1s
fx
gig
Bsp
Sei
goal
2
1
was ist f
for
Y glX
far
8
g
x
y
2x
1
y
1
g g
y
21
If
E
I
tyly
e
1
1
2
falls
0
soust


--- PAGE 190 ---
B
Sei
X
distret
g
IR
IR
Y
g
x
Massenfunktion fy
1a
P
X
x
tyly
IP
Y
y
fyly
IP Y y
IP
g x
y
Σ
fee
x G
xeR
g
ly
x
glo
y


--- PAGE 191 ---
Satz
 
 
 
( )
( )
(
)
( )
1
1
y
x
Unter geeigneten Voraussetzungen 
(Dichtetransformation):
dg
y
             f
f
y
g
y
dy
−
−
=

150
( )
( )
( )
{
1
y-1
2
y
 y 1
In Beispiel 3.4.6:
y-1
y
g
2x
1  
 g
 = 
 
y
x
2
1
y-1
         
 f
e
 ,  falls  
0
y
2
2
−
−


→
=
=
+
→

=


y
1


--- PAGE 192 ---

(
y-1
y-1
2
2
1
1
y-1
y-1
2
2
P.I.
1
1
y-1
y-1
2
2
1
1
Ist dies eine Dichtefunktion?
1
1
             
e
dy
2  e
1   
2
2
1
1
Also:
E
y e
dy =
2 y e
Y
2
2
              + e
dy =1+2 
e
)
3


−
−


−
−



−
−
−
=

=
→
=
−




=
−



✓
151


--- PAGE 193 ---
1
0.3
0
0 1
1.5.0
4
2
297
EX
3
3.7
4
ETX
2.8
EX
f xj


--- PAGE 194 ---
Beispiel 3.4.7:    Sei X eine Zufallsvariable mit
 
 ( )

(
)






         X          -1        0       1.5       2
        f
     0.3     0.1      0.4      0.2
x
 E
0.3
0 0.1 1.5 0.4
2 0.2
0.7
X
1
      E
E
3
3.7
X+3
X
      E
4 E
4 0.7
2.8
4X
X
      

=

+

+

+

=
−
=
+
=
=
=

=
152


--- PAGE 195 ---
Beispiel  3.4.8:  
Spieler 1 verspricht Spieler 2, ihm beim Würfelspiel 
die folgende Gewinne auszuzahlen:
10 Rappen, falls 1 oder 2 gewürfelt wird;
20 Rappen, bei 3 oder 4;
40 Rappen, bei 5; und
80 Rappen, bei 6.
Wie viel muss Spieler 2 vor jeder Runde an Spieler 
1 bezahlen, damit das Spiel fair ist?
 
 
153
Gewinn
FIX
10
20 3
408 80
302


--- PAGE 197 ---
Beispiel  3.4.8 (Fortsetzung):
“Faires Spiel“ soll dabei heissen, dass der Einsatz gleich dem 
“durchschnittlichen Gewinn“ sein soll.
Idealisierter durchschnittlicher Gewinn bei unendlich 
vielen Spielrunden ist
d.h. der “faire Einsatz“ in diesem Sinn ist 30 Rappen.
(→ Casino-Spiele sind nicht fair!)
( )
         X=x      10       20       40       80
2
2
1
1
        f
     
     
     
     
x
6
6
6
6
Gewinn

2
2
1
1
E
10
20
40
80
30,
X
6
6
6
6
=

+

+

+

=
154


--- PAGE 198 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.5


--- PAGE 199 ---
Sei
X
eine
2
U
mit
Evwaringswent
IIIX
mx
Dann
heist
x
1 12
E
Variant
von
X
Cangenomenen das integral existier t
Die Standartabweiding
von
ist
o
VVarx̅
Bemering
Var
X
E
X
EX
BETTE _EEX mx.EE
2xmxtmi
EEX2J 2MxII
IL
X
ME


--- PAGE 200 ---
Definition
Sei X eine Zufallsvariable und μx ihr Erwartungs-
wert, dann heisst
 
 
σx = V(X) = E[(X - μx)2]
Varianz der Zufallsvariablen X, falls die Summe 
bzw. das Integral existiert.
Die positive Wurzel aus der Varianz σx = + √ V(X) 
heisst Standardabweichung.
2
156
3.5. Varianz 
goal
x Mx
g x
Var x
Mx LEIX


--- PAGE 201 ---
Die so definierte Varianz einer Zufallsvariablen 
ist als ein Streuungsparameter der Verteilung 
anzusehen.
 
σx  = ∑ (xj - μx)2 pj ,     falls  X diskret
 
σx  = ∫ (x-μx)2 f(x)dx,     falls X stetig
 
2
2
alle j
ℝ
157
glacil
Var X
Var X
goal


--- PAGE 202 ---
Mx
EX
0
1
2
1
Var X
E
x
2
101121
1 11212
2 1
1
14
12
EX
02
12
22 114
1
2
1
Var X
EX
µ
12
1


--- PAGE 203 ---
Beispiel 3.5.1:
Sei X eine diskrete Zufallsvariable: 
      X = “Anzahl der Köpfe bei zwei Münzwürfen“
Dann:
( )
X            0        1         2
1
1
1
f
     
    
     
x
4
2
4
(
)
(
)
(
)
x
2
2
2
2
x
und   
1
1
1
1
1
      
0-1
1 1
2
1
4
4
2
2


=
=

+

+

=
−
−
158


--- PAGE 204 ---
Beispiel 3.5.2:
Sei wie vorher Y = “Differenz der Augenzahlen 
zweier Würfel“. (Siehe Beispiel 3.4.2)
Wir haben schon berechnet, dass

(
)
2
y
35
E
   Nun ist V
σ
2.05247
Y
Y
18
=

=
=
159


--- PAGE 205 ---
Beispiel 3.5.3:
Sei X eine stetige Zufallsvariable mit 
Dichtefunktion
( )
2
1
c
,     0 < x < 2
x
x
f
2
x
        0         ,       sonst.
−
=








160
1 Bestimue C
x Ex
dx
t
VartN
c I
FEET
IIIX
flocdoc
23 x
x
x
dac
ii
É


--- PAGE 206 ---
Var X
x
Mx
floc
doc
3
Elda
x
2
4
2
2
23
x
x2
do
Ex
2
3
Ex
x
dx


--- PAGE 207 ---
2
2
3
2
2
!
2
0
0
0
Wie gross ist die Konstante c ?
x
x
8
2
1
c
dx
c
c 2
c
1
x-
x
2
6
6
3
2
3
 c 
 
2

=
−
=
−
=
=

=



















161
Beispiel 3.5.3 (Fortsetzung):


--- PAGE 208 ---

(
)
( )
(
)
3
4
2
2
2
3
0
0
2
0
2
2
2
2
2
3
4
0
0
x
1
3
x
x
x -
x
dx
2
2
3
8
3
3
8
E X  = 
2
1
2
2
3
und
V
Dann:
3
1
3
5
1
X =
x-1
x-
x
dx
x-
x +2x -
x
dx
2
2
2
2
2
3
40
16
32
1
1
          = 
2-
+
-
=
;   
=
= 0.4472.
5
5
2
6
2
10

=


−
=

−
=

=

































162
Beispiel 3.5.3 (Fortsetzung):


--- PAGE 209 ---
Satz
Sei
X
eine
2 U
mitvariant
Var
X
Seien
a
beR
Dann gilt
Var
a
b
a
Var
X
Be
IDE
Var
Y
Y
EE
v.ca
EE
Et
EEe
EiE
e
a E X
2AEX
a
EEX
EX
ar
Var X
QED


--- PAGE 210 ---
Satz:
 
 
 
Wenn X eine Zufallsvariable mit der Varianz V(X) 
ist und a und b reelle Konstanten sind, dann hat 
Y = aX+b die Varianz V(Y) = a2  V(X) und die 
Standardabweichung σy = |a|  σx. 
163
Rechenregeln für Varianzen
 
Be
Var
ax
b
ax
b
Tax b
IE
ax
a Ex
b
a
EI
X EIX


--- PAGE 211 ---
Ges
Var X
Definiere
Y
X
6100
40
5 0
o
n
EE
1
02
5
0.3
Var
Y
1
0.3 20.2
0
0.3
0.3
1 0.3
0.61
Var
X
Var
404
6100
402 bgL
976


--- PAGE 212 ---
Beispiel 3.5.4:
Gegeben sei die Zufallsvariable X mit
Um die Varianz V(X) zu berechnen, nehmen wir 
zuerst eine Transformation
deren Verteilung offensichtlich
                                        ist.
( )
X         6060   6100   6140
f
     0.2      0.3      0.5     
x
X-6100
Y = 
,
40
( )
Y         -1     0      1
f
   0.2   0.3   0.5
y
164


--- PAGE 213 ---

(
)
(
)
(
)
(
)
(
)
(
)
(
)
2
2
2
2
2
Wir erhalten:
E
0.3
Y
V
0.2
0.3
1
0.3
0
0.3
Y
          +  
0.5 
 0.61
1
0.3
40
6100
40
40
0.61 
 976
            
V
V
V
Y
X
Y
=
=

+

−−
−

=
−

=
=

+

=
=
165
Beispiel 3.5.4 (Fortsetzung):


--- PAGE 214 ---
Satz (Vereinfachte Berechnung der Varianz)
 
 
 
 Vereinfachte Berechnung:  V(X) = E[X2] - μx2.
166

(
)


(
)
( )
2
2
2
2
210
70
V
5.83
3.78086
2.05247
Y
36
36
35
Schon berechnet:    E
  ,  V
2.05247
Y
Y
18
10
210
1
Nun:   E
0
1
.......
Y
6
36
36
      
.

=
−
=
−
=
=
=
=

+

+
=
Beispiel 3.5.5:
Sei wie vorher Y = “Differenz der Augenzahlen 
zweier Würfel“ (siehe Beispiel 3.5.2).
✓


--- PAGE 215 ---
2
U
mit
IIIX
1
and
sei
d
eine
beliebig
reelle
Zahl
Damn
É
j
I
ÉÉIp
Be
Ubungsaufgator


--- PAGE 216 ---
Steinerscher Satz:
 
 
 
 Ist X eine Zufallsvariable mit E[X] = μ und ist d 
eine reelle Zahl, dann gilt
 
V(X) = E[(X-d)2] – (μ-d)2.
167
Be
X2
2dX
A
m 2nd
A
X
12
Var
X


--- PAGE 217 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 9.6


--- PAGE 218 ---
Definition
Standardisieren
Sei
X
eine
Z U
mit µ
FIX
and
2
Var
X
70
Die transformiente
e hisststadardisirt
EEZ
E
E
EIJI
IAX
1
a
0
a
Var
Z
Var
1
b
Var
ax
b
a
Var
X
Var
x
1


--- PAGE 219 ---
Definition
 
 
Sei X eine Zufallsvariable mit   μx = μ   und 
 
σx = σ > 0. Dann heisst die transformierte 
Zufallsvariable
 
 
 
 
 
 
Z  =  
standardisiert.
σ
X - μ
169
3.6. Standardisieren 
 
 


--- PAGE 220 ---
Jede standardisierte Zufallsvariable hat 
Erwartungswert 0 und Varianz 1:
 X  
Y = X – μ 
Z = 
Y =
 
 E[X] = μ 
 E[Y] = 0 
 
E[Z] = 0
 V(X) = σ2 
 V(Y) = σ2 
 
V(Z) = 1
σ
1
Verschiebung
Streckung
σ
X - μ
170


--- PAGE 221 ---
Beispiel
Kopfe bei zwei Manzwarfen
EEX
1
Var
X
Standardisiering
Z
It
V2 x 1
Z
v2
0
V2
fat
EEZ
V21
to.tk
V24
0
Var z
V21
02
E 214
44
1


--- PAGE 222 ---
Beispiel 3.6.1:
X = “Anzahl Köpfe bei zwei Münzwürfen“
Man kann berechnen:   E(X) = 1  ,  V(X) = ½
X-
X
1
Standardisierung:    Z=
1
2


−
=
( )

( )
Wir erhalten für Z
                   Z      - 2      0       2
1
1
1
                f
   
     
     
z
4
2
4
und             E
0,      V
1.
Z
Z
=
=
171


--- PAGE 223 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11
Kolloquin
Wed
22.10
8 15
01
014


--- PAGE 224 ---
Gestern
Variant
WEED
E
X
EX
E X
EX
Standardabweiching
VVarxT
Aufgabe
Seien
X Y
zwei
unabhengise Z V
Zeige
Var
X
Y
Var
X
Var
Y
re
variety
ELIEE.it E EE
EYES
t.EEtY­
IEEX2
EEX
E
Y
EE
Var X
Var
Y


--- PAGE 225 ---
Regel
Var
X
b
a
Var
X
stetig
district
d
Standardisieren
X
2 V
Z
X
EX
VVI
Z
0
V
2
1


--- PAGE 226 ---
4. Stochastische Modelle und 
spezielle Verteilungen
173


--- PAGE 227 ---
Einige Verteilungen haben in der Statistik und ihrer 
Anwendung eine fundamentale Bedeutung.
Jede der speziellen Verteilungen stellt eine ganze 
Familie von Verteilungen dar.
Die einzelnen Mitglieder erhält man durch Festlegung 
ihrer Parameter.
174


--- PAGE 228 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11.1


--- PAGE 229 ---
4.1. Gleichförmige Verteilung (diskret) 
Die Zufallsvariable X habe m (also endlich viele) 
Ausprägungen, wobei alle Ausprägungen mit der 
gleichen Wahrscheinlichkeit vorkommen:
 
Notation: fGL(x; m) 
  
           
 
  
 
  
  
  
176
X
x1
x2
x3
…
xm-1
xm
f(X)
1/m
1/m
1/m
....
1/m
1/m
Parameter der Familie
Gleichverteiling
P
tm
0
m
Parameter
Text
anise
faulx
m


--- PAGE 230 ---
Beispiel
Warfel
goes
22
2
Augenzah
23 3
26 6
Massenfumbtion fail
6
files
fault
6
frnesaa.it
0
soust
Verteilingsfunktion
F
IR
011
Fla
P
X
x
7190
Fine
faux 67 14


--- PAGE 231 ---
Beispiel 4.1.1:
Für  m = 6 ist es die Massenfunktion von
X  =  "Augenzahl beim klassischen Würfelwurf":
(
)
GL
1 ,     x = 1, 2,...,6
f
x;6
6
 0,     sonst

= 

177


--- PAGE 232 ---
Beispiel 4.1.1 (Fortsetzung):
Verteilungsfunktion?
1
0
FGL(x,6)
2
3
4
5
1/6
2/6
3/6
4/6
1
5/6
6
178
verteingstunktion


--- PAGE 233 ---
warfelbeispie
EX
a
P X
x
Ei
3
1H
Var
X
IEEXY
ETX
G
512­IEEXY
i.Ei2 mt
In120
ᵗ
Var
X
3.5
2.9


--- PAGE 234 ---
Beispiel
Gleichverteilung auf
1,21
M
far
x
m
falls
kE
1,2
im
0
soust
EIX
121
Vark
m
Bsps
Sei
Summe
der Augenzahler
von
2 warfeln
1
5
5
12
flat
X
ist
nicht Gleichuenteilt


--- PAGE 235 ---




(
)
m
m
m
m
i
i
i
i
i
i
i=1
i=1
i=1
i=1
6
i=1
1
1
E
x p
x P
x
x
X
x
X
m
m
1
1
6
6
1
 Bsp:  E
i
3.5
X
6
6
2
=

=

=

=

=

+
→
=

=

=





179
Beispiel 4.1.1 (Fortsetzung):
Erwartungswert?


--- PAGE 236 ---
(
)(
)
( )

(
)
m
6
1
1
1
6
7 13
91
6
1 6 2
1
2
2
2
x
i
 
i
m
6
6
6
6
6
i=1
i=1
2
m
m
1
1
2
2
2
X
X
X
x
x
i
i
m
m
i=1
i=1
2
91
91
49
182
147
35
7
2.9167
6
6
4
12
12
2
2.9167
1.7078
E X
V
E
E
V X
X



+

+

=

=

=

=
=










=
−
=

−













−


=
−
=
−
=
=
=




=
=
=
180
Beispiel 4.1.1 (Fortsetzung):
Varianz?
É5
k
H1
26 11
6


--- PAGE 237 ---
Merke:
Die Summe von unabhängigen diskret gleichverteilten 
Zufallsvariablen ist nicht diskret gleichverteilt (keine 
reproduktive Eigenschaft).
Bsp: Summe der Augenzahlen beim Würfeln mit zwei 
oder mehr Würfeln.
 2       3      4   
   12
1
2
3
   
   
 
    
 Keine Gleichverteilung!
36
36
36
                                       Kein Laplace-Modell

K
K
181


--- PAGE 238 ---
Beispiel
Gleichverteiling
auf
1
2
M
fankins.IM
X
ferc
ny
Parameter
n my
summentonne
EEX
it
1
Var X
E X
E
make
EX
Pki
Eg
É
1 2
1
Var X
2
1
until_311th
2
12
12
4n
6n 2,53m
on
3
2,1


--- PAGE 239 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11.2


--- PAGE 240 ---
4.2. Bernoulli-Verteilung (diskret)
Die Zufallsvariable X habe nur die beiden 
Ausprägungen 0 und 1.
 
Zufallsexperiment mit zwei Ausgängen A und 
Ā, wobei A als Erfolg bezeichnet werde und Ā 
als Misserfolg. Das Ereignis A trete mit einer
bestimmten Erfolgswahrscheinlichkeit (Parameter) 
p auf.
Solche Experimente heissen
Bernoulli-Experimente.
183
peto 7
P
X
1
p
Enfolgswahnscheinlichheit
einerBernoulliZerfallsvariable
P
X
0


--- PAGE 241 ---
7
0
9
fper
p
P
falls
x
o
falls
x
1
T
O
soust
Massenfkt
einer Bernoulli verteilton
2
V
Sei
X
BerLp
seiXeineZ.u
Mit
Massenfunktion
fisertip
X
0
1 70
1 PHI
p
Var
X
ETX
ETX
p2
p
1
p
o
p x
0
1
P X
1
p


--- PAGE 242 ---
184
Definition
 
Eine diskrete Zufallsvariable X mit der Massenfunktion 
heisst
Bernoulli-Massenfunktion.
(
)
1
,
0
;
,
1
0,
Be
p
x
f
x p
p
x
sonst
−
=


=
=




--- PAGE 243 ---
Beispiel 4.2.1:
Um
beim
“Mensch
ärgere
dich
nicht”-Spiel
"herauskommen" zu können, muss man bei drei
Würfen mindestens eine Sechs würfeln (“Erfolg”).


( )

( )
(
)
3
X
216
125
91
5
p =1 - P
1
.
"keine Sechs in drei Würfen"
216
216
6
 
91
      
 E
0.4213,   
216
91
91
V
0.2438
1
216
216
          
 
0.4938
 Erfolgswahrscheinlichkeit?
X
X

−
=
−
=
=

=
=
=

=
−
=
→
185


--- PAGE 244 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel11.3


--- PAGE 245 ---
Seien
Y
Ber
p
for
it
in
unabhengig
X
EY
fx
P
X
k
P É Y
k
E
phce pj.to­
MoglichheitT
as
T.tn
versune­tpin
xip
n
2 pre p
to
far see
one
in
IE
in
Sci X
Bin
nip
EIX
E
fisinkipin
Yi stashing
IT
Yi
III
n p
Le_var
t.E.ua
QFhPI


--- PAGE 246 ---
Beispiel
Urna
mit
Zanicklegen
Urne
hat
10
schwarze
Kusel
20
weisse
Kugeln
wir
Zichen
4 Eugen
Sei
X
der gezogenen Engeln die Schwarz
sind
P X
3
Definies
Y
11
falls baseli shy
0
falls Kugh i yes
for
1
1,213,4
P Y
1
p
Yi
ist Bernoulli
verteilt
mit Parameter p
EY
X
B.in
n
4
p


--- PAGE 247 ---
P
X
3
fp.in
3
p 5
n
4
3
13 3
tain
pin
E pre p
to


--- PAGE 248 ---
Beispiel
A genome
35
der Wahlberedligten
Stimmen
links
Es
stimmer zufallig
n
12
water
ab
P
Linkswahler
die
Mehrheit haben
Y
falls Personiftings
0
Sonst
X
Y
hiker
P
6
n
12
p
0.35
P
X
6
IP X 7
P
8
P X 1
fBin
x
7
n 12,1
0 35


--- PAGE 249 ---
4.3. Binomialverteilung (diskret)
 
 
187
Definition
(
)
(
)
−
=


=


−






0,1,2,...,
; ,
1
,
0
1
n x
x
Bi
n
x
n
f
x p n
p
p
x
p
Eine diskrete Zufallsvariable X mit der Massenfunktion 
heisst binomialverteilt.


--- PAGE 250 ---
Herleitung der Binomialverteilung
Seien Yi, i=1,…n, i.i.d Zufallvariablen mit
Bernoulliverteilung mit Parameter p, dann
ist die Summe X,
binomialverteilt mit
Parametern n und p. 
188
1
n
i
i
X
Y
=
= 


--- PAGE 251 ---
Beispiel 4.3.1: Urnen-Modell mit Zurücklegen
(Laplace-Wahrscheinlichkeit)
In einer Urne befinden sich 10 schwarze und 20 
weisse Kugeln. Daraus soll eine Zufallsstichprobe
vom Umfang n=4 gezogen werden. Nachdem man 
die Farbe der einzelnen Kugeln notiert hat, wird sie
sogleich wieder in die Urne zurückgelegt.
189


--- PAGE 252 ---


(
)
4 0
0
Nun: Sei X = "Anzahl der schwarzen Kugeln in der Stichprobe":
X kann die Werte:                x = 0, 1, 2, 3, 4 annehmen
10
1
Erfolgswahrscheinlichkeit:   p = 
;   n = 4
3
30
4
16
P
p
; 
1 p
X=0
0
81
−
→
→
=

→
=
=
−




(
)
4 1
1
4
32
     P
p
;
1 p
X=1
1
81
−

=
=
−


190
Beispiel 4.3.1 (Fortsetzung):


--- PAGE 253 ---

(
)
(
)
Also: 
1
Massenfunktion von X:  
;
,4
3
1
4
E
4
 
X
3
3
1 2
8
V
1
4
X
3 3
9
Bif
x
n p
n p
p






=

=

=
=


−
=


=
191
Beispiel 4.3.1 (Fortsetzung):


--- PAGE 254 ---
Beispiel 4.3.2:
Man stelle sich vor, 35% der Wahlberechtigten wollen
links wählen. Mit einer Zufallsstichprobe vom Umfang
n=12 soll die Wahlabsicht befragt werden.
Wie wird das Stichprobenergebnis ausfallen?
192


--- PAGE 255 ---
(
)




(
)
12
12
x=7
x=7
Erwartete "Linkswähler":
 = 12 0.35 = 4.2 
Standardabweichung: 
 = 12 0.35 0.65
1.6523
     
P "Linkswähler in der Stichprobe haben die Mehrheit"
  = P
P
; 0.35, 12
0.0846
X>6
X=x
Bif
x





=
=
=
=


193
Beispiel 4.3.2 (Fortsetzung):


--- PAGE 256 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11,5


--- PAGE 257 ---
Def
Eine
Zufalls variable
mit
Massen fantation
fpo
1
it
cf.mn
Parameter
A
xe
01112,3
170
cibng
ZigT
Poi
X
É
f
Ubungsonfgabe
IEX
1
Var
X
A


--- PAGE 258 ---
Beispiel
Roulette
Ein
Spieler
setzt
immerant die
Zahl
17
wie gross
ist die
w
bit
dass
er sie genan 8 Spiele
aus 200
Spielen gewinnt
Bin
n
200
p
P X
8
fan 8
n p
0.0814
FIX
n p
2
5 40
Wahle
of
np
5.40
Angenomen x̅
Poi
A
EIXT
1
P X
8
fp
8
1
5 4
0.0812


--- PAGE 259 ---
Definition
4.4. Poisson-Verteilung (diskret)
195
(
)



−
=

;
,
!
x
Po
f
x
e
x
Die Massenfunktion der Poisson-Verteilung lautet 
 
  
 
 
x = 0, 1, 2, .... (abzählbar unendliche Werte)


--- PAGE 260 ---
Beispiel 4.4.1: Roulette
Ein Spieler gehorcht der Eingebung, die Zahl 17 
bringe ihm heute Glück. Er setzt beim Roulette 
unentwegt auf die Zahl 17. Wie gross ist die 
Wahrscheinlichkeit, dass er dabei genau 8 Mal bei 
200 Spielen gewinnt?
→ Erfolgswahrscheinlichkeit: 
 
p = P[Zahl 17] = 1/37 ;   n=200
196


--- PAGE 261 ---
Benutzen wir die Poisson-Verteilung
damit finden wir:
Bemerkung: Hätten wir die richtige Binomial-
verteilung genommen:
5.4054,
np
=
=
(
)
8
5.4054
5.4054
= 
0.0812
8 ; λ
8!
Po
f
e−
=
1
0.0814.
8 ; 
 , 200
37
Bif 
=




197
Beispiel 4.4.1 (Fortsetzung):
p Erfolg


--- PAGE 262 ---
Beispiel 4.4.2:
Der professionelle Minigolfspieler E. Inlocher rühmt 
sich damit, dass er auch auf der schwierigsten Bahn 
nur zu 20% mit dem ersten Schlag nicht einlochen 
kann.
Heute sind zahlreiche Journalisten auf der Anlage 
erschienen. Der Spieler will mit einer Serie von 50 
Versuchen zeigen, dass er wirklich so treffsicher ist.
198


--- PAGE 263 ---
Beispiel 4.4.2 (Fortsetzung)
Berechnen Sie die Wahrscheinlichkeit, dass Herr 
Inlocher nur in 38 von 50 Versuchen mit dem ersten 
Schlag ins Loch trifft:  (a) exakt; (b) mit einer 
geeigneten Approximation.
(
)




.
Bi
Poiss
a) X = "# Versuche nicht ins Loch" 
 f
x ; 0.2 , 50
      P
0.1033
X=12
b)  Benützen Sie die Poisson-Verteilung:  = np = 10
         P
 
 P( Y = 12 ) = 0.0948
X=12
     Approximationsfehler:  gross ! 


=

:
  p = 0.2  zu gross!
199


--- PAGE 264 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11.7


--- PAGE 265 ---
Disbrete
Verteilungen
Wiederholug
1 Gleichformige Verteiling
GLIMT
1,2
im
Massenfunktion
forearm
tmikeazin.im
0
sonst
EIX
Var X
1
Enfolgowbeit
PE 011
2 Bernoulli verteiling
Ber
p
e
011
P X
1
p
P
X 0
1
p
Massenfunktion
fara p
pP
0
soust
IIIX
p
Var X
pcn p
vesuche Eytle
3 Binomial Verteilung
X
Bin LIE.me
e form
in
Massenfubtion fin
a
pin
E
p G p
for
IIX
n p
Var
X
n pcr p


--- PAGE 266 ---
4 Poisson Verteilung
Poi 15
Érameter
6901112
N
viele
Werk
Massenfunktion
fpo.la A
1
for reform
IX
1
Var X
1
Bsp
X
Tore bei Fussballweltmeisterschaft
power
if.fiii
Bsp
X
kaugummis auf cinzelneen
Pflasterstein
Poi
T
Heute
Stetige Verteilungen


--- PAGE 267 ---
4.5. Rechteckverteilung (stetig)
 
 
 
 
 
201
Definition
Eine stetige Zufallsvariable x mit der 
Wahrscheinlichkeitsdichtefunktion
heisst Rechteckverteilt.
Die Dichtefunktion ist im relevanten Bereich [a, b] konstant.
( )
Re
1
,
0,
f
x
für a
x
b
b
a
sonst

=


−


a
b
a
ber
e
a b
EX
atb
Var
X
b
a


--- PAGE 268 ---
EIX
1 fee
dx
be Ea dos
talked
1
t.EEa
1
Var
X
x2
EE
lathE
EX
boeda
Ia
63 93
Vark
5393
191
siche
Notizen
b
a
Verteilings funktion
Free
P
X
x
Éi
ay


--- PAGE 269 ---
7
Free
Ea
free
O
xca
Free
251
acetalb
1
x
b


--- PAGE 270 ---
EX
fretal do
I
a doc
Ea El
E
Ea heading
Var X
E X
EX
I X
t.ae bx2dx
I
at
3 93
varix
it
45 4 53 Éii a
Lay
463 493 3926 bab 363
393 6a b 3ab
at
b 93
3a b
3ab
by
2
r
Verteilingsfunction
Fretel
P
X
x
0
7
9
free dx
bad
Etc
Em
aceta
Ea
1
x
b
E
5
Is


--- PAGE 271 ---
Beispiel 4.5.1:
Zwischen Mitternacht und sechs Uhr morgens kommt der Bus
gemäss Fahrplan jede halbe Stunde. Wie gross ist die
Wahrscheinlichkeit, dass ein Fahrgast länger als 10 Minuten
warten muss?
( )




(
)

( )
Re
Re
2
T = "Zeit zum nächsten Bus" , 
1  ,  0
 t 
 30,
Zufallsvariable mit f
30
t
 0   ,    sonst.
10
2
Also: 
P
1 P
1 F
1
T>10
T
10
10
30
3
Ausserdem: 
E
15min 
und 
V
75min
T
T




= 

= −
= −
= −
=

=
=
202
a
0
b
30
Fre 41
30
fret
WE
t
10
5
72


--- PAGE 272 ---
Verteilingsfunction
TRelaib
FLA
P
T
t ÉE9


--- PAGE 273 ---
Beispiel 4.5.2:   Wartezeit an der S-Bahn Station
An einer S-Bahn Station fahren die Züge im
12-Minuten Takt. Ein Fahrgast, der den Fahrplan 
ignoriert, erscheint zu einem zufälligen Zeitpunkt an 
der Station.
X = Wartezeit 
Zufallsvariable mit Wertevorrat   W = [0,12] (Minuten)
203


--- PAGE 274 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11.8


--- PAGE 275 ---
4.6. Exponentialverteilung (stetig)
205
Definition
(
)




−


=


0
;
,
0
x
Ex
x
f
x
e
Eine stetige Zufallsvariable X mit der 
Wahrscheinlichkeitsdichtefunktion
 
heisst  exponentialverteilt.
Ex
A
0,0
mit Parameter 170
FIX
Var
X
112
Frage EIX
Fex
a


--- PAGE 276 ---
ITX
a fee ait
dx
x é
da
1
da
easan
e
do
é
I
ETX
x
Ex
x
9
7 dy
Tx
Aétay
é
571


--- PAGE 277 ---
1
5ᵗʰ
E
1


--- PAGE 278 ---
Ben
Sei
X
Ex
1
FIX
Be
EX
x 1512
dx
1
da
it fed
e
da
f
é
Do
E
c
P
X
x
xéty dy
é's
571
1
5ᵗʰ


--- PAGE 279 ---
Beispiel 4.6.1:
Nach Angaben des Herstellers beträgt die mittlere 
Lebensdauer seiner neuen 100-Watt-Glühbirne 5000 
Stunden. Als geeignetes Modell für die Approximation 
der Verteilung der Lebensdauer X verwenden wir:
( )
5000
1
      
x
5000
x
Ex
f
e
−
=
206
Bemerkung: 

1
E X
=
Lebansdamer der
Glahbirne
11
5000
Y
5
EX


--- PAGE 280 ---
2500
P
X
2500
Feet
dx
iE
ii
I
P
X 2101000
1
P X
10
000
1
Ex
101000
0 13


--- PAGE 281 ---
Wie gross ist die Wahrscheinlichkeit, dass eine 
Glühbirne:
(1) weniger als halb so lange brennt?
(2) mehr als doppelt so lange brennt?


(
)
1
2
2500
2500
1
0.3935
Ex
P X
F
e
−

=
= −
=
207


(
)
(
)
2
10000
1
10000
1
1
0.1353
Ex
P X
F
e−

= −
= −
−
=
Beispiel 4.6.1 (Fortsetzung):


--- PAGE 282 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 11.9


--- PAGE 283 ---
4.7. Normalverteilung (stetig)
 
 
209
Definition
Eine stetige Zufallsvariable x mit der Dichtefunktion 
 
heisst standardnormalverteilt.
( )

−
=

−


2
2
1
,
2
x
Zf
x
e
für
x
XER
X
W1o
Standard
Normalverteiling
FIX
0
Var
X
1


--- PAGE 284 ---
Deftion
XER
W M
02
mit
Parameter
MEIR
0
µ
Lageparameter
6
Strengsparameter
Dichtefunition
toE
Algabe
IIIX
M
Var
X
62


--- PAGE 285 ---
Definition
Allgemeine Normalverteilung 
210
(
)
(
)






−
−



¡
2
2
2
2
0
x
-
2
N
x
1
 f
 =
e
,
x; ,
2
Eine stetige Zufallsvariable X mit der Dichtefunktion
heisst normalverteilt.
Es wurde ein Lageparameter μ und eine 
Streuungsparameters  σ > 0 eingeführt.
W µ
4
FIX
µ
Var X
02


--- PAGE 286 ---
Eigenschaft
Seien
X
W Mine
unable gig
Definiere
x̅
EX
x̅
WC
Ñ
52
Eigenschaft
der
Normal verteiling
Seien
X
for
i
1
n
Zfallsvariabeln
mit
X
W Mi
0
die
unabhanging
sind
Dann
gilt
for
x̅
I EnXi
dass
x̅
W
ñ
52
fr
t
I E Mi
52
Eno


--- PAGE 287 ---
211
Illustration der Normalverteilung (Dichtefunktion) mit 
verschiedenen Lage- und Streuungsparametern
σ = 1
σ = 2
σ = 3
μ = -5
μ = 0
μ = 5


--- PAGE 288 ---
Beispiel 4.7.1:
Die Zufallsvariable X sei normalverteilt mit 
E[X] = 5  und  V(X) = 9.
Gesucht ist die Wahrscheinlichkeit    P(-2 < X ≤ 4).
(
)








=








=









=








=
2
4
2
4
7
1
1
7
3
3
0.3694 0.0098
0.3596
Z
Z
- -
X -
-
P
P
<
- < X
-
P -
< Z
3
3
F
- F
-
-
-
=
212


--- PAGE 289 ---
213
σ = 1
σ = 2
σ = 3
μ = -5
μ = 0
μ = 5
Beispiel 4.7.1 (Fortsetzung): 
Wahrscheinlichkeit  P(-2 < X ≤ 4) für E[X] = μ und 
V(X) = σ2.


--- PAGE 290 ---
Beispiel
Asset
Allocation
R
Renaite
eines
Assets
im
Abtienmarkt
Any
Rie
W
M
T
fari
112,3
M
EIR
Erwartete
Rendite
GR
VarCRT
Volatilitat
Risitro
des Assets
Assets
eratt
BifE
An
44
Az
36
20
As
10
4
IPCR.co
P
peers
z
IP
z
0 4
0.0228
P
Rzco
0.0359
P Rzco
0.0062
das Verlust risibo
von Az
ist
an Kleisten


--- PAGE 291 ---
IDEE
Gleichmassiges Autteilen
in
A1 Az Az
Rit R2
R3
EER
EEE EEE EFF
30
IT
Var
R
Var
R
3
2
Var Rn
Var R
f
0.222
0.22
0.0
2
0 01
52
IP
5 015
0.0013
Wissen
W
T
82


--- PAGE 292 ---
Beispiel 4.7.2:  Asset Allocation
In der Portfoliotheorie wird oft mit der Annahme gearbeitet, 
dass Renditen von Wertpapieren normal-verteilte 
Zufallsvariablen sind.
Ein Anleger möchte einen bestimmten Geldbetrag in Aktien 
anlegen
 
Aktie  
erwartete Rendite 
Volatilität
 
 A1 
 
 
44% 
 
22%
 
 A2 
 
 
36% 
 
20%
 
 A3 
 
 
10% 
 
  4%

(
)
    erwartete Rendite
     
  Volatilität (Risiko) des Wertpapiers
R
E
 =
R
= V R


→
214
Mittelwerte
standardabweichengin
risiboreich
risiboarm


--- PAGE 293 ---
) Bei einer Anlage, deren Gesamtbetrag in einer einzelnen 
Aktie angelegt ist (Vermeidung eines 
Kapitalverlustes):
 
 
 
 
Verlustrisiko am kleinsten (aber auch Rendite-Erwartung)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
2
2
2
3
3
3
R
0
0.44
P
0.0228
Z < -2
0.22
R
0
0.36
P
P
0.0359
Z < -1.8
0.2
R
0
0.1
P
P
0.0062
Z < -2.5
0.04
1
2
3
P
P
R < 0
P R < 0
P R < 0






−
−

=
=
=
−
−

=
=
=
−
−

=
=
=


















215
Beispiel 4.7.2 (Fortsetzung)


--- PAGE 294 ---
) Bei gleichmässiger Aufteilung des Anlagebetrages und 
unkorrellierten Renditen:
 
Portfolio-Rendite:
 
→ Die Streuung des Vermögens auf mehrere Anlage- 
 
 
arten erweist sich also als überlegene Strategie!
(
)
(
)
1
    
 
3
2
N
1
2
3
R
R
R
f
R + R + R
x; 
,


=
:

(
)


(
)
(
)
(
)
(
)
R
2
2
2
2
2
2
1
 
30
44
36
10
%
3
1
      
100%
22
20
4
3
0.0013 
 
  !
3
R
E R
V R
P
P R < 0
R < 0


→
=
=
=
+
+
=
=
=
+
+
=
=







K
216
Beispiel 4.7.2 (Fortsetzung)


--- PAGE 295 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 10


--- PAGE 296 ---
5. Mehrdimensionale 
Zufallsvariablen
218
Multivariate


--- PAGE 297 ---
Bei vielen Problemstellungen in der Theorie und in 
der 
Praxis 
sind 
Analysemethoden, 
die 
auf 
Beziehungen, Zusammenhängen oder Abhängig-
keiten zwischen verschiedenen Variablen basieren, 
angemessen, weil sie das Wesentliche erfassen.
  
219
Bop
Portfolioreaate
R
X
Rendite
in
SMI
IR
Y
Rendine
in
S
P 500
Z
R
Multi
variate
2
U


--- PAGE 299 ---
220
Beispiel 5.0.1: Portfoliorendite
Wir betrachten ein Finanzportfolio basierend auf 
zwei Indizes (dem S&P 500 und dem FTSE 100). 
Um die Porftoliorendite zu modellieren könnte man 
nun die beiden Indexrenditen getrennt analysieren 
(zwei univariate Zufallsvariablen). Dabei würde 
jedoch die stochastische Abhängigkeit der beiden 
Variablen vernachlässigt.
Um diese zu berücksichtigen verwendet man 
üblicher weise einen multivariaten (in diesem Fall 
bivariaten) Ansatz.


--- PAGE 300 ---
→  Beziehung zwischen den Variablen ist 
stochastischer Natur
→  zweidimensionale Zufallsvariable: 
  → bivariate Verteilung
→  mehrdimensionale Zufallsvariable: 
  → multivariate Verteilung
221


--- PAGE 301 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 10.1


--- PAGE 302 ---
XM
bÉ
r
Fall
Esseien
X
and
Y
die
distoreten Komponenten
einer
2 dimensionalen Zufallsvariable
Z
XY
Dann
heist
die
Funktion
flag
P
X
x
n
Y
y
P
X
x
Y
y
IP
Z
Gary
die geneinsame
Wahrscheinlidilats
Massenfunktion
von
X and Y
Eigenschatten
faking
20
titj
far.net
of
fzki.si
1
titj


--- PAGE 303 ---
5.1. Gemeinsame Verteilung und 
Randverteilungen
Diskrete Zufallsvariablen:
Sind X und Y die diskreten Komponenten einer 
zweidimensionalen 
Zufallsvariablen 
heisst 
die 
Funktion
 
 
 
f(x,y) = P [{X=x} ∩ {Y=y}]
die gemeinsame Wahrscheinlichkeitsmassen-
funktion von X und Y.
223


--- PAGE 304 ---
Eigenschaften:
 
 
224
( )
(
)
( )
(
)
( )
(
)
1
,
0
3
,
1,
,
2
,
1
i
j
i
j
i
j
i
j
f x y
f x y
i j
f x y









=




--- PAGE 305 ---
Beispiel 5.1.1:
 
In einer Urne befinden sich sechs Kugeln. Drei sind mit einer 
"1" beschriftet, zwei Kugeln mit "2" und eine mit "3".
Nacheinander (ohne Zurücklegen) werden zwei Kugeln 
gezogen. Die Wahrscheinlichkeitsmassenfunktion von
(X, Y) = (Aufschrift 1. Kugel,  Aufschrift 2. Kugel) ist: [Siehe 
nächste Seite]





U
225
Z


--- PAGE 306 ---
(
)
(
)
(
)
3 2
1
1, 1
6 5
5
2 3
1
2, 1
6 5
5
1 3
1
3, 1
6 5
10
f
f
f
=

=
=

=
=

=
Zum Beispiel:
226
Beispiel 5.1.1 (Fortsetzung)
Y
X
y1=1
y2=2
y3=3
fx
x1=1
1/5
1/5
1/10
1/2
x2=2
1/5
1/15
1/15
1/3
x3=3
1/10
1/15
0
1/6
fy
1/2
1/3
1/6
1
É i
mx
O
O
O
O
Randverteiling
Randwertering


--- PAGE 307 ---
Beispiel 5.1.2:  
Wie oben, aber mit Zurücklegen
(Siehe Beispiel 5.1.1)
(
)
3 3
1
1,1
6 6
4
f
=

=
K
K
K
Zum Beispiel:
227
Y
X
y1=1
y2=2
y3=3
fx
x1=1
1/4
1/6
1/12
1/2
x2=2
1/6
1/9
1/18
1/3
x3=3
1/12
1/18
1/36
1/6
fy
1/2
1/3
1/6
1
piggong


--- PAGE 308 ---
Beispiel 5.1.3  (Buch Seite 311):
Eine faire Münze werde viermal hintereinander geworfen.
Sei (X, Y) = (Anzahl der Köpfe, Anzahl der Wechsel)
ZZZZ: (0,0); ZZZK: (1,1); 
ZZKZ: (1,2); 
ZKZZ: (1,2);   KZZZ: (1,1);
ZZKK: (2,1); ZKZK: (2,3); 
ZKKZ: (2,2); 
KZKZ: (2,3);    KKZZ: (2,1);
KZZK: (2,2); ZKKK: (3,1); 
KZKK: (3,2); 
KKZK: (3,2);   KKKZ: (3,1);
KKKK: (4,0).
→ # Möglichkeiten: 24=16
228
Kopfe
Y
der Weeksel


--- PAGE 309 ---
229
Y
X
y1 = 0
y2 = 1
y3 = 2
y4 = 3
fx
x1 = 0
1/16
0
0
0
1/16
x2 = 1
0
1/8
1/8
0
1/4
x3 = 2
0
1/8
1/8
1/8
3/8
x4 = 3
0
1/8
1/8
0
1/4
x5 = 4
1/16
0
0
0
1/16
fy
1/8
3/8
3/8
1/8
1
Beispiel 5.1.3 (Fortsetzung)
Kept
Y
Weasel
Massenftt
o
pay
many


--- PAGE 310 ---
Merke: 
In diesen drei Beispielen:
 
heissen Randverteilung von X, resp. Y.
(
)


(
)
(
)
(
)
,
,
;
;
i
j
x
i
i
i
j
j
j
i
j
y
j
i
x ,y
f
= P
=
f
= p
x
X = x
y
Y = y
x ,y
f
= P
=
f
= p






g
g
230
P X 1,4
1
Pij
o
Pig
flai
y


--- PAGE 311 ---
Seien
X
Y
stetige
Zufalls variablem
damn
neisst
die
Funktion
c
d
ay
mit
der Eigenschaft
9
51
fcoc.gsdydoc
IP
a
x
b
n
c
Y
d
P
a
x
b
c
Yed
gemeinsame
Wahrscheinlithlets
Didlefunk
von
X
and
Y
Eigenschatten
else
1
flag
20
Oxy
flag dydx
1
Frage
ist
es mopia
flag
1
for
einiger
yer
Ja


--- PAGE 312 ---
Exhours
W
a
02
Diane
tation
fn
tgé
he


--- PAGE 313 ---
Stetige Zufallsvariablen:
Sind X und Y stetige Zufallsvariablen, dann heisst die 
Funktion f(x,y) mit
 
 
für  a<b  und  c<d  die gemeinsame Wahr-
scheinlichkeitsdichtefunktion von X und Y.
( )
(
)
( )
(
)
 
   
1
 
  
2
-
-
f
0
x,y
f
dydx =1
x,y




(
)






b d
a c
f
dydx
=
P
x,y
a < X
b
c <Y
d




231
Eigenschaften:


--- PAGE 314 ---
Beispiel 5.1.4:
Die Dichtefunktion der zweidimensionalen 
Normalverteilung könnte lauten:
(
)
(
)
2
2
2
2
1
e
,  
.
2
x
y
f x, y
x, y

+
−
=

¡
232
R


--- PAGE 315 ---
Ges
Finden
sie
C
IDEE
doc
1
c
FLod
Fbi
c
k
xy dy
c Xy
3
clitz
Foa da
E
ax
c
e
f
get In
c


--- PAGE 316 ---
Beispiel 5.1.5:
Die Dichtefunktion von (X, Y) sei
→ nicht-negativ ? ✓
(
)
(
)
12
   für 0
1 und 0
1
7
    0,               
 sonst
,
2
x
y
x + xy
f
x,y




=


(
)
(
)
(
)
2
1 1
1
1
1
1
2
2
2
0
0
0 0
0
0
3
2
1
0
12
12
xy
12
x
dydx = 
x y
 dx = 
 dx
x +
x
xy
7
7
2
7
2
12
12
12
7
1
1
x
x
                                 = 
1
7
7
7
12
3
4
3
4
+
+
=
=

=
+
+









✓
233


--- PAGE 317 ---
Bei stetigen Zufallsvariablen sind die 
Randverteilungen von X und Y durch:
bestimmt.
( )
(
)
( )
(
)
,
,
x
y
f
f
dy
x, y
x
f
f
dx
y
x, y

−

−
=
=


234
f
coal dx
1
fly dy
IR


--- PAGE 318 ---
Gesuet
Randventilingen
floc
fix
y dy
xy
dy
ay
E
lot freton
x2
E
I
fyly
fix
g
do
litany
dic
E
1
fury e tan


--- PAGE 319 ---
Wofer
sind Randverteilingen
nittlich
X Y
geneinsame Dichte floc y
EIX 1
ndx
Dichte der Randverteily
f
flas dy
ELY
19
dy
fly
fray doe
var
x
E
X EYE
n
aÑ
Var
X
E
X
EEX
E
X
IN


--- PAGE 320 ---
( )
(
)


( )
(
)


1
0
1
0
1
0
1
0
.
12
12
=
7
7
12
,    
.
0,1
7
12
12
=
7
7
12
1
,     
+
0,1
7
3
2
        
        
2
2
2
x
2
3
2
2
y
y
f
dy
x + xy
x y + x
x
2
x
x
x + 2
x
x y
f
dx
y
x + xy
+
3
2
y
y


=






=








=






=








Beispiel 5.1.5 (Fortsetzung):
235


--- PAGE 321 ---

( )
(
)
(
)
( ) (
)

( )
(
)
(
)
( )
(
)
x
i x
i
i
2
2
x
x
i
x
i
i
x
x
2
2
x
x
x
 
E
x f
  
x
X
      
V
f
  
x -
x
diskret
X
 
E
x f
dx   
X
x
      
V
f
 dx  
x-
stetig
X
x
;
;







−

−
→
=
=
=
=
→
=
=
=
=




Bemerkung:
Für 
Zufallsvariablen 
mit 
einer 
gemeinsamen 
Verteilung kann man Erwartungswert und Varianz der 
einzelnen Komponenten mittels der Randverteilung 
berechnen:
236


--- PAGE 322 ---
Definition
Die gemeinsame Verteilungsfunktion
 
 
  
gibt an, mit welcher Wahrscheinlichkeit X-Werte 
kleiner oder gleich x und gleichzeitig Y-Werte kleiner 
oder gleich y sind.
237
(
)


,
,
F x y
P X
x Y
y
=




--- PAGE 323 ---
Berechnung:
  
238
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
,
,
,
,
,
,
i
j
i
j
x
x y
y
y
x
x , y
F
f
X Y diskret
x y
F
f
dv du
X Y stetig
x y
u, v


−−
=
=






--- PAGE 324 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 10.2


--- PAGE 325 ---
Bedingle Verteiling
1
DistoreterFall
Seien X Y
dislocte Z U
Mit gemeinsamer Massenfunktion fxx
x y
Dann ist
die bediyte Massenfunktion
von
gegeben Y yp
definietals
fxly.gg
x
fxx
aig
g
tyly
PX
ly y
nur definiest for y
Mit fyly 170
2 statiger Fall
Seien X Y statige Z
U
wit gemeinsamer Dichte fxx
say
Dann
ist
die bedingle Dichtefunction
von X
gegeber
i
Emd
fly
a
fax
only
2
X Y
EEZ
IX
IF
1
12147 Fy 6T


--- PAGE 327 ---
Definition
a) Seien X und Y diskrete Zufallsvariablen mit gemeinsamer 
Massenfunktion pij. Die bedingte Massenfunktion von X, 
gegeben dass Y=yj, ist definiert durch
b) Seien X und Y stetige Zufallsvariablen mit gemeinsamer 
Dichtefunktion f(x,y). Die bedingte Dichtefunktion von X, 
gegeben dass Y = y, ist definiert durch
5.2. 
Bedingte Verteilungen und stochastische 
Unabhängigkeit
 
 
 
 
240
(
)
(
)
(
)
,
,
,
,
,
,
0
0
.
j
X Y
i
j
i j
i
j
X Y y
j
Y
j
f
x y
p
f
x
für p
und
sonst
p
f
y

=

=
=

( )
(
)
( )
( )
,
,
,
0
0
.
X Y
y
X Y y
Y
f
x y
f
x
für f
y
und
sonst
f
y
=
=



--- PAGE 328 ---
Beispiel 5.2.1:
Beim dreimaligen Münzwurf mit (X,Y)=(Kopfanzahl 
beim ersten Wurf, Gesamtanzahl der Köpfe) suchen 
wir die bedingte Verteilung von X gegeben dass Y=1.
 
241
Y
X
y1=0
y2=1
y3 =2
y4=3
fx
x1=0
1/8
1/4
1/8
0
1/2
x2=1
0
1/8
1/4
1/8
1/2
fy
1/8
3/8
3/8
1/8
1


--- PAGE 329 ---
fxiy.nl
fxly
fxly
wheit der Kopfahl
bein
enster went
gegeben dass
total
1 Kopf geschen winde
fxly
o
f
om
fye
1
fate
let


--- PAGE 330 ---
,
,
,
,
1/ 4
2
 0:
 
 = 
 = 
3 / 8
3
1/ 8
1
               X
 1: 
 = 
 = 
 
3/8
3
1 2
X|Y=1
1
× 2
2 2
2
× 2
p
f
(x) : X = x =
p
p
 = x =
p
1
 
 = 0: 
4
1
                
 = 1: 
2
1
                 Y = 2: 
4
                 Y = 3:   0       ...
Y|X=0
f
(y) :  Y
 Y
242
Andere Beispiele:
Beispiel 5.2.1 (Fortsetzung):
SET
up
to
one


--- PAGE 331 ---
gemeinsamer
v
130
bedingle Dichte
fx
a
flag
dy
1
5
dy
o
18
y l
A
xe
α
fyly
f cony
do
1
é
doc
y
12
f
é
1
e ty
tallies
Y ExpI


--- PAGE 332 ---
1
Bedingbe
Dichte
von X
Y y
ftp.ytn
tfxylay
f
Rand verteiling
14 01
120
ye
19
yzo
1
9
for xzy o
2
Bedi fte Dichte
von
Y
s
fy
Y
Fxx
x
y
fx
a
12 51
1801 yex
xé
1401 4
for
O
yex


--- PAGE 333 ---
(
)
2
x   , 
      0      , sonst.
e
0
y
x
f x,y


−




= 

Beispiel 5.2.2:
(X,Y) sei ein stetiger Zufallsvektor mit Dichtefunktion
( )
( )
( )
(
)
2
2
0
2
2
 Randverteilung: 
      
d
,   
0
1
      
d
,   
0
                                   
.
Exp
x
x
x
x
x
y
x
y
y
y
f
e
y
e
x
x
x
f
e
x
e
y
y
e
y












−
−
+
+
−
−
−
→
=

=





=

=

=


−




→


:
243


--- PAGE 334 ---




A = ( , )|
[0,
),
[0, ]
oder
A = ( , )|
[0,
),
[ ,
)
 
x y
x
y
x
x y
y
x
y







A
y=x
244


--- PAGE 335 ---
( )




(
)
( )




2
0
0
2
0
2
0
 Bedingte Dichte von 
 gegeben : 
 1
      
,  
0.
 1
 Bedingte Dichte von 
 gegeben : 
 1
1
      
,  
0
.
x  1
x
y x
x-y
X|Y=y
y
y
x
y x
Y|X=x
x
x
X
Y
e
f
e
x
y
x
e
Y
X
e
f
y
x
y
e
x










−

−
−

−

−

→


=
=





→


=
=




245
Beispiel 5.2.2 (Fortsetzung):


--- PAGE 336 ---
Def
stochastische Unabhengigheit
Zwei Zufallsvariables
X Y
heissen
stochastisch
unabhangig
wenn
p
mpq
y


--- PAGE 337 ---
x
y
f(x,y) = f (x)  f (y),    für   (x,y).


246
Definition: Stochastische Unabhängigkeit
(
)
( )
( )
(
)
,
,
,
,
X Y
X
Y
f
x y
f
x
f
y
für
x y
=


¡
Die 
Zufallsvariablen 
X 
und 
Y 
heissen 
stochastisch 
unabhängig 
(oder 
kurz: 
unabhängig) wenn die gemeinsame Massen- bzw. 
Dichtefunktion gerade gleich dem Produkt der 
beiden Randverteilungen ist. Das heisst:
is


--- PAGE 338 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel10.3
XER
YER
g IRHIR
fay ay


--- PAGE 339 ---
Ein
dimensionaler
fall
g
IR
IR
g X
gasfix
doc
E g D Egai flap


--- PAGE 340 ---
5.3. Kovarianz und Korrelationskoeffizient
 
 
 
248
Definition
Seien X und Y zwei Zufallsvariablen mit der gemeinsamen 
Massen- bzw. Dichtefunktion f(x,y). Der Erwartungswert 
der Funktion g(X,Y) ist definiert als 




 i
 j
E
  =  
,    falls (
) diskret;   
E
  = 
d
d ,    falls (
) stetig.
          
i
j
i
j
g(X,Y)
g(x ,y ) f(x ,y )
X,Y
g(X,Y)
g(x,y) f(x,y) y x
X,Y




−−




--- PAGE 341 ---
Erinuering
var
X
X Mx
X Mx
Definitions
Seien
X Y
zwei
Zufalls variables
mit Erwartangswerten
ITX Mx
EE
14
Dann heist
Cov XY
E
X Mx
M M
heist
Kovariant
zwischen X
Y
D
Mx
IIIX
p
Radverting
Multiplikationssafe for Erwangswerte
IIIX
Y
IEEXJ.EE
co
XY
Pew
Ann
XY
stetig
coulx.tl
1l
mfxybiisldady­xy
xuy
yMx MxMY


--- PAGE 342 ---
12yfxmp.mx
EM­xfxla
My ftp.ydydoe­
MX
yydcdyFly
MxMy
fyylay dedy
1
EEXYJ myfxfpad.IE
Mx­
Mxfyfyly dy
EtJ M­
Mx'MY
EIXY
MM
1
AT


--- PAGE 343 ---
Definition
Seien X und Y zwei Zufallsvariablen mit den 
Erwartungswerten μx und μy.
Die Grösse
heisst Kovarianz zwischen X und Y.
249
Eine Masszahl für den stochastischen 
Zusammenhang zwischen X und Y ist die Kovarianz.
(
)
(
) (
)
,
X
Y
Cov X Y
X
Y




= 
−

−




--- PAGE 344 ---
cnn.si ififin
fayty1ag1dI
IN
My
fentanylday
Mx
9taxlastdiady
MM 1txmktd
EIXY My
My
f asldydx
Mx
Y
ldxdy
Fx
a
191
EXY
MM MM
MM
EXT MM
A
m


--- PAGE 345 ---
Multiplikationssatz für Erwartungswerte:





(
)





E
  =   E
  E
  +   Cov
,
 Sind X und Y unabhängig, ist 
      E
  =  E
E
XY
X
Y
X Y
XY
X
Y

→

250
Um festzustellen, ob zwei Zufallsvariablen unabhängig oder 
abhängig sind, könnte man also die Kovarianz ausrechnen 
und nachschauen, ob sie Null ist oder nicht.
,  sind unabhängig 
    
(
) = 0
oder   
(
) 
 0   
    
 sind nicht unabhängig
X Y
Cov X,Y
Cov X,Y
X,Y
→

→
1g
B


--- PAGE 346 ---
Rechenregein
mit
Kovarianzen
Seien
X Y
U
V
Zufallsvariables
Seien
a
b
c
d
eR
Parameter
reelle Zahlen
Dann gilt
covla.tl
b.v
CX
dY
cuyyyyqqyy
1
Bop
Cov
2 X
5 7
10 Cov
X Y
Eninnering
Skala
product
a
u
bu
coat dy
a c
nix
adding
be
Vix
bday


--- PAGE 347 ---
LinearAlebe
VERY
UERN
Lving
VTU
0
V
u
sind
orthogonal
cou É
abcoulxie
Erected
ab XY
abXu
abYmx
abMxMy
COULXY
EIR


--- PAGE 348 ---
Rechenregeln für die Kovarianz
Die Kovarianz ist bilinear, das heisst, sie ist linear in 
beiden Argumenten. Seien zu diesem Zweck U, V, X, 
Y geeignete Zufallsvariablen und a, b, c, d reelle 
Zahlen. Dann gilt
251
·
·
·
·
· ·
·
Cov(aU+b V, c X+d Y) =
a c Cov(U,X) + a dCov(U,Y) + b c Cov(V,X)+ b d Cov(
·
· ·
· ·
V,Y)
Bemerkung: Die Kovarianz erfüllt also dieselben 
Rechenregeln wie ein Skalarprodukt. Deshalb meint 
man mit X und Y sind orthogonal, dass ihre 
Kovarianz verschwindet, d.h., Cov(X,Y) = 0.


--- PAGE 349 ---
Definition
Der Quotient aus der Kovarianz und σxσy von X und Y
 
 
heisst Korrelationskoeffizient zwischen X und Y.
,
(
,
)
 
 
 
X Y
X
Y
Cov X Y



=

252
Zufallsvariablen mit positiver (negativer) Kovarianz heissen 
positiv (negativ) korreliert. Verschwindet die Kovarianz, so 
sind die Zufallsvariablen unkorreliert.


--- PAGE 350 ---
G
Vvartt


--- PAGE 351 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 10.4


--- PAGE 352 ---
5.4. Summe von zwei oder mehreren 
Zufallsvariablen
Erwartungswert?
 




 i
 j
 i
 j
 i
 j
E
 = 
(
)  (
)  
              = 
 + 
 
 
              = E
 + E
i
j
i
j
i
i
j
j
i
j
X +Y
x + y
f x ,y
x  f (x ,y )
y
f(x ,y )
X
Y
























254


--- PAGE 353 ---
Varianz?
(
)
(
) (
)
(
)
(
) (
)
(
)
(
)
(
)
(
)(
)
(
)
(
)
(
)(
)
(
)
( )
(
)
2
2
2
2
2
2
V
 = E
-
 = E
+
               = E
+
 + 2
               = E
+ E
+ 2 E
               = V
 + V
 + 2
x
y
x
y
x
y
x
y
x
y
x
y
X +Y
X +Y
+
X -
Y -
X -
Y -
X -
Y -
X -
Y -
X -
Y -
X
Y
Cov X,Y















































255


--- PAGE 354 ---
Arithmetisches Mittel von Zufallsvariablen:
(
)
(
)


(
)
(
)


2
1
1
1
1
n
n
n
1
n
i
i=1
2
n
2
n
1
n
i
2
i=1
X
E X
 = E
X +
+ X
 =
E X
=
n
 
n
n
n
1
1
V X
 = V
X +
+ X
 = 
 
V X  = 
n
 =
n
n
n
n
 
=
n











=
















K
K
256
(um den Faktor         kleiner,      - gesetz!)  
n
n


--- PAGE 355 ---
Beispiel  5.4.1:
Betrachten Sie folgendes Spiel: Zur Teilnahme muss eine 
Gebühr von 1 Euro bezahlt werden. Es wird dreimal eine 
ideale Münze geworfen, für jeden Kopf wird “1 Euro“ 
ausgezahlt.
257


--- PAGE 356 ---
258



(
)
(
)
(
)
3
,1
=1
,2
3
3
3
i
i
=1
i=1
=1
3
3
3
i
i=1
i=1
=1
1
=1,   "Kopf" ,  p =
    
  i.i.d.
=
-1,  
 =
2
=0,   "Zahl".
1
1
E
E
E
-1
p -1
3
1
;
-1
2
2
1
3
V
V
V
p
3
.
Y
-1
1-p
4
4
i
i
i
i
i
i
i
i
i
i
y
Y
X
Y
Y
y
X
Y
Y
Y
X




→
=
=
=
=

−=






→
=
=
=

=

=











:
a) Beschreiben Sie die Zufallsvariable X: “Gewinn pro Spiel" 
über Bernoulli-verteilte Zufallsvariablen.
 
Zeigen Sie, dass  E[X] = ½   und   V(X) = ¾   gilt.
Beispiel 5.4.1 (Fortsetzung)


--- PAGE 357 ---
b) Anton spielt dieses Spiel drei Runden hinter-einander. U 
sei sein Gewinn nach drei Spielen. Drücken Sie die 
Zufallsvariable U unter Zuhilfenahme von Xi, i = 1, 2, 3 aus.
Berechnen Sie E[U] und V(U).



( )
(
)
, 
3;
2
3
4
 
 X
i.i.d. 
   E
3 E
9
   V
3 V
3
.
4
1
2
3
i
+
+ 
U
X
U
X
 U = X
 X
X
→
=

=
→
=

=

=
:
259
Beispiel 5.4.1 (Fortsetzung)


--- PAGE 358 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 12, 
insb. 12.4


--- PAGE 359 ---
6. Der zentrale Grenzwertsatz
261


--- PAGE 360 ---
Denken wir uns eine n-dimensionale Zufalls-variable 
und nehmen an, dass die einzelnen Komponenten 
X1,..., Xn , unabhängig und identisch (i.i.d) verteilt 
sind mit
Wie 
entsteht 
nun 
eine 
solche 
Folge 
von 
Zufallsvariablen in der Praxis und welche Bedeutung 
hat sie? → Zwei verschiedene Fälle:
1) Stichprobe mit Zurücklegen
2) Versuchsreihe


(
)
2
E
 =    und   V
 = 
.
i
i
X
X


262


--- PAGE 361 ---
Beispiel 6.1:
Der 
Gewinn 
bei 
einem 
Glücksspiel 
wird 
durch 
die 
Zufallsvariable X beschrieben. Bei wiederholtem Spielen 
erhalten wir also Zufallsvariablen X1,..., Xn. 
Der Gesamtgewinn nach n Runden ist
Der durchschnittliche Gewinn pro Runde ist
Frage: 
Wie 
gross 
ist 
nach 
vielen 
Runden 
die 
Wahrscheinlichkeit für einen 
Gesamtgewinn 
zwischen a und b,  d.h.  P[a ≤ Sn ≤ b] = ?
 
→ 
Der zentrale Grenzwertsatz gibt dafür eine 
 
 
Approximation für grosse n (n→∞).
n
n
i
i=1
.
S =
X

n
n
.
1
X
S
=n
263


--- PAGE 362 ---
Spezialfall:   Alle Xi i.i.d. Bernoulli-verteilt. 
264
Theorem: Zentraler Grenzwertsatz (ZGS)
Seien X1, X2,..., Xn 
i.i.d.  Zufallsvariablen mit μ = E[Xi]  und  
σ2  =  V(Xi).Sei Sn die Summe und                 das 
arithmetische Mittel dieser Zufallsvariablen. Dann strebt die 
Verteilungsfunktion Fn der standardisierten Grösse 
mit wachsendem n        gegen die Standard-
normalverteilung: 
n
n
S
X
n
=
(
)
n →
(
)
( )
n
n
Z
F
Z
F
Z
→
/
n
n
n
n
S - n
X -
Z
n




=
=


--- PAGE 363 ---
Illustration des zentralen Grenzwertsatzes
Seien X1, X2,..., Xn sind i.i.-gleichverteilt auf [0,1].
265


--- PAGE 364 ---
Sei Sn eine binomialverteilte Zufallsvariable mit den 
Parametern n und p. Dann strebt ihre Verteilungsfunktion 
 
 
 
   
 
 
  mit 
wachsendem 
n 
gegen die Normalverteilung mit den entsprechenden 
Momenten. Die Verteilungsfunktion F der standardisierten 
Zufallsvariablen
konvergiert für              
gegen die Standardnormal-
verteilung                          . 
266
Theorem: Grenzwertsatz von De Moivre und Laplace
(
)
(
)
(
)
;
;
Bi
n
N
n
F
s  n, p
 F
s
np, np 1- p
→
(
)
(
)
1
n
n
n
S - np
X - p
Z
np 1- p
p
p
n
=

−
n →
(
)
( )
n
n
Z
F
Z
F
Z
→


--- PAGE 365 ---
Beispiel 6.2 (Buch Seite 412):
Eine Theorie behauptet, die Entwicklung von Aktien- 
und 
Wechselkursen 
auf 
informations-effizienten 
Märkten folge einem sogenannten Random Walk 
 
 
   wobei die Kursänderungen von heute auf 
morgen
den Erwartungswert Null und die gleiche Varianz 
hätten. 
t
t-1
t
K  = K + ,


( )
t
t
t 1
t
2
t
K
K
,   E
0
                      V




−
=
−
=
=
267


--- PAGE 366 ---
Die monatliche Kursänderung wäre dann eine 
Summe 
 
 
 
   (Anzahl der Han-delstage 
im Monat).
 
ZGS 
Monatliche Kursänderung asymptotisch 
normalverteilt mit Erwartungswert Null, aber mit der 
n-fachen Varianz.
→ Naive Prognose: Der Kurs bleibt, wie er ist.
t
t+1
t+n, mit n
25



+
+
+
L
;
268
Beispiel 6.2 (Fortsetzung):


--- PAGE 367 ---
Beispiel 6.3:
Seien X1,..., X12 i.i.d. gleichverteilt auf [-½, ½], und
Wegen  
 
 
 
 
 
 
 
 
sagt uns der 
ZGS, dass
Bemerkung: Mit n=12 finden wir eine erstaunlich gute 
 
 
 
Übereinstimmung der beiden Verteilungen.
12
=1
.
12
i
i
S
X
= 


(
)
(
)
2
2
1
E
0
,  V
,
12
12
i
i
b - a
X
X


=
=
=
=
=
(
)
(
)
approx
.
0, 1
2
12
S
N
N
n  , n


=
:
269


--- PAGE 368 ---
Beispiel 6.4:
In einem Experiment wird eine Münze 100 Mal 
geworfen, und man erhält 60 Mal Kopf. Ist die Münze 
fair?
Xi ist Bernoulli-verteilt mit p=½.
(Annahme: faire Münze)
1, 
Kopf bei Wurf  , 
=1,...,100.
Sei 
0, sonst
i
i
i
X

= 

270


--- PAGE 369 ---
S100 ist binomialverteilt mit p=½, n=100.
Also: Für eine faire Münze ist die 
Wahrscheinlichkeit für das Ereignis {S100 ≥ 60} sehr
klein; dass wir nun dieses Ereignis beobachtet
haben stellt die Fairness der betrachteten Münze in 
Frage.
Notiz:   Für die Approximation einer diskreten
Verteilung mit dem ZGS ist eine Stetigkeitskorrektur
nötig.


( )
Standardisierung
50
60
50
=
P
P
60
25
25
                       
.
1
0.0228
2
100
100
100
Z
S
Z
S
F

−
−

=







−
=
;
271
Beispiel 6.4 (Fortsetzung):


--- PAGE 370 ---
Teil II: Statistik
272
Kapitel 1


--- PAGE 371 ---
Repräsentative Stichprobe:
Wünschenswert wäre es, eine Stichprobe 
auszuwählen, die repräsentativ für die 
Grundgesamtheit ist, also eine Struktur bezüglich der 
interessierenden Merkmale aufweist, die der 
Grundgesamtheit möglichst ähnlich ist.
Achtung: Es gibt keine strikt statistische Definition 
von repräsentativ.
273


--- PAGE 372 ---
Eine sehr wichtige Einteilung der Typen von 
statistischen Variablen ist mit welcher Skala sie 
gemessen 
werden 
können. 
Das 
Niveau 
der 
Messbarkeit bestimmt dabei die Möglichkeiten und 
Grenzen der statistischen Auswertungen.
274


--- PAGE 373 ---
1. Nominal messbare Variablen:
Eine Variable ist nominal skaliert, wenn lediglich die 
Gleichheit oder Andersartigkeit verschiedener 
Ausprägungen festgestellt  werden kann.
2. Ordinal messbare Variablen:
Eine Variable ist ordinal skaliert, wenn die möglichen 
Ausprägungen unterscheidbar und zusätzlich in eine 
natürliche oder sinnvoll festzulegende Rangordnung 
gebracht werden können.
275


--- PAGE 374 ---
3. Kardinal messbare Variablen:
Eine Variable ist kardinal skaliert, wenn die       
verschiedenen Ausprägungen nicht nur eine 
Rangfolge ausdrücken, sondern ausserdem der 
quantitative Unterschied zwischen ihnen bestimmt 
ist. Die Ausprägungen müssen Zahlen sein.
276


--- PAGE 375 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 1


--- PAGE 376 ---
7. Beschreibende / 
deskriptive Statistik
278


--- PAGE 377 ---
Ziel:
Übersicht über einige Methoden zur graphischen 
Aufbereitung von Daten; das ist oft nützlich, um 
einen ersten Eindruck und erste Ideen zu 
bekommen.
279


--- PAGE 378 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 1.5


--- PAGE 379 ---
7.1. Häufigkeitsverteilung, Histogramm 
und Verteilungsfunktion
281


--- PAGE 380 ---
Beispiel 7.1.1: Radioaktiver Zerfall von Americium-241
Das radioaktive Element Am-241 emittiert beim Zerfall    –
Teilchen. Wir interessieren uns für die Anzahl der Emissionen 
innerhalb eines Zeitintervalls bestimmter Länge; z.B. von 10 
Sekunden. 
Man beobachtet den Zerfallsprozess eine Weile und möchte 
mit Hilfe der erhobenen Daten ein passendes Modell finden.
Zur Messung der Daten wird die Beobachtungs-periode in 
1207 Intervalle von 10 Sekunden unterteilt. In jedem Intervall 
zählt man die Anzahl der Emissionen und erhält so Daten 
 
                                       
                 
Die Gesamtzahl der Emissionen ist 10'129.

282

=
1,
, 
  
1207,
;
n
X
X
n


--- PAGE 381 ---
Beispiel 7.1.1 (Fortsetzung):  Radioaktiver Zerfall von 
Americium-241)
In einem ersten Schritt werden nun Klassen gebildet:  
für                          zählt man die Anzahl     der Intervalle mit 
genau      Emissionen, also die Anzahl der      mit              .
Zusätzlich bildet man eine Klasse für  0 - 2  Emissionen und 
eine Klasse für 17 und mehr Emissionen. 
Daraus ergibt sich die folgende Tabelle:
283
3,4,
,16,
jy =

jt
jy
jx
j
j
x
y
=


--- PAGE 382 ---
Beispiel 7.1.1 (Fortsetzung):  Radioaktiver Zerfall von 
Americium-241
Aus der Tabelle erhält man das folgende Histogramm:
284


--- PAGE 383 ---
Beispiel 7.1.2:  Bevölkerungspyramide und 
Histogramme: (Siehe Buch S. 34)
Bemerkung: 
Histogramme können auch für nominal messbare 
Variablen gebildet werden.
Ein weiteres Hilfsmittel ist die empirische 
Verteilungsfunktion    .Sie ist definiert durch

=
=


, y
.
(
 
 
 x
 x
) 
1
( ) 
 
 
 
j
i
i
j
y
n
j
Anzahl der Daten
mit
y
F y
n
f
n
285
nF


--- PAGE 384 ---
Fn liefert eine Schätzung für die Verteilungsfunktion F, die den 
Mechanismus hinter den Daten modelliert.
Weil offenbar Fn ausserhalb der Werte y1, …., ym unserer 
Daten konstant ist, plottet man für die graphische Darstellung 
die Punkte
Manchmal verbindet man diese Punkte auch noch mit einer 
Treppenfunktion.
Beispiel 7.1.1 (Fortsetzung):  Radioaktiver Zerfall von 
Americium-241
→ Tabelle und Bild 
(
, 
(
))   
    
 1, ...., .
j
n
j
y
F
y
für
j
m
=
286


--- PAGE 385 ---
287


--- PAGE 386 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 2


--- PAGE 387 ---
Definition
7.2. Messzahlen zur Beschreibung 
statistischer Verteilungen
a1) Arithmetisches Mittel (oder Mittelwert) als    
Lagemass: (→ kardinal skaliert)
n
i
i=1
1
x = 
 
x
n 
289


--- PAGE 388 ---
Definition
a2) Median als Lagemass: (→ ordinal skaliert)































n+1
2
Med
n
n+1
2
2
          x
,          falls n ungerade
x
 = 1 x
+ x
,     falls n gerade
2
290
a3) Modus als Lagemass: (→ nominal skaliert)
 
 
             
 

M
i
i
j
x = x , mit h
h , für alle j
i.


--- PAGE 389 ---
Beispiel 7.2.1:   (Siehe Buch S. 46, Bsp. 2)
Die statistische Reihe 
4, 7, 7, 7, 12, 12,  13, 16, 19, 23, 23,  97    hat
M
Med
12+13
 = 12.5.
2
Mittelwert
= 20;  
Modus             x  = 7;
Median          x
         x 
=
291


--- PAGE 390 ---
Definition
b) Aus den obgenannten Ordnungsstatistiken  x(1), 
...., x(n) (den geordneten Daten) erhält man das
empirische     -Quantil für                wie folgt:
Man bestimmt zuerst                      , wobei ⌊∙⌋ den
ganzzahligen Teil bezeichnet und erhält dann als
empirisches      -Quantil
Damit liegt etwa der Anteil    der Daten unterhalb des 
empirischen     -Quantils.


( )
(
)
(
)






K
K-1
(K)
1 x
+ x
2
x
,        für  
n  nicht ganzzahlig
,  für 
n ganzzahlig.

(0,1)


292




K= αn +1


--- PAGE 391 ---
Beispiel 7.2.2: 
Sei                  Für n = 100 ist                  ganzzahlig, 
also K = 76.
Damit gerade 75% der Daten links von z liegen 
müssen wir den Wert z zwischen x(75) und x(76) 
wählen. Das empirische 75%-Quantil (Q3) 
                           erfüllt das. 
(
)
(75)
(76)
1
2
x
+ x
75%.
=
 = 75
n



→




3
(76)
 Für n = 101:   
n = 75.75   
   K=76, 
                
n nicht ganzzahlig 
 Q = x
.
293


--- PAGE 392 ---
Beispiel 7.2.3: (Siehe Buch Seite 61, Bsp. 14)
Die Lageparameter wie Modus, Median und 
arithmetisches Mittel geben jeweils nur eine zentrale 
Tendenz einer Verteilung an. Nun soll aber auch 
noch das Ausmass der Streuung oder Dispersion 
der Werte einer statistischen Reihe in einer 
Masszahl ausgedrückt werden.
Das Ausmass der Streuung kann helfen, zwei 
Verteilungen mit derselben zentralen Tendenz zu 
unterscheiden.
294


--- PAGE 393 ---
Definition
c1) Spannweite als Streuungsmass: Sie ist die 
Differenz zwischen der grössten und der kleinsten 
Merkmalsausprägung: Spannweite = xmax – xmin.
c2) Mittlerer Quartilsabstand als Streuungsmass:
 
 
wobei man IQA = Q3-Q1 Interquartilsabstand 
nennt. 
(
) (
)
3
2
2
1
Q -Q
+ Q -Q
IQA
MQA=
=
,
2
2
295


--- PAGE 394 ---
11
Beispiel 7.2.4: (Siehe Buch Seite 53, Bsp. 9)
n = 14 Daten
Spannweite = 38-11=27
( )
( )
(
)
2
7
8
+
1
Q  =  
x
 x
= 26.8
2
12.5
15
18 19.5
23
28
25.6
29 30
31.5
34 35
38
x
Q1
Q3
IQA
Q2
 IQA = 13.5 
 MQA = 6.75.


3
(11)
31.5  
Q  = x
 = 
( )
1
4
18
Q  = x
  
=  
 
296


--- PAGE 395 ---
Definition
c3) Empirische Varianz und Standardabweichung 
als Streuungsmass:
 
Die mittlere quadratische Abweichung vom 
arithmetischen Mittel
 
heisst empirische Varianz.
 
Die positive Wurzel aus der empirischen Varianz 
 
 
 
 
 
 
 
 
 
heisst empirische Standardabweichung.
2
x
x
S = + S
(
)
n
2
2
x
i
i=1
1
S =
x -x
n 
297


--- PAGE 396 ---
Beispiel 7.2.5:  (Siehe Buch Seite 54, Bsp. 11)
Bei der statistischen Reihe 
3, 5, 9, 9, 6, 6, 3, 7, 7, 6, 7, 6, 5, 7, 6, 9, 6, 5, 3, 5
rechnen wir mit folgender Arbeitstabelle:
(
)
(
)
{ {
{
{
2
x
2
2
j
j
j
j
j
j
j
j
j
n=20
x 6
1
0
S
j
x
 n
  h
h x
   x -x
x -x
h
x -x
1
3
 3
0.15
0.45    
3
    9
   1.35
2
5
 4
0.20
  1
   1
    1
   0.20
3
6
 6
0.30
1.8
      0
    0
      0
4
7
 4
0.20
1.4
       1
    1
    0.20
5
9
 3 
0.15
1.35      3 
    9
   1.35
=
=
=
−
−


{
3.1
=
✓
298


--- PAGE 397 ---
Definition
Die bisherigen Masse der Streuung sind alle 
Masse der absoluten Streuung und ihre Werte 
hängen von der Einheit ab, in der die Variable 
gemessen wird.
Um das Mass der Streuung zweier Variablen in 
verschiedenen Einheiten zu vergleichen, müssen 
wir ein Mass der relativen Streuung definieren, wie 
den Variationskoeffizienten (sofern         ):
x
0

x
x
S
VK = x
299


--- PAGE 398 ---
Beispiel 7.2.6: Aktienkurse (250 Handelstage):             
(Siehe Buch Seite 58, Bsp. 13)
Daimler Chrysler-Aktie:
Porsche AG-Aktie:
Somit streute die Daimler Chrysler-Aktie trotz geringerer 
Standardabweichung relativ stärker.
→ VK oft benützt als Mass für die Volatilität eines  
     Aktienkurses!
x
x = 50.59 Euro ,  S = 36.18 Euro
y
y = 396.10 Euro , S = 182.96 Euro
x
y
VK  = 
VK  = 
36.18  = 0.72
50.59
182.96  = 0.46
396.10

300


--- PAGE 399 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 2


--- PAGE 400 ---
Bildlich:    alles skaliert!
i
i
3
d  =grösster Datenwert x
mit x -Q  < 1.5 IQA

3
2
3
1
1
Q  = 75%-Quantil
Q  = Median
IQA = Q
Q
Q  = 25%-Quantil
−




i
1
i
c = kleinster Datenwert x
mit Q -x  < 1.5 IQA

25% der
Daten
liegen hier
x
x 
 
Ausreisser
x
7.3. Boxplot
302
25% der
Daten
liegen hier


--- PAGE 401 ---
Beispiel 7.3.1:  Lebensdauer von 16 Geräten 
 
                                (Angaben in Monaten)
1.5; 3.5; 6.5; 11.50; 12.50; 14; 17; 17; 19; 20; 23.5; 
32.5; 34.5; 39; 55.5; 119
1
Q - 1.5 IQA < 0

( )
( )
(
)
8
9
2
X
+X
Q = 
18 ;
2
=
( )
( )
(
)
13
12
3 = 
X
+X
Q
33.5
2
=
3
Q + 1.5 IQA = 65.75;  

         1.5  IQA = 32.25
IQA = 21.5
 
 


( )
( )
(
)
4
5
1 = 
X
+X
Q
12 ;
2
=
55.5
Q3
1.5
Q1
Q2
•  119 Ausreisser
•  relativ kleine Streuung
•  nicht symmetrisch!
   (mehr konzentriert zwischen Q1/Q2)
303


--- PAGE 402 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 2


--- PAGE 403 ---
7.4. Quantile-Quantile-Plot (QQ-Plot)
Manchmal hat man eine Idee, von welcher Verteilung 
die vorliegenden Daten stammen könnten. Ein 
QQ-Plot ist eine graphische Methode mit der man 
beurteilen kann, wie gut die Daten zur vermuteten 
Verteilung passen.
Für praktische Beispiele wird man den Computer 
benutzen.
Doch was für eine theoretische Überlegung steckt 
dahinter?
305


--- PAGE 404 ---
Annahme:  
Die Daten  x1, …., xn  stammen von Zufallsvariablen 
X1,….., Xn, die alle die Verteilungsfunktion F haben.
Aus der Analogie zwischen empirischen und 
theoretischen Quantilen erwarten wir dann
denn zirka der Anteil     der Daten liegt links von            
und mit Wahrscheinlichkeit      liegen die Werte  Xi 
links von 
           F-1 
(
)
( )
-1
n
1
x
F


+






(
)
n
1
x
,

+





( )

( )
( )
(
)
(
)
1
-1
i
P X
F
F F
.



−



=



306


--- PAGE 405 ---
Ist                         so ist                   und wir erwarten 
also
Also sollten die Punkte
etwa auf der Winkelhalbierenden im ersten 
Quadranten liegen. 
K=
n
1,

+




K-1
,
n


( )
( )
-1
-1
-1
K
1
K-
K-1
2
x
F
F
F
.
n
n




















( )
-1
K
1
K- 2
F
 , x
, K=1,...., n,
n






















307


--- PAGE 406 ---
Wir zeichnen also die empirischen Quantile x(K) gegen die 
theoretische Quantile                     und  untersuchen, wie stark 
diese Punktwolkle von einer Geraden abweicht. 
Bespiel 7.3.1 (Fortsetzung):
QQ - Plot der Daten
von 7.3.1 (theoretische
Verteilung: Normalverteilung)
-1
1
K- 2
F
n








308


--- PAGE 407 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
(Kapitel 3.1)


--- PAGE 408 ---
7.5. Streudiagramm
Für Intervall- und Verhältnisdaten sind Abstände zwischen den 
Merkmalsausprägungen definiert.
Eindimensional: Die einzelnen Beobachtungen lassen sich als Punkte auf 
der x-Achse darstellen.
Zweidimensional: Die Daten lassen sich als Sammlung von Punkten 
darstellen, jeder mit dem Wert einer Variable als x-Koordinate und dem 
Wert der anderen als y-Koordinate.
Der übliche Zweck dieser Abbildungsmethode ist es, eine Idee für eine 
zweckmässige Klassenbildung in einem Histogramm zu bekommen oder 
die Beziehung zwischen zwei Variablen zu veranschaulichen.
310


--- PAGE 409 ---
Beispiel 7.5.1:  Streudiagramm
Zweidimensional:
Wartezeit zwischen Ausbrüchen und
Dauer der Ausbrüche für den Old Faithful
Geysir im Yellowstone National Park.
Streudiagramme sind einfach zu konstruieren und zu 
interpretieren (sofern die Stichprobe nicht zu gross ist): Sie 
bringen den Messbereich, Extremwerte sowie allfällig 
vorhandene Konzentrationen augenfällig zum Ausdruck.
¡
x
xxx 
xxx 
xxx 
xxx 
311


--- PAGE 410 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 13


--- PAGE 411 ---
313
8. Schätzung unbekannter 
Parameter: Punktschätzung


--- PAGE 412 ---
314
Ein zentraler Problemkreis der Statistik ist die 
Schätzung unbekannter Parameter von 
Grundgesamtheiten. Von einer Zufallsvariablen sei 
das Verteilungsgesetz bekannt, es enthalte jedoch 
unbekannte Parameter.
Diese sollen aufgrund einer Stichprobe X1,…, Xn 
möglichst gut geschätzt werden.


--- PAGE 413 ---
315
Bei der Schätzung unbekannter Parameter 
unterscheidet man grundsätzlich zwei Methoden:
1) Punktschätzung: 
Man erhält einen einzigen Wert aus der 
Stichprobe, welcher für die Schätzung 
herangezogen wird.
2) Intervallschätzungen: 
Diese lassen Schlüsse über einen Bereich zu, 
welcher mit grosser Wahrscheinlichkeit den 
unbekannten Parameter enthält. 


--- PAGE 414 ---
316
Ausgangspunkt für beide Ansätze bilden sogenannte 
Schätzfunktionen
welche angeben, in welcher Art und Weise die 
Stichprobenvariablen im Hinblick auf optimale 
Schätzungen zu verarbeiten sind.
(
)
n
1
n
θ
X ,…, X
,
$


--- PAGE 415 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 13


--- PAGE 416 ---
318
8.1. Intuitiv heuristische Ansätze für 
 
Schätzfunktionen
In der Realität steht man oft vor dem Problem, dass 
man von einer Zufallsvariablen zwar das 
Verteilungsgesetz kennt, letzteres hingegen 
unbekannte Parameter enthält. Bezeichnet fx(x) die 
Wahrscheinlichkeitsfunktion (Dichte) einer 
Zufallsvariablen, so gilt für ihren Mittelwert:
( )
j
x
j
j
μ = E[X] = 
x  f
x .



--- PAGE 417 ---
319
µ soll über eine Stichprobe X1,..., Xn aus einer 
Grundgesamtheit mit der Wahrscheinlichkeits-
funktion fx geschätzt werden.
Idee: Benutze das arithmetische Mittel
Eine solche Schätzung heisst Punktschätzung, weil 
ein punktueller Wert als Schätzwert genannt wird 
(und nicht etwa ein Intervall).
n
n
1
n
i
i=1
1
ˆ = t(X ,…, X ) = X  = 
X .
n




--- PAGE 418 ---
320
Beispiel 8.1.1: Aus der Grundgesamtheit der Studenten in 
einer Vorlesung wurde eine Stichprobe vom Umfang 10 
gezogen. Die Körpergrösse X in cm wurde festgestellt und in 
folgender Tabelle erfasst:
Die Schätzung für die Körpergrösse der Studenten im 
Hörsaal lautet einfach:
i
xi
176
180
181
168
177
186
184
173
182
177
1
2
3
4
5
6
7
8
9
10
n
ˆ = x  = 178.4 cm.



--- PAGE 419 ---
321
Frage: Handelt es sich um eine gute Schätzformel?
Um eine solche Frage zu beantworten, muss man 
sogenannte Eigenschaften von Punktschätzungen einführen. 
Dies werden wir im nächsten Abschnitt machen.
Würde man nach demselben Ansatz Schätzfunktionen für die 
Varianz σ2 einer Zufallsvariablen oder für den Erfolgsanteil p 
in einem Binomialexperiment suchen, so wären folgende 
Schätzfunktionen angezeigt:
respektive
n
2
2
2
n
1
n
x
i
i=1
1
ˆ  = t(X ,…, X ) = S =
(X -X )
n


n
1
n
i
i=1
1
ˆp = t(X ,…, X ) =
X .
n 


--- PAGE 420 ---
Illustration: Binomialexperiment
322


--- PAGE 421 ---
323
Beispiel 8.1.2:  
Mit den Beobachtungswerten aus Beispiel 8.1.1 
bekommen wir als Varianz der Stichprobe
und damit die Punktschätzung
 
2
x
S  = 25.84
2
2
x
ˆσ  = S  = 25.84.


--- PAGE 422 ---
Beispiel 8.1.3: Firms' lifetime 
324


--- PAGE 423 ---
325
Schätzfunktionen t(X1,…, Xn) sind ebenfalls Zufallsvariablen 
und unterliegen somit auch einem Verteilungsgesetz.
Notation:
• Symbol "^" bedeutet Schätzfunktion ("Dach" oder "Hat" 
auf Englisch)
• T = t(X1,…, Xn)  ist eine Zufallsvariable, während die auf 
den Realisationen xi (i = 1,…, n) basierenden Werte 
Realisationen dieser Zufallsvariablen darstellen.
• E[T] = µT ist der Erwartungswert der Schätzfunktion T.
•                                         ist die Varianz der Schätzfunk-
tion T.
Für ein und denselben Parameter stehen oft mehrere 
Schätzfunktionen zur Verfügung.
(
)
2
2
T
T
V(T)  = E
T - μ
 = σ






--- PAGE 424 ---
326
Beispiel 8.1.4:  
Sei X poissonverteilt mit Parameter   , so gilt:
Soll nun 
oder andere Möglichkeiten gewählt werden?
Es stellt sich somit ein Bewertungsproblem für Schätz-
funktionen. Ihre Qualität wird an wünschenswerten 
Eigenschaften gemessen.
$
(
)
$
(
)
1
n
2
1
n
x
λ = t X ,…, X
 = X,
λ = t X ,…, X
 = S

(
)

( )
x
-λ
λ
P X = x  = 
e     mit    E X  = V X  = λ.
x!


--- PAGE 425 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 13.2


--- PAGE 426 ---
Definition
328
8.2. Eigenschaften von Punktschätzungen
A) Erwartungstreue Schätzfunktionen
Eine Schätzfunktion                                   heisst 
erwartungstreu für    falls:
Es erscheint vernünftig zu fragen, ob eine 
Schätzformel im Mittel den gesuchten Wert trifft.
Die Differenz                        heisst Verzerrung (Bias).
(
)
1
n
ˆ
T = t X ,…, X
 = θ
θ


T
T
E T =μ  existiert und E T =μ =θ.

bias = θ - E T


--- PAGE 427 ---
Beispiel 8.2.1:
i)                                                     ist eine erwartungs-
 
treue Schätzfunktion für
ii)
 
 
 
 
 
 
     ist keine 
 
erwartungstreue Schätzfunktion für
 
und ist um den Faktor            verzerrt.  
(
)
n
n
1
n
i
i=1
1
T = t X ,…, X
 = X  = 
X
n 



(
)



i
n
n
n
i
i
i=1
i=1
μ = E X  d.h. μ = E X ,  i = 1,
, n .
1
1
1
E T  = E X
 = E
X
 = 
E X  = 
  n  μ = μ.
n
n
n














K
(
)
n
2
2
1
n
i
i=1
1
T = t X ,…, X
 = S  = 
(X - X)
n
( )
2
σ  = V X
(
)
n-1
n
329


--- PAGE 428 ---

(
)
(
) (
)
(
)
(
)
(
)
(
)
(
)
(
)
( )
(
)
n
n
2
2
2
i
i
i=1
i=1
n
2
2
i
i=1
n
2
2
i
i=1
2
2
2
2
1
1
E T =E S
=
E
X -X
E
X -μ - X-μ
n
n
1
=
E
X -μ
n X-μ
n
1
=
E
X -μ
n E
X-μ
n
1
=
  n  V X  - n V X
n
1
σ
1
n-1
=
n  σ  - n  
 
  σ   n-1
 
  σ
n
n
n
n






=












−









−




















=


=









=> Eine erwartungstreue Schätzfunktion für σ2 ist somit
(
)
n
2
2
1
n
i
i=1
n
1
T = t X ,…, X
 = 
S  = 
 
(X - X) .
n-1
n-1 
Beispiel 8.2.1 (Fortsetzung):
330


--- PAGE 429 ---
iii) Sind Z1,…, Zn iid Bernoulli Variablen mit
 
so gilt für die Schätzfunktion
Der Erfolgsanteil in der Stichprobe schätzt die 
Erfolgswahrscheinlichkeit p eines Binomial-
experimentes erwartungstreu.
( )
(
)


1-z
z
zf
z  = p
1-p
, z
0,1 ,

(
)

n
1
n
i
i=1
n
i
i=1
1
Z = t Z ,…, Z
 =Z =
Z ;
n
1
E Z  = E
Z
 
 p.
n

=






Beispiel 8.2.1 (Fortsetzung):
331


--- PAGE 430 ---
Frage: Welche Schätzfunktion für V(Z)=p(1-p)?
  Idee:  
   Ist diese Schätzfunktion erwartungstreu?
    Aber: 
332
Z(1
Z)
−
(
)
2
2
2
2
n p +np 1-p
E Z(1
Z)
E Z
E Z
p-
n





−
=
−
=







(
)
(
)
n-1p 1-p
p 1-p
n
=

(
)
E Z(1
Z)
p 1-p  für n


−
→
→




--- PAGE 431 ---
Graphische Illustration:  
333


--- PAGE 432 ---
Definition
334
B) Asymptotisch erwartungstreue Schätz-
funktionen
 
Wenn die Verzerrung mit zunehmendem Stich-       
probenumfang geringer wird und für 
    
verschwindet, dann heisst der Schätzer
    asymptotisch erwartungstreu:

nlimE T  = θ.
→
n →


--- PAGE 433 ---
Definition
335
C) Konsistente Schätzfunktionen
 
 
Eine Schätzfunktion                            für den 
 
unbekannten Parameter    heisst konsistent, wenn die
 
Folge                     stochastisch gegen      konvergiert:
     Notation:
$
(
)
n
1
n
θ  = t X ,…, X
θ
$

n
n
θ
, n →
θ
$
(
)


−

→
→

n
P θ
θ
 0   für   n
,    
0.
$ ⎯⎯→
→
P
n
θ
θ   für   n
.


--- PAGE 434 ---
Graphische Illustration der Konsistenzeigenschaft: 
336


--- PAGE 435 ---
337
Ein Schätzer ist konsistent, wenn er:
1. erwartungstreu ist (mindestens asymptotisch); und 
2. wenn ausserdem seine Varianz für
gegen Null geht. 
n
.
→
Theorem (Konsistente Schätzer):


--- PAGE 436 ---
338
Beispiel 8.2.2:
X1,…, Xn sei eine Stichprobe aus einer Gesamtheit 
mit Erwartungswert µ und Standardabweichung σ. 
Sei
Wir wissen:        ist erwartungstreu für µ.
Nun:
(
)
n
n
n
1
n
i
i=1
1
T  = t X ,…, X
 = X  = 
X .
n

⎯⎯→
p
n
n
Regel
  
   T  ist konsistent für μ, oder T
 μ.
(
)
2
n
n
i
n
i=1
1
σ
V T  = V
X
0.
n
n
→

=
→





n
T


--- PAGE 437 ---
Definition
339
D) Effiziente Schätzfunktion
 
Bezeichnen T sowie U1, U2,…, UK erwartungstreue 
Schätzfunktionen für den unbekannten Parameter     mit 
so heisst T effizient, falls 
Existieren also mehrere erwartungstreue Schätz-
funktionen, so wählt man jene mit der kleinsten Varianz. 
(→ Sie liefert Schätzwerte, welche im Mittel am  
  wenigsten vom wahren     abweichen.)
( )
( )
i
V T  
 V U ,   i=1,…, K.

θ




1
K
E T  = E U  = 
 = E U
 = θ
L
θ


--- PAGE 438 ---
Graphische Illustration der Effizienzeigenschaft (I) 
Seien X1, …, Xn Zufallsvariablen mit Erwartung μ und 
Varianz σ2 und n eine gerade Zahl. Dann sind 
zwei erwartungstreue Schätzer für den Mittelwert, da
Varianz?
340
1
/2
2
1
1
2
b
n
a
i
i
n
i
i
X
X
n
X
X
n
=
=
=
=


1
1
/2
/2
2
2
2
1
1
)
1
1
1
(
)
(
)
(
)
(
)
2
2
2
(
(
)
(
)
/ 2
(
)
n
n
a
i
i
i
i
i
n
n
i
i
i
i
b
i
E X
E
X
E X
n E X
n
n
n
E X
E
X
E X
n
E X
n
n
n


=
=
=
=
=
=
=

=
=
=
=


=






--- PAGE 439 ---
Graphische Illustration der Effizienzeigenschaft (I, 
Fortsetzung) 
Es gilt, dass
341
2
2
1
2
(
)
 und 
(
)
b
a
Var X
Var X
n
n


=
=
a
X
b
X


--- PAGE 440 ---
Graphische Illustration der Effizienzeigenschaft (II) 
342


--- PAGE 441 ---
Definition
343
E) Mittlerer quadratischer Fehler (MSE)
 
Der MSE ergänzt die bisherigen Kriterien zur Beurteilung 
der Güte von Schätzfunktionen.
 
Bezeichnet                                die Schätzfunktion für den
 
unbekannten Parameter   , so heisst
 
mittlerer quadratischer Fehler der Schätzfunktion.
(
)
1
n
T = t X ,…, X
θ
(
)
( )
2
E
T-θ
 = MSE θ






--- PAGE 442 ---
344
Nun:
( )
(
)
(
)
(
)
(
)
(
)

(
)
(
)
(
)
( )
2
2
T
T
2
2
T
T
T
T
= 0
2
2
T
T
2
MSE θ
E
T-θ
= E
T-μ - θ-μ
E
T-μ
2 θ-μ
E T-μ
θ-μ
E
T-μ
θ-μ
V T  + bias




=








=
−

+




=
+


=
14 2 43


--- PAGE 443 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 13


--- PAGE 444 ---
346
8.3. Methoden zur Konstruktion von 
Schätzfunktionen
Bisher beschränkten sich die Überlegungen auf die 
Diskussion der Qualität von Schätzfunktionen.
Frage: Nach welchen Methoden und Prinzipien können 
solche Schätzfunktionen konstruiert werden?
Es gibt eine reiche Palette von möglichen Ansätzen:
➢Momentenmethode
➢Minimumquadratmethode
➢Maximum-Likelihood-Methode


--- PAGE 445 ---
347
A) Momentenmethode
Regel: Man schätze die Momente der Verteilung              
der Grundgesamtheit mit den entsprechenden 
Momenten der Stichprobe.


--- PAGE 446 ---
348
Grundgesamtheit 
Stichprobe
 

n
j
j=1
n
2
2
2
2
j
j=1
n
m
m
j
j=1
1
E X  = μ
X
n
1
E X
 = μ + σ
X
n
1
E X
X
n



















--- PAGE 447 ---
349
Daraus lassen sich die Schätzformeln
herleiten.
 
n
n
2
2
2
i
n
i=1
ˆμ = X
1
ˆσ =
(X -X ) = S
n


--- PAGE 448 ---
Illustration: Normalverteilung
350


--- PAGE 449 ---
351
B) Minimumquadratmethode 
Regel: Auf den unbekannten Mittelwert
 
angewandt, schlägt das Prinzip vor, 
diejenige Grösse        als Schätzfunktion zu 
wählen, von der die Summe der quadrierten 
Abstände zu den Stichprobenwerten  Xj 
minimal ist:
Lösung: 
μ
KQ
ˆμ
n
2
KQ
i
μ
i=1
ˆμ
= argmin
(X -μ) .

n
n
KQ
i
i=1
1
ˆμ
 = 
X  = X .
n


--- PAGE 450 ---
352
C) Die Maximum-Likelihood-Methode   
 
("Methode der maximalen Mutmasslichkeit")
Einführungsbeispiel:
Wir betrachten eine Urne mit 2 Sorten Kugeln (S, W) 
im Mischungsverhältnis 1:3.
Der Anteil p der Erfolgskugeln nimmt dann entweder 
den Wert 0.25 oder 0.75 an.
Der Entscheid für p soll aufgrund einer konkreten 
Stichprobe X1, X2, X3 mit Zurücklegen vom Umfang 
n=3 gefällt werden.
(
)
(
)
(
)
3-x
x
Bi
3
X 
 f
x ; n, p ; P X = x  = 
p
1-p
, x = 0,1,2,3.
x



:


--- PAGE 451 ---
353
Problem: 
Aufgrund einer konkreten Beobachtung von X schätze 
man die Erfolgswahrscheinlichkeit p.
                  hängt im Falle einer konkreten Realisation 
von X nur noch von p ab.
Man bezeichnet diese Funktion als Likelihoodfunktion
Annahme: Die Stichprobe zeigt einen Erfolg (d.h. X=1).
Aufgrund dieser Beobachtung: 
(
)
(
)
Bin
Bin
p
X
0
1
2
3
0.25
f
x ; 3 , 0.25
27 64
27 64
9 64
1 64
0.75
f
x ; 3 , 0.75
1 64
9 64
27 64
27 64
(
)
Bi
f
x ; 3 , p
(
)
L p ; x .
ˆp = 0.25.


--- PAGE 452 ---
354
Maximum Likelihood Prinzip: 
X1,…, Xn sei eine Zufallsstichprobe aus einer 
Grundgesamtheit mit bekanntem Verteilungsgesetz fx 
und zu schätzendem Parameter   .
Dann wird die Funktion:
als Likelihoodfunktion bezeichnet.
θ
(
)
(
) (
)
(
)
1
n
1
n
1
n
x ,..., x
n
x
i
i=1
L θ; x ,..., x
 = f
θ; x ,..., x
f
θ; x
=


--- PAGE 453 ---
355
Nun:
ist ein Maximumlikelihood-Schätzer  für   . 
$
(
)
(
)
ML
1
n
θ
1
n
θ
θ
= argmax   L θ ; x ,..., x
argmax   log L θ ; x ,..., x
.
=
θ


--- PAGE 454 ---
356
Beispiel 8.3.1: ML-Schätzer für p einer Bernoulli-
verteilten Grundgesamtheit: 
(
)
(
)


(
)
(
)
(
)
(
)
(
)
$
(
)
$
$
i
i
n
1-x
x
1
n
i
i=1
n
1
n
i
i
i=1
n
n
!
i
i=1
i=1
1
n
n
n
i
i
i=1
i=1
n
n
ML
i
i=1
L p ; x ,..., x
p
1-p
, x
0,1 .
log L p ; x ,..., x
x  log p + 1-x log 1-p
(1-x )
logL p ; x ,..., x
 
-
 
 0
ˆ
ˆ
p
p
1-p
1 p
x
p
n
x
1
p
X
X .
n
ix
=

=

=
=




−

=

−





=
=









--- PAGE 455 ---
357
Beispiel 8.3.2:  X1,…, Xn sei eine Zufallsstichprobe 
aus einer stetig gleichverteilten Grundgesamtheit: 
( )





R
1
,
0 
 x 
 θ
f
X = θ
,
sonst
0
(
)




$
(
)










K
n
1
n
i
i
i
ML
1
n
1
L θ ; x ,..., x
 = 
,
falls alle x
0,θ .
θ
L ist streng monoton fallend in θ, aber:  alle x
0,θ ,
d.h.  θ 
 x  , i=1,..., n
 θ
= max X ,
, X
.


--- PAGE 456 ---
Graphische Illustration von Beispiel 8.3.2:
358


--- PAGE 457 ---
359
Eigenschaften von Likelihoodschätzfunktionen
1. Maximumlikelihood-Schätzfunktionen genügen 
dem Invarianzprinzip:
Ist     eine ML-Schätzung für    und g(.) eine 
eineindeutige Abbildung von   , so ist         eine 
ML-Schätzung für        .
2. ML-Schätzfunktionen sind konsistent und 
asymptotisch erwartungstreu.
θ$
θ
( )
g θ$
θ
( )
g θ


--- PAGE 458 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 14


--- PAGE 459 ---
361
9. Intervallschätzungen-
 
Konfidenzintervalle


--- PAGE 460 ---
362
9.1. Konzept des Konfidenzintervalls
Während bei der Punktschätzung aus einem 
Stichprobenergebnis nur ein punktueller Schätzwert 
ermittelt wird, informiert eine Intervallschätzung 
zusätzlich über den Stichprobenfehler.
Sie stellt einen Zusammenhang zwischen der 
Punktschätzung und dem Parameterwert der 
Grundgesamtheit her.


--- PAGE 461 ---
Definition
363
Symmetrisches (1-   )-Konfidenzintervall:
wobei der Stichprobenfehler      so bestimmt wird, dass das 
Konfidenzintervall den unbekannten Parameter     mit einer 
vorgegebenen Wahrscheinlichkeit           überdeckt:
Die Wahrscheinlichkeit (1-  ) heisst Überdeckungs-
wahrscheinlichkeit oder Konfidenzniveau.
Üblicherweise werden Konfidenzniveaus von 0.95 oder 0.99 
gewählt.
( )
n
n
1-α
n
n
KONF
θ  = θ -  ; θ +
,
f
f




$
$
nf
θ
( )
1-α
P θ
KONF
θ
 = 1-α.





α
(1-α)
α


--- PAGE 462 ---
364
Interpretation:
Das Konfidenzintervall ist als ein (aus dem 
Stichprobenergebnis resultierendes) Zufallsintervall 
für einen unbekannten, aber numerisch fixierten 
Parameter zu interpretieren.
Abbildung: Aus verschiedenen Stichproben 
berechnete Konfidenzintervalle für   . 
θ






θ
X


--- PAGE 463 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 
Kapitel 14


--- PAGE 464 ---
366
9.2. Ableitung von Konfidenzintervallen
(bei grossen Stichproben)
Exemplarisch betrachten wir die Ableitung des (1-  )-Kon-
fidenzintervalls für das arithmetische Mittel     der Grund-
gesamtheit.
Daraus folgt, dass (für n gross genug):
wobei        das                           der Standardnormal-
verteilung bezeichnet. 
μ
( )
n
Z
n
X -μ
ZGS:
P
 
 F
.
σ
n
q
q
→



→




n
α
α
1
1
2
2
X -μ
P
 
1-α,
σ
n
q
q
−
−


−







α
1
2
q
−
α
1
Quantil
2


−
−




α


--- PAGE 465 ---
367
Äquivalent können wir umformen und erhalten für  
das         -Konfidenzintervall für   : 
Wenn auch    unbekannt ist und            , dann ist
das (1-   ) - Konfidenzintervall für    , wobei
( )
n
n
1-α
α
α
1
1
2
2
σ
σ
KONF
μ  = X
  ;  X
n
n
q
q
−
−


−
+




μ
σ
n
50

( )
n
n
n
n
1-α
α
α
1
1
2
2
S
S
KONF
μ  = X
  ;  X
n
n
q
q
−
−


−
+




n
2
2
n
i
n
i=1
1
S
(X -X )  .
n
=

μ
α
(1-α)


--- PAGE 466 ---
368
Beispiel 9.2.1: (Buch, Seiten 452-453)
Für die Berechnung der Daten für den örtlichen 
Mietspiegel befragt die Gemeindeverwaltung 50 
Haushalte nach der Kaltmiete pro m2 .
Resultat:
Wie gross ist das Konfidenzintervall zum Niveau 0.9 
für die durchschnittliche Kaltmiete    ?
50
50
X  = 8.30 €   und   S = 2.07 € .
μ
( )


90%
KONF
μ =
2.07
2.07
       8.30 1.645
 ; 8.30 1.645
= 7.82 ; 8.78
50
50


−

+







--- PAGE 467 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 468 ---
370
9.3. Zusammenhang mit Hypothesentests
Betrachte erneut das statistische Problem 
bezüglich eines Parameters  dessen wahrer Wert 
unbekannt ist, von welchem allerdings bekannt ist 
das er in einem Parameterraum  liegt.
Angenommen  kann in zwei disjunkte Teilmengen 
0und 1 geteilt werden, und wir möchten 
überprüfen ob  in 0 oder in 1 liegt.
Ein Problem dieser Art wird als 
Hypothesentestproblem bezeichnet. Die 
beobachteten Werte liefern Informationen über  
um eine Entscheidung zu treffen.


--- PAGE 469 ---
371
𝐻0:  𝜃𝜖 0
𝐻1:  𝜃𝜖 1
werden als Ziel (Null) und Alternativ Hypothese 
bezeichnet.
Wird bei der Durchführung eines Tests 
entschieden dass  in 1 liegt, so spricht man 
davon dass die Nullhypothese verworfen wird.
Entscheidet man das  in 0 leigt, so sagt man das 
𝐻0 nicht verworfen wird.
Eine Möglichkeit, um eine solche Entscheidung zu 
treffen ist ein Konfidenzintervall zu konstruieren. 


--- PAGE 470 ---
372
In diesem Fall sind wir an Hypothesen folgender 
Art interessiert:
𝐻0:  𝜃= 𝜃0
𝐻1:  𝜃≠𝜃0.
Beispiel: Mittelwert einer Verteilung (in grossen 
Stichproben)
Idee: Verwirf 𝐻0: µ= µ0 falls der Abstand zwischen 
dem arithmetischen Mittel und µ0 gross genug ist:
𝑋𝑛−µ0 ≥𝑐,
mit c bestimmt wird durch das Signifikanz Niveau 
α des Tests: 𝑐= 𝑞1−𝛼
2
𝜎
𝑛 .


--- PAGE 471 ---
373
Falls  𝑋𝑛= 𝑥𝑛 beobachtet wird, dann ist die Menge 
der µ0 so dass die 𝐻0 verworfen wird gleich der 
Menge der µ0 so dass
𝑥𝑛−µ0 < 𝑞1−𝛼
2
𝜎
𝑛
falls 𝜎 bekannt ist (ansonsten siehe Folie 367).
Diese Ungleichung kann einfach in die Formel für 
das Konfidenzintervall auf Folie 367 
umgeschrieben werden (µ = µ0):
( )
n
n
1-α
α
α
1
1
2
2
σ
σ
KONF
μ  = X
  ;  X
n
n
q
q
−
−


−
+






--- PAGE 472 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 473 ---
375
10. Hypothesentests


--- PAGE 474 ---
376
10.1.  Arten von Hypothesen
Betrachte eine Zufallsstichprobe X1,..., Xn einer 
gegebenen Verteilung fx, abhängig von einem 
Parameter θ welcher im Parameterraum  liegt. Es 
soll die folgende Hypothese getestet werden
𝐻0:  𝜃 𝜖 0
𝐻1:  𝜃 𝜖 1
mit 0∪ 1= .
Wenn 𝑖, 𝑖= 1,2, nur einen einzigen Wert von θ 
enthält, so ist 𝐻𝑖 eine  einfache Hypothese. 
Andernfalls ist 𝐻𝑖 eine zusammengesetzte 
Hypothese.  


--- PAGE 475 ---
377
Darüber hinaus haben einseitige Hypothesen für 
einen eindimensionalen Parameter 𝜃 die Form
𝐻𝑖:  𝜃≥𝜃0 or 𝐻𝑖:  𝜃> 𝜃0
wohingegen zweiseitige Hypothesen die Form
𝐻𝑖:  𝜃≠𝜃0
annehmen.
Beispiel: Mittelwert einer Normalverteilung mit 
bekannter Varianz:
𝐻0: µ= µ0 (einfach)
𝐻1: µ≠µ0(zweiseitig, zusammengesetzt)
Oder wenn eine bestimmte Richtung getestet wird: 
𝐻1: µ> µ0 oder 𝐻1: µ< µ0 (einseitig, zusammengesetzt)


--- PAGE 476 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 477 ---
379
10.2.  Kritischer Bereich und Teststatistik
Betrachte erneut folgende Hypothese:
𝐻0:  𝜃 𝜖 0
𝐻1:  𝜃 𝜖 1
mit 0∪ 1= .
Man nehme an, dass wir, bevor wir entscheiden 
welche Hypothese wir wählen, eine 
Zufallsstichprobe X1,..., Xn einer Verteilung fx, 
welche von einem unbekannten Parameter θ 
abhängt, beobachten können. 


--- PAGE 478 ---
380
Sei S der Stichprobenraum des n-dimensionalen 
Zufallsvektors X=(X1,..., Xn). S ist die Menge aller 
möglichen Werte der Zufallsstichprobe.
In einem solchen Hypothesentestproblem können 
wir ein Testverfahren basierend auf den 
beobachteten Ereignissen spezifizieren, welches S 
in zwei Teilmengen unterteilt:
𝑆1 enthält die Werte von X sodass 𝐻0 verworfen 
wird, während 𝑆𝑜 die Werte enthält bei denen 𝐻0 
nicht verworfen wird.
Die Menge 𝑆1wird als kritischer Bereich des 
Tests bezeichnet.


--- PAGE 479 ---
381
In den meisten Hypothesentestproblemen wird der 
kritische Bereich basierend auf einer Teststatistik 
definiert:
𝑇=f(X1,..., Xn).
Beispiel: (fortgesetzt) Mittelwert einer 
Normalverteilung mit bekannter Varianz:
Für eine zweiseitige Alternativhypothese wird 𝐻0 
verworfen, wenn der Stichprobenmittelwert 𝑋𝑛 weit 
von µ0 entfernt ist. Dies führt zur Verwendung 
folgender Teststatistik:  
𝑇= 𝑋𝑛−µ0
und 𝐻0 wird verworfen falls 𝑇≥𝑐.


--- PAGE 480 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 481 ---
383
10.3.  Gütefunktion und Arten von Fehlern
Aufgrund von Unsicherheit besteht bei jedem 
Testverfahren die Möglichkeit von Fehlern.
Ein Fehler erster Art  ist die Fehlentscheidung 
eine wahre Nullhypothese zu verwerfen. Die 
Wahrscheinlichkeit eines Fehlers erster Art nennt 
man Signifikanzniveau des Tests.
Ein Fehler zweiter Art ist die Fehlentscheidung 
eine falsche Nullhypothese nicht zu verwerfen. Für 
einen gegebenen Wert unter der 
Alternativhypothese bezeichnet man eins minus 
der Wahrscheinlichkeit für einen Fehles zweiter Art 
als Güte des Tests. 


--- PAGE 482 ---
384
Beispiel: (fortgesetzt) Mittelwert einer 
Normalverteilung mit bekannter Varianz :
𝐻0: µ= µ0 
𝐻1: µ≠µ0


--- PAGE 483 ---
385
Der kritische Bereich eines Tests wird 
grundsätzlich festgelegt, indem das 
Signifikanzniveau des Tests α fixiert wird.
Betrachte die einfache Nullhypothese 𝐻0:  𝜃= 𝜃0. 
𝛼 kann in Bezug auf die Teststatistik 𝑇 und den 
Ablehnbereich R berechnet werden als:
𝑃𝑇𝜖 𝑅𝜃0 = 𝑃𝑿𝜖 𝑆1 𝜃0 = 𝛼.
Übliche Werte für α sind 10%, 5%, und 1%.


--- PAGE 484 ---
386
Beispiel: (fortgesetzt) Mittelwert einer 
Normalverteilung mit bekannter Varianz :
𝐻0: µ= µ0 ;  𝐻1: µ ≠µ0
Basierend auf der Teststatistik  𝑇= 𝑋𝑛−µ0  mit 
Ablehnbereich 𝑅= [𝑐, ∞) folgt
𝛼= 𝑃𝑇𝜖 𝑅µ0 = 𝑃𝑋𝑛−µ0 ≥𝑐µ0
= 1 −𝑃−𝑐< 𝑋𝑛−µ0 < 𝑐µ0
= 1 −𝑃𝑋𝑛−µ0
𝜎
𝑛
< 𝑐
𝜎
𝑛
µ0
+ 𝑃𝑋𝑛−µ0
𝜎
𝑛
< −𝑐
𝜎
𝑛
µ0
= 1 − Φ
𝑛𝑐
𝜎+ Φ
𝑛−𝑐
𝜎
= 2Φ
𝑛−𝑐
𝜎
. 


--- PAGE 485 ---
387
Es folgt
Φ
𝑛−𝑐
𝜎
= 𝛼
2
und
−𝑐= 𝑞𝛼
2
𝜎
𝑛
Oder äquivalent
𝑐= 𝑞1−𝛼
2
𝜎
𝑛
mit 𝑞𝛼 dem 𝛼-Quantil der 
Standardnormalverteilung.
Die Nullhypothese mit Signifikanzniveau 𝛼 wird 
verworfen wenn der Abstand zwischen 
Stichprobenmittelwert und µ0 grösser als 𝑐 ist.


--- PAGE 486 ---
388
Für ein Testverfahren 𝛿 wird die Funktion
𝜋𝜃𝛿= 𝑃𝑿𝜖 𝑆1 𝜃, for 𝜃 𝜖 ,
als Gütefunktion bezeichnet. Wird das 
Testproblem in Bezug auf die Teststatistik 𝑇 und 
den Ablehnbereich R formuliert, so lautet die 
Gütefunktion
𝜋𝜃𝛿= 𝑃𝑇𝜖 𝑅𝜃, for 𝜃 𝜖 .
Die Gütefunktion gibt für jeden möglichen Wert des 
Parameters 𝜃 die Wahrscheinlichkeit an, dass die 
Nullhypothese abgelehnt wird.


--- PAGE 487 ---
389
Beispiel: (fortgesetzt) Mittelwert einer 
Normalverteilung mit bekannter Varianz:
𝐻0: µ= µ0 ;  𝐻1: µ ≠µ0
Basierend auf der Teststatistik 𝑇= 𝑋𝑛−µ0  mit 
Ablehnbereich 𝑅= [𝑐, ∞) es folgt für alle μ 𝜖 ℝ 
𝜋𝜇𝛿= 𝑃𝑇𝜖 𝑅𝜇= 𝑃𝑋𝑛−µ0 ≥𝑐𝜇
= 1 −𝑃−𝑐< 𝑋𝑛−µ0 < 𝑐𝜇
= 1 −𝑃𝑋𝑛−𝜇
𝜎
𝑛
< 𝑐+ µ0 −𝜇
𝜎
𝑛
𝜇
+ 𝑃𝑋𝑛−𝜇
𝜎
𝑛
< −𝑐+ µ0 −𝜇
𝜎
𝑛
𝜇
= 1 − Φ 𝑐+ µ0 −𝜇
𝜎
𝑛
+ Φ −𝑐+ µ0 −𝜇
𝜎
𝑛
. 


--- PAGE 488 ---
390
Veranschaulichendes Beispiel: µ0 = 4, 𝑛= 15, 𝜎= 3
Signifikanzniveau des Tests: α = 0.05 (gestrichelte rote Linie)
𝐻0: µ= µ0 
 
𝐻1: µ > µ0  
𝐻1: µ≠µ0
          


--- PAGE 489 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 490 ---
392
10.4.  Der p-Wert
Das Ergebnis eines Tests hängt vom gewählten 
Niveau 𝛼 ab. Das bedeutet, dass bei 
unterschiedlichen Signifikanzniveaus die 
Schlussfolgerung des Tests unterschiedlich 
ausfallen könnte.
Um dieses Problem zu umgehen, wird die Idee des 
p-Werts eingeführt.
Der p-Wert ist das kleinste Niveau 𝛼 für welches 
die Nullhypothese mit Niveau 𝛼 mit den 
beobachteten Daten abgelehnt würde.


--- PAGE 491 ---
393
Beispiel: (fortgesetzt) Mittelwert einer 
Normalverteilung mit bekannter Varianz:
𝐻0: µ= µ0 ;  𝐻1: µ ≠µ0
Wir beobachten in der Stichprobe den Wert 2.78 
für die standardisierte Statistik 
𝑍=
𝑛 𝑇
𝜎
=
𝑛𝑋𝑛−µ0
𝜎
.
Der p-Wert kann berechnet werden, indem das 
kleinste 𝛼 gefunden wird, welches die Bedingung 
2.78 ≥𝑞1−𝛼
2 
erfüllt. Das ist 𝛼= 0.0054.


--- PAGE 492 ---
Teil I: Wahrscheinlichkeitsrechnung
1. 
Grundlagen der Wahrscheinlichkeit 
 
1.1. Ereignisse, Ereignisraum und Ereignismenge
 
1.2. Das Rechnen mit Ereignissen
 
1.3. Der Wahrscheinlichkeitsbegriff
 
1.4. Axiomatik der Wahrscheinlichkeitstheorie
 
1.5. Wichtige Regeln der Wahrscheinlichkeitsrechnung
 
1.6. Wahrscheinlichkeitsräume
 
1.7. Bedingte Wahrscheinlichkeit und stochastische Unabhängigkeit
 
1.8. Totale Wahrscheinlichkeit
 
1.9. Das Bayes-Theorem
2. Elementare Kombinatorik 
 
2.1. Fakultät und Binomialkoeffizient
 
2.2. Das Fundamentalprinzip der Kombinatorik
 
2.3. Permutationen
 
2.4. Kombinationen
 
2.5. Variationen
3. 
Zufallsvariablen 
 
3.1. Die Verteilungsfunktion
 
3.2. Diskrete Zufallsvariablen
 
3.3. Stetige Zufallsvariablen
 
3.4. Erwartungswerte von Zufallsvariablen
 
3.5. Varianz
 
3.6. Standardisieren
4. 
Stochastische Modelle und spezielle Verteilungen
 
4.1. Gleichförmige Verteilung (diskret)
 
4.2. Bernoulli-Verteilung (diskret)
 
4.3. Binomialverteilung (diskret)
 
4.4. Poisson-Verteilung (diskret)
 
4.5. Rechteckverteilung (stetig)
 
4.6. Exponentialverteilung (stetig)
 
4.7. Normalverteilung (stetig)
5.
Mehrdimensionale Zufallsvariablen
 
 5.1. Gemeinsame Verteilung und Randverteilungen
 
5.2. Bedingte Verteilungen und stochastische Unabhängigkeit
 
5.3. Kovarianz und Korrelationskoeffizient
 
5.4. Summe von zwei oder mehreren Zufallsvariablen
6. 
Der zentrale Grenzwertsatz
Teil II: Statistik
7. 
Beschreibende/Deskriptive Statistik 
 
7.1. Häufigkeitsverteilung, Histogramm und Verteilungsfunktion
 
7.2. Messzahlen zur Beschreibung statistischer Verteilungen
 
7.3. Boxplot
 
7.4. Quantile-Quantile Plot
 
7.5. Streudiagramm
8. 
Schätzung unbekannter Parameter: Punktschätzung
 
8.1. Intuitiv heuristische Ansätze für Schätzfunktionen
 
8.2. Eigenschaften von Punktschätzungen
 
8.3. Methoden zur Konstruktion von Schätzfunktionen
9. 
Intervallschätzungen-Konfidenzintervalle
 
9.1. Konzept des Konfidenzintervalls
 
9.2. Ableitung von Konfidenzintervallen
 
       (bei grossen Stichproben)
 
9.3 Zusammenhang mit Hypothesentests
10. 
Hypothesentests
 
10.1 Arten von Hypothesen
 
10.2 Kritischer Bereich und Teststatistik
 
10.3 Gütefunktion und Arten von Fehlern
 
10.4 Der p-Wert
Teil III: Aufgaben
 


--- PAGE 493 ---
Teil III: 
Aufgaben
395


--- PAGE 494 ---
Aufgaben 1.1.1
1) Eine Person wird nach ihrem Geburtstag 
befragt:
2) K Personen werden nach ihrem Geburtstag 
befragt:
3) Position eines Zeigers im Einheitskreis:
4) Ist 
       ein 
 
Ereignisraum für den Doppelwürfel?


S = 0 Sechsen, 2 Sechsen
396


--- PAGE 495 ---
Aufgaben 1.2.1
1) Wurf eines regelmässigen Würfels: 
2)  
3) Beweise die De Morgan‘sche Gesetze:
 
 
 
 
 
(
)


=
x, y |ax + by + c = 0
A
A
B = A 
 B  und  A
B = A
B




→ A  B = ?
→

A: "Augenzahl kleiner als 4"
B: "Augenzahl ungerade"


A
B = ? 
A
B = ? 
397
(
)


=
x, y |ax + by + d = 0
B


--- PAGE 496 ---
4) Werfen von zwei Würfeln:
und
 
A: „Mindestens ein Würfel zeigt eine Sechs“
 
B: „Die Augenzahl beider Würfel ist gleich“
 
C: „Beide Würfel zeigen ungerade Zahlen“
(
) (
)
(
) (
) (
)
(
)


=
S
,
,...,
,
,
,...,
1,1
1,2
1,6
2,1
2,2
6,6
398
Aufgaben 1.2.1 (Fortsetzung):


--- PAGE 497 ---
Als Teilmenge von S: 
A = ? 
B = ? 
C = ? 
A = ? 
C = ? 
399
Aufgaben 1.2.1 (Fortsetzung):


--- PAGE 498 ---




B
C    = ?
B \ C     = ?
A
C        = ?
A
B
C = ?
400
Aufgaben 1.2.1 (Fortsetzung):


--- PAGE 499 ---
Aufgabe 1.3.1: 
Welcher Wahrscheinlichkeitsbegriff?
(1) Die Wahrscheinlichkeit, dass man beim Lotto drei 
oder mehr richtige Zahlen tippt, beträgt knapp 2%.
(2) Die Wahrscheinlichkeit, dass es im nächsten Juni 
in Frankfurt schneien wird, ist geringer als 5%.
(3) Der Bewerber X wird bei der Ausschreibung der 
Stelle Y mit einer Wahrscheinlichkeit von 80% 
zum Bewerbungsgespräch eingeladen.
401


--- PAGE 500 ---
Aufgabe 1.5.1:
“Werfen von zwei Würfeln” 
(Siehe Aufgabe 1.2.1 (4))
A: “Mindestens ein Würfel zeigt eine Sechs”
B: “Die Augenzahl beider Würfel ist gleich”
C: “Beide Würfel zeigen ungerade Zahlen”
Berechnen Sie die Wahrscheinlichkeit der
verschiedenen Ereignisse.
402


--- PAGE 501 ---
Aufgabe 1.5.2:
Krankmeldungen:
Wahrscheinlichkeiten für die Krankmeldungen von drei
Mitarbeitern X, Y und Z (statistisch):
 
 
403
Ei
{-}
{X}
{Y}
{Z}
{XY} 
{XZ}
{YZ} 
{XYZ}
P(Ei) 
0.751
0.1
0.063 0.061
0.011
0.008
0.005
0.001
Σ=1
Berechne:
→ P[X krank] 
=
?
→ P[Y krank] 
=
?
→ 
P[X und Y krank]
=
?
→ P[X oder Y krank]
=
?


--- PAGE 502 ---
Aufgabe 1.6.1:
Berechnen Sie die Wahrscheinlichkeit der folgenden 
Ereignisse:
a) A: “Bei vier Würfen mit einem Würfel tritt 
 
mindestens eine Sechs auf.“
b) B: “Bei 24 Würfen mit zwei Würfeln 
 
 tritt mindestens eine Zwölf auf.“
404


--- PAGE 503 ---
Aufgabe 1.6.2:
(
)


a
U 
 
a,b  | a
 0,3 ,   b < 1 
 6
P(U)
?


=

−




=
405


--- PAGE 504 ---
Aufgabe 1.7.1: 
Krankenmeldungen (siehe Aufgabe1.5.2)
→ Sind die Krankenmeldungen der Mitarbeiter X, Y 
und Z paarweise stochastisch unabhängig?
I) P(X und Y krank) = ?
II) P(X und Z krank) = ?
III) P(Y und Z krank) = ?
=> Wie verändert eine Krankmeldung von X die 
Wahrscheinlichkeit einer Krankmeldung von Y?
 
 
P(Y krank | X krank) = ?
406


--- PAGE 505 ---
Aufgabe 1.7.2:
In einem Bürohaus befinden sich zwei technisch 
und funktional gleiche, unabhängig voneinander 
operierende Aufzüge A und B.
Die Wahrscheinlichkeit, dass sich Aufzug A bzw. B 
zu einem bestimmten Zeitpunkt im Erdgeschoss 
befindet, beträgt jeweils 0.2.
407


--- PAGE 506 ---
→ Wie gross ist die Wahrscheinlichkeit, dass ein zu einem 
zufälligen Zeitpunkt eintreffender Besucher....
 
I) beide Aufzüge im Erdgeschoss vorfindet?
 
II) mindestens einen Aufzug im Erdgeschoss vorfindet?
 
III)  genau einen Aufzug im Erdgeschoss vorfindet?
→ Die Aufzüge haben jeweils eine Ausfall-Wahrschein-
 
lichkeit von 5% (befinden sich nicht im Erdgeschoss). 
Wie gross ist die Wahrscheinlichkeit, dass...
 
I) Aufzug A ausgefallen ist, wenn er sich nicht im  
 
 
Erdgeschoss befindet?
408
Aufgabe 1.7.2 (Fortsetzung):


--- PAGE 507 ---
Aufgabe 1.7.3
Die Übertragung einer Nachricht von A nach 
B erfolgt über 3 voneinander unabhängige 
Kanäle, die mit Wahrscheinlichkeit “p“ 
ausfallen können.
 P[“Übertragung kommt zustande“] = ?
409


--- PAGE 508 ---
Aufgabe 1.8.1: Ziehung der Zusatzzahl im Lotto
Beim Zahlenlotto werden insgesamt sieben Kugeln, 
ohne Zurücklegen, aus einer Urne mit 49 
durchnummerierten Kugeln gezogen, wobei die 
letzte Ziehung die Zusatzzahl bestimmt.
Wie gross ist die Wahrscheinlichkeit von
A: “Zusatzzahl 1 wird gezogen“?
410


--- PAGE 509 ---
Aufgabe 1.8.2:
Ein Fahrzeughersteller stattet seine Fahrzeuge mit 
Klimaanlagen aus, die er von drei verschiedenen Zulieferern A, 
B und C bezieht.
 
 Zulieferer 
Lieferanteil 
Mängelquote
 
  
A 
 
50% 
 
5%
 
  
B 
 
30% 
 
9%
 
  
C 
 
20% 
 
24%
M: “Zufällig ausgewähltes Fahrzeug besitzt eine mangelhafte 
Klimaanlage“
→ P(M) = ?
→ M ist beobachtet: P(A|M) = ?  =  P(A) = 0.5 ?
 
  
 
P(B|M) = ?
 
  
 
P(C|M) = ?
411


--- PAGE 510 ---
Aufgabe 1.9.1:  
Zulieferer mit Qualitätsunterschieden
(Siehe Aufgabe 1.8.2, Folgeaufgabe)
(
)
(
)
(
)
P A M = ?
P B M
P C M
= ?
= ?
412


--- PAGE 511 ---
Aufgabe 1.9.2:  Urne 
Gegeben seien 2 Urnen U1 und U2. 
U1 enthalte 5 weisse und 7 rote Kugeln.  
U2  eine weisse und 5 rote Kugeln.
Aus einer zufällig ausgewählten Urne wird 
ebenfalls zufällig eine Kugel entnommen. Diese 
sei rot. Mit welcher Wahrscheinlichkeit wurde U1 
gewählt?

413


--- PAGE 512 ---
Aufgabe 3.0.1:
→ Anzahl der Kunden in einem Geschäft:
→ Brenndauer einer Glühbirne (Stunden):
414


--- PAGE 513 ---
Aufgabe 3.1.1:  Doppelwürfel


(
)


S = {(i, j) | i , j 
}
1,...,6
X = Summe der beiden Augenzahlen: 
i+j
i, j
W = 2,...,12
Verteilungsfunktion ?

→
415


--- PAGE 514 ---
Aufgabe 3.1.2: 
Eine Maschine produziert defekte Elemente mit 
einer Wahrscheinlichkeit von 5%. Der Produktion 
werden zufällig 4 Elemente entnommen.


X = # defekter Elemente in der Stichprobe
W = 0, 1, 2, 3, 4
Verteilungsfunktion ?
416


--- PAGE 515 ---
Aufgabe 3.2.1:  
Eine Zufallsvariable besitze die 
    
Wahrscheinlichkeitsfunktion
( )
x
K       für   x = 0
2K     für   x = 1
f
 = 3K     für   x = 2
x
5K     für   x = 3
 0      sonst.






417


--- PAGE 516 ---
(
)
(
)
(
)
(
)
( )
x
 
 
      
      
 Man bestimme K:
P
 =  ?
1 < X 
 3
P
          =  ?
X > 1
P
          =  ?
X = 1
 Welches ist der kleinste Wert von X, für den gilt
                P
 = F
0.5?
X
x
x
→
→

→


418
Aufgabe 3.2.1 (Fortsetzung):  


--- PAGE 517 ---
Aufgabe 3.2.2:
Umsatz bei unsicherer Auftragslage (Fortsetzung)
(Siehe Beispiel 3.0.5)
Die Geschäftsleitung interessiert sich für die  
Wahrscheinlichkeit, dass
1. der Umsatz im nächsten Jahr höchstens 30 Mio 
beträgt.
2. die absolute Abweichung vom Umsatzziel 36 Mio 
im nächsten Jahr höchstens 6 Mio beträgt.
419


--- PAGE 518 ---
Aufgabe 3.3.1:   
Eine Zufallsvariable X  besitzt die Dichtefunktion
a) Ermitteln Sie die obere Grenze c, so dass f eine 
Dichtefunktion ist. Zeichnen Sie die Dichte.
b) Bestimmen Sie die Verteilungsfunktion von X.
( )


=









2
3x    ,   0
x
f
3x
1
x
   ,   
x
c
2
2
 0     ,   sonst.
1
2
420


--- PAGE 519 ---
Aufgaben 3.4.1:
( )



=









2
1. Zufallsvariable X mit 
3x    ,   0
x
    f
3
x
 x   ,   
x
c
2
  0     ,    sonst.
    E X  = ?
1
2
1
2
421


--- PAGE 520 ---

2. Wartezeit an der S-Bahn-Station (Fortsetzung)
     (Siehe Beispiel 3.3.2)
     E X  = ?
3. Zwei Spieler A und B würfeln abwechselnd um Geld.
     Der würfelnde Spieler erhält von seinem Gegenspieler
     
   3 Euro wenn er eine 1 oder 2 würfelt;
     
   6 Euro bei einer 6; und
     
   0 Euro wenn er eine 3, 4 oder eine 5 würfelt.
Berechnen Sie den Erwartungswert der Zufallsvariable 
X = "Gewinn von A" w
•
•
•
enn jeder Spieler einmal würfelt.
      
422


--- PAGE 521 ---
Aufgabe 3.5.1:
Eine Zufallsvariable X besitzt die Dichtefunktion
Berechnen Sie E[X] und V(X).
( )


=


2
x  ,   0 
 x 
 c
f x
       0 ,         sonst.
3
8
423


--- PAGE 522 ---
Aufgabe 3.5.2:
Man bestimme den Erwartungswert und die 
Varianz der Zufallsvariablen 
wobei X die Wahrscheinlichkeitsmassenfunktion
 
  
      
besitzt.
Y = 3X + 2,
424
( )
   X       1     2      5         
f
  0.2   0.3   0.5
x


--- PAGE 523 ---
Aufgabe 3.5.3:
Eine Rohrleitung besteht aus 20 Segmenten. Da 
die Ausflussmenge kleiner ist als die 
Zuflussmenge, muss irgendwo ein Leck bestehen. 
Wir nehmen an, dass es genau ein Leck gibt und 
dass es mit Wahrscheinlichkeit 1/20 in einem 
bestimmten Segment liegt. Wir möchten das 
defekte Segment mit möglichst wenigen 
Inspektionen (d.h. Messen der Durchflussmenge 
an einer Segmentgrenze) ausfindig machen.
425


--- PAGE 524 ---
a) Bestimmen Sie die Verteilung der Anzahl 
Inspektionen X, wenn man sukzessive jede 
Segmentgrenze inspiziert. Berechnen Sie ferner 
E(X) und σx.
b) Bessere Strategie?  (günstigere)
426
2
Aufgabe 3.5.3 (Fortsetzung):


--- PAGE 525 ---
Aufgabe 4.3.1:
Qualitätskontrolle
Bei der Produktion von hochwertigen Trinkgläsern beträgt die
Ausschussquote 20%. Im Rahmen einer Qualitätskontrolle
werden nach einem Zufallsprinzip vier Gläser zur Prüfung
entnommen (mit Zurücklegen).
X:
# der fehlerhaften Gläser in der Stichprobe
Y:
# der einwandfreien Gläser in der Stichprobe
Berechnen Sie die Wahrscheinlichkeit, dass
(1)
in der Stichprobe genau ein Glas defekt ist;
(2)
in der Stichprobe mindestens zwei Gläser defekt sind;
(3)
in der Stichprobe genau ein Glas einwandfrei ist.
(4)
E[X], E[Y], V(X), V(Y)?
427


--- PAGE 526 ---
Aufgabe 4.4.1: 
Kunden an einem Bankschalter
An einen Bankschalter kommen zu unvorhersehbaren 
Zeitpunkten vormittags (8-12 Uhr) im Durchschnitt 12 Kunden 
pro Stunde und nachmittags (14-16 Uhr) im Durchschnitt 10 
Kunden 
pro 
Stunde 
(Kunden 
kommen 
unabhängig 
voneinander gleichmässig über den Vor- bzw. Nachmittag 
verteilt).
Berechnen Sie die Wahrscheinlichkeit,
1) dass an einem Tag zwischen 09.00h 
und 
09.15h 
kein 
Kunde kommt;
2) dass an einem Tag zwischen 15.00h 
und 
15.15h 
kein 
Kunde kommt;
3) dass an einem Tag zwischen 15.30h und 16.00h mehr als 
6 Kunden kommen.
428


--- PAGE 527 ---
Aufgabe 4.6.1: 
Kunden an einem Bankschalter (Siehe Aufgabe 4.4.1, 
Fortsetzung)
→ vormittags (8-12): 
12 Kunden pro Stunde
→ nachmittags (14-16): 
10 Kunden pro Stunde
Wie gross ist an einem beliebigen Zeitpunkt am Vormittag 
bzw. Nachmittag die Wahrscheinlichkeit, dass in den nächsten 
fünf Minuten ein Kunde an den Schalter kommt?
→ Xvor 
= Zeit bis zur Ankunft des nächsten Kunden 
 
 
 
(vormittags) ~ fEx (x; 12)
→ Xnach = Zeit bis zur Ankunft des nächsten Kunden 
 
 
 
(nachmittags) ~ fEx (x; 10)
429


--- PAGE 528 ---
Aufgabe 4.7.1: 
Arbeiten mit beliebiger Normalverteilung N(µ, σ2)
Sei X ~ fN (x ; 2,16)
Nun sei X ~ fN (x ; 5, 100). Finden Sie q, so dass:



 
P
  
  ?
0
P
  ?
2
X
X
→
=

→
=





P
 
 0.25
q
P
 
 0.75
q
X
X
→
=

→
=

430


--- PAGE 529 ---
Aufgabe 5.1.1:
Aus einer Schachtel mit 2 weissen, 3 schwarzen und 1 blauen 
Kugel werden 2 Kugeln mit Zurücklegen gezogen.
X = Anzahl weisse Kugeln 
Y = Anzahl blaue Kugeln
Finden Sie die gemeinsame (bivariate) Verteilung und die 
Randverteilungen.
431


--- PAGE 530 ---
Aufgabe 5.4.1:
Berechnen Sie   E[Z]   und   V(Z).


(
)
(
)
(
)


=
+
+
=

2
2
i=1
.
...  sei eine Zufallsstichprobe (iid) mit  E
  
1
4
3
2
und   V
Sei    
= 22
1
2
i
i
2i
i
X , X , 
X
X
X
X
Z
432


--- PAGE 531 ---
Aufgabe 5.4.2:
X und Y seien zwei unabhängige, poissonverteilte 
Zufallsvariablen. Es gelte  V(X) + V(Y) = 5.  
Bestimmen Sie P(X+Y ≤ 2).
Hinweis:
Die Poisson-Verteilung besitzt die reproduktive Eigenschaft:
(
)
  
 
 unabhängig
Po
Po
Po
 
X +Y
X
f
Y
f
 
f
X,Y




:
:
:
433


--- PAGE 532 ---
Aufgabe 6.1:
Die Wahrscheinlichkeit, dass es an einem Tag im Juni regnet, 
beträgt in einem mediterranen Urlaubsort 0.08, unabhängig 
davon welche Wetterlage an den übrigen Juni-Tagen dort 
herrscht.
a) Wie ist die Anzahl der Regentage in einer Juni-Woche 
(X7), bzw. im gesamten Monat Juni (X30) verteilt?
b) Bestimmen Sie den Erwartungswert und die Varianz von 
X7.
c) Wie gross ist die Wahrscheinlichkeit, dass es ...
 
  - in einer Woche nicht regnet?
- in einer Woche an mindestens drei Tagen regnet?
- im ganzen Juni höchstens zwei Regentage gibt?
434


--- PAGE 533 ---
435
Aufgabe 6.1 (Fortsetzung):
d)
Am gleichen Ort ist die Sonnenscheindauer an einem
Juni-Tag normalverteilt mit µ = 10 [Stunden] und σ2 = 10.8 
[Stunden2].  Wie ist die Gesamt-Sonnenscheindauer im 
Juni (Y30), bzw. die durchschnittliche tägliche 
Sonnen-
 
scheindauer im Juni ( Y30 ) verteilt?
e) Wie gross ist die Wahrscheinlichkeit, dass die Sonne im 
Juni eines Jahres durchschnittlich mehr 
als 11 Stunden 
täglich scheint?


--- PAGE 534 ---
Aufgabe 6.2:
100 Zufallsziffern von 1 bis 5 werden gewählt und addiert. Wie
gross ist die Wahrscheinlichkeit, dass die Summe
a)
höchstens den Wert 250 annimmt?
b) zwischen den Werten 275 und 305 (inkl. Grenzen) liegt?
436


--- PAGE 535 ---
Aufgabe 6.3:
Ein
Würfel
wird
300
Mal
geworfen.
Sei
X
die
Anzahl
geworfener 3er.
Bestimme P(50 < X ≤53) und P(X < 40).
437


--- PAGE 536 ---
Aufgabe 7.4.1:  
Gegeben sind die Ergebnisse einer Prüfungsklausur  
(23 Studenten)
6.2; 4.82; 2.96; 6.18; 6.52; 7.9; 9.62, 6.22; 0.42; 
9.06; 11.7; 6.54; 3.14; 4.74; 2.66; 7.04; 7.78; 11.8; 
9.44; 20.76; 2.9; 8.42; 8.02
Boxplot?   QQ-Plot?
438


--- PAGE 537 ---
Aufgabe 8.2.1 : 
Schätzung für λ im Falle einer Poissonverteilung.
Betrachten wir:
                                                    
für den Parameter λ einer Poissonverteilung.
i)  Sind T1 und T2 erwartungstreu für λ ?
ii) Ist T1 konsistent für λ ?
iii) Ist T1 effizient gegenüber T2?
439
2
n
1
2
n
T  = X
und T =
S
n-1


--- PAGE 538 ---
440
Aufgabe 8.2.2 : 
Seien zwei Schätzfunktionen
für E[X]=μ gegeben.
Zeigen Sie
1. Beide sind erwartungstreu
2. Berechnen Sie die Varianz von beiden
3. Welche Schätzfunktion ist effizient (zwischen 
diesen beiden)? 
n
n
i
1
i
i=1
i=2
1
1
X=
X
und
X =
2X +
X
n
n+1










--- PAGE 539 ---
441
Aufgabe 8.3.1: 
n identische Münzen werden je solange geworfen, 
bis erstmals Kopf erscheint.
Finde ML-Schätzer für  p = P[ "Kopf" ]  aufgrund 
einer Stichprobe X1,…, Xn, wobei Xi die Anzahl 
Würfe vor dem ersten Erfolg für die Münze i 
bezeichnet.


--- PAGE 540 ---
442
Aufgabe 9.2.1: 
Ein Sektor besteht aus N=12.100 Einzelunternehmen. Wir 
betrachten eine Zufallsstichprobe von Größe n=225. Die 
Zielgröße ist P = „Jahresgewinn“ (in Schweizer Franken). 
Deskriptive Statistiken für die Ergebnisse im Jahr 2006 sind:
Bilden Sie:
1.
ein Konfidenzintervall für den durchschnittlichen 
Jahresgewinn mit Signifikanzniveau α=4,55%;
2.
ein Konfidenzintervall für den gesamten Jahresgewinn 
des Sektors mit Signifikanzniveau α=4,55%.
225
600,000. ;
90,000.
P
p
s
=
−
=
−


--- PAGE 541 ---
443
Aufgabe 9.2.2: 
Im Juni 1986 veröffentlichte Consumer Reports einige Daten 
über den Kaloriengehalt von Rinder-Hotdogs. Hier sind die 
Kalorienzahlen für 20 verschiedene Hotdog-Marken (in kcal):
186, 181, 176, 149,184, 190, 158, 139, 175, 148,
152, 111, 141, 153,190, 157, 131, 149, 135, 132.
Nehmen Sie an, dass diese Zahlen die beobachteten Werte aus 
einer Zufallsstichprobe von zwanzig unabhängigen 
normalverteilten Zufallsvariablen mit unbekanntem Mittelwert μ 
und unbekannter Standardabweichung σ sind.
Finden Sie ein 90% Konfidenzintervall für den Erwartungswert
der Kalorien µ.


--- PAGE 542 ---
444
Aufgabe 10.4.1: 
Ein Psychologe behauptet, dass der durchschnittliche IQ einer 
Population 100 beträgt. Eine Zufallsstichprobe von 30 Individuen 
aus dieser Population hat einen durchschnittlichen IQ von 102. 
Nehmen Sie an, dass der IQ der Population normalverteilt ist 
und die Standardabweichung 15 beträgt.
Testen Sie die Behauptung des Psychologen mit 
Signifikanzniveau von 1%.
1. Formulieren Sie die Null- und Alternativhypothese.
2. Berechnen Sie die Teststatistik.
3. Bestimmen Sie den p-Wert.
4. Treffen Sie eine Entscheidung bezüglich der Nullhypothese.


--- PAGE 543 ---
445
Aufgabe 10.4.2: 
Ein Autohersteller behauptet, dass sein neues Modell im 
Durchschnitt 30 Meilen pro Gallone (mpg) auf der Autobahn 
erreicht. Eine Zufallsstichprobe von 25 Autos zeigte einen 
Mittelwert von 28 mpg. Nehmen Sie an, dass die Population 
normalverteilt ist und die Standardabweichung 4 mpg beträgt.
Konstruieren Sie einen einseitigen Test für die Behauptung des 
Herstellers mit Signifikanzniveau von 10%.
1. Formulieren Sie die Null- und Alternativhypothese.
2. Berechnen Sie die Teststatistik.
3. Bestimmen Sie den p-Wert.
4. Treffen Sie eine Entscheidung bezüglich der Nullhypothese.
